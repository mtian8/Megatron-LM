{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mtian8/anaconda3/envs/HF/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# hf_model = torch.load('/u/mtian8/LLM/data/global_step17000/global_step17000_hugging_face/pytorch_model.bin')\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/u/mtian8/LLM/data/global_step17000/global_step17000_hugging_face\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Key | Value[0] |  \n",
      "| --- | --- |  \n",
      "|  decoder.final_layernorm.weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.0.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.0.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.0.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.0.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.0.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.0.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.0.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.0.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.0.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.0.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.0.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.1.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.1.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.1.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.1.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.1.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.1.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.1.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.1.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.1.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.1.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.1.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.10.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.10.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.10.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.10.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.10.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.10.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.10.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.10.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.10.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.10.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.10.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.11.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.11.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.11.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.11.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.11.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.11.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.11.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.11.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.11.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.11.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.11.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.12.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.12.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.12.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.12.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.12.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.12.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.12.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.12.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.12.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.12.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.12.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.13.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.13.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.13.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.13.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.13.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.13.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.13.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.13.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.13.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.13.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.13.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.14.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.14.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.14.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.14.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.14.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.14.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.14.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.14.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.14.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.14.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.14.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.15.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.15.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.15.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.15.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.15.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.15.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.15.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.15.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.15.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.15.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.15.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.16.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.16.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.16.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.16.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.16.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.16.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.16.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.16.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.16.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.16.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.16.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.17.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.17.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.17.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.17.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.17.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.17.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.17.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.17.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.17.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.17.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.17.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.18.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.18.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.18.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.18.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.18.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.18.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.18.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.18.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.18.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.18.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.18.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.19.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.19.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.19.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.19.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.19.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.19.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.19.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.19.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.19.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.19.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.19.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.2.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.2.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.2.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.2.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.2.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.2.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.2.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.2.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.2.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.2.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.2.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.20.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.20.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.20.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.20.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.20.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.20.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.20.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.20.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.20.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.20.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.20.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.21.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.21.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.21.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.21.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.21.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.21.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.21.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.21.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.21.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.21.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.21.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.22.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.22.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.22.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.22.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.22.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.22.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.22.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.22.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.22.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.22.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.22.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.23.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.23.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.23.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.23.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.23.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.23.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.23.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.23.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.23.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.23.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.23.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.24.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.24.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.24.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.24.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.24.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.24.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.24.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.24.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.24.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.24.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.24.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.25.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.25.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.25.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.25.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.25.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.25.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.25.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.25.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.25.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.25.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.25.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.26.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.26.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.26.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.26.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.26.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.26.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.26.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.26.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.26.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.26.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.26.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.27.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.27.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.27.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.27.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.27.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.27.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.27.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.27.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.27.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.27.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.27.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.28.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.28.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.28.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.28.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.28.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.28.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.28.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.28.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.28.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.28.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.28.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.29.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.29.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.29.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.29.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.29.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.29.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.29.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.29.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.29.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.29.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.29.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.3.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.3.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.3.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.3.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.3.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.3.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.3.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.3.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.3.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.3.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.3.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.30.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.30.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.30.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.30.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.30.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.30.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.30.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.30.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.30.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.30.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.30.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.31.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.31.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.31.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.31.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.31.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.31.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.31.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.31.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.31.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.31.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.31.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.4.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.4.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.4.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.4.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.4.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.4.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.4.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.4.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.4.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.4.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.4.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.5.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.5.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.5.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.5.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.5.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.5.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.5.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.5.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.5.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.5.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.5.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.6.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.6.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.6.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.6.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.6.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.6.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.6.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.6.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.6.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.6.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.6.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.7.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.7.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.7.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.7.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.7.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.7.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.7.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.7.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.7.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.7.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.7.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.8.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.8.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.8.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.8.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.8.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.8.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.8.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.8.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.8.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.8.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.8.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  decoder.layers.9.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.9.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.9.mlp.linear_fc1.weight | ('Tensor', torch.Size([22016, 4096])) | \n",
      "|  decoder.layers.9.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.9.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 11008])) | \n",
      "|  decoder.layers.9.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.9.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.9.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 4096])) | \n",
      "|  decoder.layers.9.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.9.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.9.self_attention.linear_qkv.weight | ('Tensor', torch.Size([6144, 4096])) | \n",
      "|  embedding.word_embeddings.weight | ('Tensor', torch.Size([32000, 4096])) | \n",
      "|  output_layer._extra_state | ('Object', <class 'NoneType'>) | \n",
      "|  output_layer.weight | ('Tensor', torch.Size([32000, 4096])) | \n"
     ]
    }
   ],
   "source": [
    "show_state_dict_info(loaded[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3541587/2422443789.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded = torch.load('/work/nvme/bcbw/mtian8/model/MG_model/agpt17000_7B_tp_4_pp_1/iter_0000001/mp_rank_00/model_optim_rng.pt', map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "loaded = torch.load('/work/nvme/bcbw/mtian8/model/MG_model/agpt17000_7B_tp_4_pp_1/iter_0000001/mp_rank_00/model_optim_rng.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, hidden_size=4096, ffn_hidden_size=11008, num_attention_heads=32, kv_channels=128, group_query_attention=True, num_query_groups=8, max_position_embeddings=4096, position_embedding_type='rope', use_rotary_position_embeddings=True, rotary_base=10000, rotary_percent=1.0, rotary_interleaved=False, rotary_seq_len_interpolation_factor=None, add_position_embedding=False, make_vocab_size_divisible_by=128, normalization='RMSNorm', norm_epsilon=1e-05, apply_layernorm_1p=False, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=True, onnx_safe=None, bert_binary_head=True, untie_embeddings_and_output_weights=True, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.01, start_weight_decay=0.01, end_weight_decay=0.01, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1024, rampup_batch_size=None, decrease_batch_size_if_needed=False, recompute_granularity=None, check_for_nan_in_loss_and_grad=True, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=None, clone_scatter_output_in_embedding=True, profile=False, profile_step_start=10, profile_step_end=12, profile_ranks=[0], tp_comm_overlap=False, tp_comm_overlap_cfg=None, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_bulk_dgrad=True, tp_comm_bulk_wgrad=True, use_cpu_initialization=True, empty_unused_memory_level=0, deterministic_mode=False, check_weight_hash_across_dp_replicas_interval=None, calculate_per_token_loss=False, train_iters=None, train_samples=None, log_interval=100, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir=None, masked_softmax_fusion=False, bias_gelu_fusion=False, bias_swiglu_fusion=True, bias_dropout_fusion=False, apply_rope_fusion=True, cross_entropy_loss_fusion=False, use_flash_attn=False, add_bias_linear=False, add_qkv_bias=False, optimizer='adam', dataloader_type='single', async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, gradient_accumulation_fusion=True, deprecated_use_mcore_models=False, use_legacy_models=False, manual_gc=False, manual_gc_interval=0, manual_gc_eval=True, tp_comm_split_ag=True, tp_comm_split_rs=True, seed=1234, data_parallel_random_init=False, init_method_std=0.02, init_method_xavier_uniform=False, lr=None, lr_decay_style='linear', lr_wsd_decay_style='exponential', lr_decay_iters=None, lr_decay_samples=None, lr_wsd_decay_samples=None, lr_wsd_decay_iters=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_init=0.0, min_lr=0.0, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, decoupled_lr=None, decoupled_min_lr=None, save='/work/nvme/bcbw/mtian8/model/MG_model/agpt17000_7B_tp_4_pp_1', save_interval=1, no_save_optim=True, no_save_rng=True, load=None, no_load_optim=True, no_load_rng=True, non_persistent_save_interval=None, non_persistent_ckpt_type=None, non_persistent_global_ckpt_dir=None, finetune=False, pretrained_checkpoint=None, ckpt_step=None, perform_initialization=False, use_checkpoint_args=False, exit_on_missing_checkpoint=False, use_dist_ckpt=False, auto_detect_ckpt_format=False, dist_ckpt_format='torch_dist', ckpt_fully_parallel_save_deprecated=False, ckpt_fully_parallel_save=True, async_save=None, ckpt_fully_parallel_load=False, ckpt_assume_constant_structure=False, dist_ckpt_strictness='assume_ok_unexpected', fp16=False, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, pipeline_model_parallel_size=1, encoder_pipeline_model_parallel_size=0, pipeline_model_parallel_split_rank=None, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, overlap_grad_reduce=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, delay_grad_reduce=True, ddp_bucket_size=None, ddp_average_in_collective=False, overlap_param_gather=False, delay_param_gather=False, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, standalone_embedding_stage=False, use_distributed_optimizer=False, context_parallel_size=1, nccl_communicator_config_path=None, use_tp_pp_dp_mapping=False, eval_iters=100, eval_interval=1000, test_mode=False, skip_train=False, data_path=[], split=None, train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, mmap_bin_files=True, mock_data=True, vocab_size=32000, vocab_file=None, merge_file=None, vocab_extra_ids=0, seq_length=4096, encoder_seq_length=4096, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, num_workers=2, tokenizer_type='Llama2Tokenizer', tokenizer_model=None, tiktoken_pattern=None, tiktoken_num_special_tokens=1000, tiktoken_special_tokens=None, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask_in_dataloader=True, num_dataset_builder_threads=1, s3_cache_path=None, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, qk_layernorm=False, expert_model_parallel_size=1, num_experts=None, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_pre_softmax=False, moe_grouped_gemm=False, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dispatcher_type='allgather', moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_extended_tp=False, log_params_norm=False, log_num_zeros_in_grad=False, log_throughput=False, log_progress=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, wandb_project='', wandb_exp_name='', wandb_save_dir='', logging_level=None, log_straggler=False, disable_straggler_on_startup=False, straggler_ctrlr_port=65535, straggler_minmax_count=1, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, transformer_impl='transformer_engine', window_size=None, retro_project_dir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_attention_gate=1, retro_verify_neighbor_count=True, spec=None, hybrid_attention_ratio=0.0, hybrid_mlp_ratio=0.0, hybrid_override_pattern=None, yaml_cfg=None, enable_one_logger=True, one_logger_project='megatron-lm', one_logger_run_name=None, one_logger_async=False, app_tag_run_name=None, app_tag_run_version='0.0.0', config_logger_dir='', rank=0, world_size=4, transformer_pipeline_model_parallel_size=1, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float32, consumed_train_samples=0, skipped_train_samples=0, consumed_valid_samples=0, variable_seq_lengths=False, blendable_index_path=None, model_type=<ModelType.encoder_or_decoder: 1>, padded_vocab_size=32000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded['args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/bcdz/mtian8/Megatron-LM/megatron/core/tensor_parallel/layers.py:278: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/projects/bcdz/mtian8/Megatron-LM/megatron/core/tensor_parallel/layers.py:294: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/projects/bcdz/mtian8/Megatron-LM/megatron/core/tensor_parallel/layers.py:389: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/projects/bcdz/mtian8/Megatron-LM/megatron/core/tensor_parallel/layers.py:428: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "source": [
    "from megatron.core import dist_checkpointing\n",
    "\n",
    "def save_distributed_checkpoint(checkpoint_path, gpt_model):\n",
    "    sharded_state_dict = gpt_model.sharded_state_dict(prefix='')\n",
    "    dist_checkpointing.save(sharded_state_dict=sharded_state_dict, checkpoint_dir=checkpoint_path)\n",
    "\n",
    "def load_distributed_checkpoint(checkpoint_path, gpt_model):\n",
    "    sharded_state_dict=gpt_model.sharded_state_dict(prefix='')\n",
    "    checkpoint = dist_checkpointing.load(sharded_state_dict=sharded_state_dict, checkpoint_dir=checkpoint_path)\n",
    "    gpt_model.load_state_dict(checkpoint)\n",
    "    return gpt_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_distributed_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3541587/763201809.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 34\u001b[0m\n\u001b[1;32m     25\u001b[0m     merged_state_dict\u001b[38;5;241m.\u001b[39mupdate(state_dict)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# a more general way to merge state_dicts\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# for key in state_dict:\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#     if key not in merged_state_dict:\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#         merged_state_dict[key] = state_dict[key]\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m#     else:\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m#         merged_state_dict[key] += state_dict[key]\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_state_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'args'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Path to the model checkpoint\n",
    "checkpoint_paths = ['/u/mtian8/LLM/data/agpt/iter_0000140/mp_rank_00/model_optim_rng.pt', \n",
    "                    '/u/mtian8/LLM/data/agpt/iter_0000140/mp_rank_01/model_optim_rng.pt',\n",
    "                    '/u/mtian8/LLM/data/agpt/iter_0000140/mp_rank_02/model_optim_rng.pt',\n",
    "                    '/u/mtian8/LLM/data/agpt/iter_0000140/mp_rank_03/model_optim_rng.pt']\n",
    "\n",
    "merged_state_dict = OrderedDict()\n",
    "\n",
    "\n",
    "# Load the model checkpoint\n",
    "# loaded = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "# Load the model checkpoints\n",
    "\n",
    "original_state_dicts = {}\n",
    "\n",
    "for checkpoint_path in checkpoint_paths:\n",
    "    # Load the model checkpoint\n",
    "    loaded = torch.load(checkpoint_path, map_location='cpu')\n",
    "    state_dict = loaded[\"model\"]\n",
    "    original_state_dicts[checkpoint_path] = state_dict\n",
    "    merged_state_dict.update(state_dict)\n",
    "    # a more general way to merge state_dicts\n",
    "    # for key in state_dict:\n",
    "    #     if key not in merged_state_dict:\n",
    "    #         merged_state_dict[key] = state_dict[key]\n",
    "    #     else:\n",
    "    #         merged_state_dict[key] += state_dict[key]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state_dict_info(*state_dicts):\n",
    "    print(\"| Key | \", end=\"\")\n",
    "    for i, state_dict in enumerate(state_dicts):\n",
    "        print(f\"Value[{i}] | \", end=\" \")\n",
    "    print()\n",
    "    print(\"| --- | \", end=\"\")\n",
    "    for i, state_dict in enumerate(state_dicts):\n",
    "        print(f\"--- | \", end=\" \")\n",
    "    print()\n",
    "    def get_printable_content(value):\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            return \"Tensor\", value.shape\n",
    "        elif isinstance(value, list):\n",
    "            return \"List  \", len(value)\n",
    "        else:\n",
    "            return \"Object\", type(value)\n",
    "    all_keys = set()\n",
    "    for state_dict in state_dicts:\n",
    "        all_keys.update(state_dict.keys())\n",
    "    all_keys = sorted(list(all_keys))\n",
    "    for key in all_keys: \n",
    "        print(\"| \", key, end=\" | \")\n",
    "        for state_dict in state_dicts:\n",
    "            if key in state_dict:\n",
    "                print(get_printable_content(state_dict[key]), end=\" | \")\n",
    "            else:\n",
    "                print(\"None\", end=\" | \") \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Key | Value[0] |  Value[1] |  \n",
      "| --- | --- |  --- |  \n",
      "|  decoder.final_layernorm.weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.0.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.0.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.0.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.0.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.0.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.0.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.0.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.0.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.0.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.0.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.0.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.1.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.1.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.1.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.1.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.1.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.1.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.1.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.1.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.1.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.1.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.1.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.10.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.10.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.10.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.10.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.10.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.10.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.10.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.10.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.10.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.10.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.10.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.11.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.11.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.11.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.11.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.11.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.11.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.11.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.11.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.11.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.11.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.11.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.12.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.12.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.12.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.12.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.12.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.12.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.12.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.12.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.12.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.12.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.12.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.13.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.13.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.13.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.13.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.13.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.13.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.13.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.13.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.13.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.13.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.13.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.14.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.14.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.14.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.14.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.14.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.14.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.14.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.14.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.14.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.14.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.14.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.15.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.15.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.15.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.15.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.15.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.15.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.15.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.15.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.15.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.15.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.15.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.16.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.16.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.16.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.16.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.16.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.16.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.16.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.16.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.16.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.16.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.16.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.17.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.17.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.17.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.17.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.17.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.17.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.17.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.17.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.17.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.17.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.17.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.18.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.18.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.18.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.18.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.18.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.18.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.18.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.18.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.18.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.18.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.18.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.19.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.19.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.19.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.19.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.19.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.19.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.19.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.19.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.19.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.19.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.19.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.2.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.2.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.2.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.2.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.2.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.2.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.2.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.2.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.2.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.2.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.2.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.20.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.20.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.20.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.20.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.20.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.20.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.20.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.20.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.20.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.20.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.20.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.21.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.21.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.21.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.21.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.21.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.21.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.21.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.21.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.21.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.21.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.21.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.22.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.22.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.22.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.22.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.22.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.22.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.22.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.22.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.22.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.22.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.22.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.23.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.23.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.23.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.23.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.23.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.23.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.23.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.23.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.23.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.23.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.23.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.24.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.24.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.24.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.24.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.24.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.24.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.24.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.24.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.24.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.24.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.24.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.25.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.25.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.25.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.25.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.25.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.25.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.25.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.25.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.25.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.25.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.25.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.26.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.26.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.26.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.26.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.26.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.26.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.26.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.26.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.26.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.26.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.26.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.27.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.27.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.27.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.27.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.27.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.27.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.27.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.27.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.27.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.27.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.27.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.28.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.28.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.28.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.28.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.28.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.28.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.28.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.28.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.28.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.28.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.28.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.29.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.29.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.29.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.29.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.29.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.29.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.29.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.29.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.29.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.29.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.29.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.3.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.3.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.3.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.3.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.3.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.3.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.3.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.3.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.3.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.3.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.3.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.30.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.30.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.30.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.30.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.30.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.30.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.30.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.30.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.30.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.30.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.30.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.31.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.31.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.31.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.31.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.31.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.31.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.31.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.31.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.31.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.31.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.31.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.4.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.4.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.4.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.4.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.4.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.4.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.4.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.4.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.4.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.4.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.4.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.5.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.5.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.5.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.5.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.5.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.5.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.5.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.5.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.5.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.5.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.5.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.6.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.6.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.6.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.6.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.6.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.6.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.6.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.6.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.6.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.6.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.6.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.7.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.7.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.7.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.7.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.7.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.7.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.7.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.7.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.7.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.7.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.7.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.8.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.8.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.8.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.8.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.8.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.8.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.8.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.8.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.8.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.8.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.8.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  decoder.layers.9.mlp.linear_fc1._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.9.mlp.linear_fc1.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.9.mlp.linear_fc1.weight | ('Tensor', torch.Size([5504, 4096])) | ('Tensor', torch.Size([5504, 4096])) | \n",
      "|  decoder.layers.9.mlp.linear_fc2._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.9.mlp.linear_fc2.weight | ('Tensor', torch.Size([4096, 2752])) | ('Tensor', torch.Size([4096, 2752])) | \n",
      "|  decoder.layers.9.self_attention.core_attention._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.9.self_attention.linear_proj._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.9.self_attention.linear_proj.weight | ('Tensor', torch.Size([4096, 1024])) | ('Tensor', torch.Size([4096, 1024])) | \n",
      "|  decoder.layers.9.self_attention.linear_qkv._extra_state | ('Object', <class '_io.BytesIO'>) | ('Object', <class '_io.BytesIO'>) | \n",
      "|  decoder.layers.9.self_attention.linear_qkv.layer_norm_weight | ('Tensor', torch.Size([4096])) | ('Tensor', torch.Size([4096])) | \n",
      "|  decoder.layers.9.self_attention.linear_qkv.weight | ('Tensor', torch.Size([1536, 4096])) | ('Tensor', torch.Size([1536, 4096])) | \n",
      "|  embedding.word_embeddings.weight | ('Tensor', torch.Size([8000, 4096])) | ('Tensor', torch.Size([8000, 4096])) | \n",
      "|  output_layer._extra_state | ('Object', <class 'NoneType'>) | ('Object', <class 'NoneType'>) | \n",
      "|  output_layer.weight | ('Tensor', torch.Size([8000, 4096])) | ('Tensor', torch.Size([8000, 4096])) | \n"
     ]
    }
   ],
   "source": [
    "show_state_dict_info(merged_state_dict, original_state_dicts[checkpoint_paths[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embedding.word_embeddings.weight',\n",
      " 'decoder.layers.0.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.0.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.0.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.0.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.0.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.0.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.0.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.0.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.0.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.0.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.0.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.1.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.1.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.1.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.1.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.1.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.1.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.1.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.1.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.1.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.1.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.1.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.2.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.2.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.2.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.2.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.2.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.2.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.2.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.2.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.2.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.2.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.2.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.3.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.3.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.3.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.3.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.3.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.3.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.3.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.3.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.3.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.3.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.3.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.4.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.4.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.4.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.4.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.4.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.4.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.4.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.4.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.4.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.4.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.4.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.5.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.5.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.5.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.5.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.5.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.5.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.5.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.5.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.5.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.5.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.5.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.6.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.6.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.6.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.6.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.6.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.6.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.6.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.6.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.6.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.6.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.6.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.7.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.7.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.7.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.7.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.7.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.7.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.7.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.7.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.7.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.7.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.7.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.8.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.8.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.8.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.8.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.8.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.8.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.8.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.8.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.8.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.8.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.8.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.9.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.9.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.9.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.9.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.9.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.9.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.9.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.9.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.9.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.9.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.9.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.10.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.10.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.10.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.10.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.10.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.10.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.10.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.10.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.10.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.10.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.10.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.11.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.11.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.11.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.11.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.11.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.11.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.11.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.11.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.11.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.11.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.11.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.12.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.12.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.12.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.12.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.12.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.12.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.12.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.12.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.12.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.12.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.12.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.13.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.13.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.13.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.13.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.13.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.13.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.13.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.13.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.13.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.13.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.13.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.14.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.14.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.14.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.14.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.14.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.14.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.14.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.14.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.14.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.14.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.14.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.15.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.15.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.15.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.15.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.15.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.15.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.15.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.15.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.15.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.15.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.15.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.16.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.16.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.16.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.16.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.16.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.16.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.16.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.16.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.16.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.16.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.16.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.17.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.17.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.17.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.17.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.17.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.17.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.17.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.17.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.17.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.17.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.17.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.18.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.18.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.18.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.18.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.18.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.18.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.18.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.18.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.18.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.18.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.18.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.19.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.19.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.19.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.19.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.19.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.19.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.19.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.19.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.19.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.19.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.19.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.20.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.20.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.20.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.20.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.20.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.20.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.20.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.20.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.20.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.20.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.20.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.21.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.21.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.21.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.21.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.21.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.21.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.21.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.21.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.21.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.21.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.21.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.22.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.22.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.22.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.22.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.22.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.22.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.22.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.22.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.22.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.22.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.22.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.23.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.23.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.23.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.23.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.23.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.23.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.23.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.23.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.23.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.23.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.23.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.24.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.24.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.24.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.24.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.24.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.24.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.24.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.24.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.24.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.24.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.24.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.25.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.25.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.25.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.25.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.25.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.25.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.25.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.25.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.25.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.25.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.25.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.26.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.26.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.26.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.26.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.26.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.26.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.26.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.26.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.26.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.26.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.26.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.27.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.27.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.27.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.27.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.27.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.27.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.27.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.27.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.27.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.27.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.27.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.28.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.28.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.28.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.28.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.28.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.28.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.28.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.28.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.28.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.28.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.28.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.29.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.29.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.29.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.29.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.29.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.29.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.29.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.29.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.29.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.29.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.29.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.30.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.30.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.30.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.30.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.30.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.30.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.30.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.30.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.30.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.30.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.30.mlp.linear_fc2._extra_state',\n",
      " 'decoder.layers.31.self_attention.core_attention._extra_state',\n",
      " 'decoder.layers.31.self_attention.linear_proj.weight',\n",
      " 'decoder.layers.31.self_attention.linear_proj._extra_state',\n",
      " 'decoder.layers.31.self_attention.linear_qkv.layer_norm_weight',\n",
      " 'decoder.layers.31.self_attention.linear_qkv.weight',\n",
      " 'decoder.layers.31.self_attention.linear_qkv._extra_state',\n",
      " 'decoder.layers.31.mlp.linear_fc1.layer_norm_weight',\n",
      " 'decoder.layers.31.mlp.linear_fc1.weight',\n",
      " 'decoder.layers.31.mlp.linear_fc1._extra_state',\n",
      " 'decoder.layers.31.mlp.linear_fc2.weight',\n",
      " 'decoder.layers.31.mlp.linear_fc2._extra_state',\n",
      " 'decoder.final_layernorm.weight',\n",
      " 'output_layer.weight',\n",
      " 'output_layer._extra_state']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "\n",
    "pprint.pprint(list(original_state_dicts[checkpoint_paths[0]].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, hidden_size=4096, ffn_hidden_size=14336, num_attention_heads=32, kv_channels=128, group_query_attention=True, num_query_groups=8, max_position_embeddings=8192, position_embedding_type='rope', use_rotary_position_embeddings=True, rotary_base=10000, rotary_percent=1.0, rotary_interleaved=False, rotary_seq_len_interpolation_factor=None, add_position_embedding=False, make_vocab_size_divisible_by=128, normalization='RMSNorm', norm_epsilon=1e-05, apply_layernorm_1p=False, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=True, onnx_safe=None, bert_binary_head=True, untie_embeddings_and_output_weights=True, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.01, start_weight_decay=0.01, end_weight_decay=0.01, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1024, rampup_batch_size=None, recompute_granularity=None, check_for_nan_in_loss_and_grad=True, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=None, clone_scatter_output_in_embedding=True, profile=False, profile_step_start=10, profile_step_end=12, profile_ranks=[0], tp_comm_overlap=False, tp_comm_overlap_cfg=None, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_bulk_dgrad=True, tp_comm_bulk_wgrad=True, use_cpu_initialization=True, empty_unused_memory_level=0, deterministic_mode=False, check_weight_hash_across_dp_replicas_interval=None, calculate_per_token_loss=False, train_iters=None, train_samples=None, log_interval=100, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir=None, masked_softmax_fusion=False, bias_gelu_fusion=False, bias_swiglu_fusion=True, bias_dropout_fusion=False, apply_rope_fusion=True, cross_entropy_loss_fusion=False, use_flash_attn=False, add_bias_linear=False, add_qkv_bias=False, optimizer='adam', dataloader_type='single', async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, gradient_accumulation_fusion=True, deprecated_use_mcore_models=False, use_legacy_models=False, manual_gc=False, manual_gc_interval=0, manual_gc_eval=True, tp_comm_split_ag=True, tp_comm_split_rs=True, seed=1234, data_parallel_random_init=False, init_method_std=0.02, init_method_xavier_uniform=False, lr=None, lr_decay_style='linear', lr_wsd_decay_style='exponential', lr_decay_iters=None, lr_decay_samples=None, lr_wsd_decay_samples=None, lr_wsd_decay_iters=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_init=0.0, min_lr=0.0, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, decoupled_lr=None, decoupled_min_lr=None, save='/lcrc/project/TuningLLMs/mtian8/LLM4Tutorial/model/MG_model/llama3_8B_tp_1', save_interval=1, no_save_optim=True, no_save_rng=True, load=None, no_load_optim=True, no_load_rng=True, non_persistent_save_interval=None, non_persistent_ckpt_type=None, non_persistent_global_ckpt_dir=None, finetune=False, pretrained_checkpoint=None, ckpt_step=None, perform_initialization=False, use_checkpoint_args=False, exit_on_missing_checkpoint=False, use_dist_ckpt=False, auto_detect_ckpt_format=False, dist_ckpt_format='torch_dist', ckpt_fully_parallel_save_deprecated=False, ckpt_fully_parallel_save=True, async_save=None, ckpt_fully_parallel_load=False, ckpt_assume_constant_structure=False, dist_ckpt_strictness='assume_ok_unexpected', fp16=False, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, pipeline_model_parallel_size=1, encoder_pipeline_model_parallel_size=None, pipeline_model_parallel_split_rank=None, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, overlap_grad_reduce=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, delay_grad_reduce=True, ddp_bucket_size=None, ddp_average_in_collective=False, overlap_param_gather=False, delay_param_gather=False, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=None, lazy_mpu_init=None, standalone_embedding_stage=False, use_distributed_optimizer=False, context_parallel_size=1, nccl_communicator_config_path=None, use_tp_pp_dp_mapping=False, eval_iters=100, eval_interval=1000, test_mode=False, skip_train=False, data_path=[], split=None, train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, mmap_bin_files=True, mock_data=True, vocab_size=128256, vocab_file=None, merge_file=None, vocab_extra_ids=0, seq_length=4096, encoder_seq_length=4096, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, num_workers=2, tokenizer_type='Llama3Tokenizer', tokenizer_model=None, tiktoken_pattern=None, tiktoken_num_special_tokens=1000, tiktoken_special_tokens=None, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask_in_dataloader=True, num_dataset_builder_threads=1, s3_cache_path=None, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, qk_layernorm=False, expert_model_parallel_size=1, num_experts=None, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_grouped_gemm=False, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dispatcher_type='allgather', moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_extended_tp=False, log_params_norm=False, log_num_zeros_in_grad=False, log_throughput=False, log_progress=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, wandb_project='', wandb_exp_name='', wandb_save_dir='', logging_level=None, log_straggler=False, disable_straggler_on_startup=False, straggler_ctrlr_port=65535, straggler_minmax_count=1, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, transformer_impl='transformer_engine', retro_project_dir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_attention_gate=1, retro_verify_neighbor_count=True, spec=None, hybrid_attention_ratio=0.0, hybrid_mlp_ratio=0.0, hybrid_override_pattern=None, yaml_cfg=None, enable_one_logger=True, one_logger_project='megatron-lm', one_logger_run_name=None, one_logger_async=False, app_tag_run_name=None, app_tag_run_version='0.0.0', rank=0, world_size=1, transformer_pipeline_model_parallel_size=1, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float32, consumed_train_samples=0, consumed_valid_samples=0, variable_seq_lengths=False, blendable_index_path=None, model_type=<ModelType.encoder_or_decoder: 1>, padded_vocab_size=128256)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'rope_theta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope_theta\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'rope_theta'"
     ]
    }
   ],
   "source": [
    "args.rope_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding.word_embeddings.weight',\n",
       "              tensor([[ 1.3733e-03,  5.0964e-03, -3.0365e-03,  ...,  2.2888e-03,\n",
       "                       -1.9531e-03, -1.7166e-05],\n",
       "                      [-2.7313e-03,  1.9379e-03, -1.3733e-03,  ..., -5.1498e-05,\n",
       "                       -1.3962e-03, -1.9836e-03],\n",
       "                      [ 9.5367e-04, -1.3367e-02,  4.1771e-04,  ...,  2.5940e-03,\n",
       "                        7.0496e-03,  4.1809e-03],\n",
       "                      ...,\n",
       "                      [ 1.8715e-23,  3.2699e-24,  1.8198e-23,  ...,  5.3767e-23,\n",
       "                       -2.2360e-24, -1.9852e-23],\n",
       "                      [ 1.9335e-23, -1.8612e-24, -1.8818e-23,  ...,  2.3368e-23,\n",
       "                        7.3412e-24, -3.1226e-23],\n",
       "                      [-7.4860e-23, -6.3693e-23,  5.5059e-24,  ...,  4.9631e-24,\n",
       "                       -5.4594e-23, -2.2877e-24]])),\n",
       "             ('decoder.layers.0.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2345e0>),\n",
       "             ('decoder.layers.0.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0022, -0.0011, -0.0090,  ..., -0.0012, -0.0131,  0.0018],\n",
       "                      [-0.0025,  0.0151,  0.0088,  ..., -0.0031, -0.0067,  0.0046],\n",
       "                      [-0.0018,  0.0005,  0.0039,  ..., -0.0077, -0.0010, -0.0005],\n",
       "                      ...,\n",
       "                      [ 0.0077, -0.0044, -0.0398,  ...,  0.0162, -0.0037,  0.0063],\n",
       "                      [-0.0093,  0.0006, -0.0050,  ..., -0.0019, -0.0020, -0.0029],\n",
       "                      [-0.0078,  0.0028, -0.0047,  ..., -0.0054,  0.0037,  0.0014]])),\n",
       "             ('decoder.layers.0.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f234680>),\n",
       "             ('decoder.layers.0.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.0537, 0.2090, 0.4492,  ..., 0.0859, 0.0437, 0.0292])),\n",
       "             ('decoder.layers.0.self_attention.linear_qkv.weight',\n",
       "              tensor([[-0.0028, -0.0291, -0.0032,  ...,  0.0074, -0.0469, -0.0216],\n",
       "                      [-0.0125, -0.0698, -0.0039,  ..., -0.0126, -0.0498,  0.0205],\n",
       "                      [-0.0188, -0.0466, -0.0048,  ...,  0.0115, -0.0132,  0.0115],\n",
       "                      ...,\n",
       "                      [ 0.0079,  0.0008,  0.0029,  ..., -0.0014, -0.0064, -0.0064],\n",
       "                      [ 0.0032,  0.0012,  0.0025,  ...,  0.0027, -0.0046, -0.0011],\n",
       "                      [-0.0024, -0.0070,  0.0017,  ...,  0.0033,  0.0071, -0.0034]])),\n",
       "             ('decoder.layers.0.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f234770>),\n",
       "             ('decoder.layers.0.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.1514, 0.1396, 0.1543,  ..., 0.1533, 0.1523, 0.1494])),\n",
       "             ('decoder.layers.0.mlp.linear_fc1.weight',\n",
       "              tensor([[-0.0121, -0.0051, -0.0036,  ...,  0.0149, -0.0134, -0.0030],\n",
       "                      [-0.0067, -0.0267, -0.0032,  ...,  0.0131,  0.0046, -0.0016],\n",
       "                      [ 0.0110, -0.0005,  0.0135,  ..., -0.0006,  0.0047,  0.0050],\n",
       "                      ...,\n",
       "                      [-0.0006,  0.0154,  0.0066,  ..., -0.0069,  0.0065,  0.0121],\n",
       "                      [-0.0178,  0.0133,  0.0030,  ...,  0.0034,  0.0071, -0.0061],\n",
       "                      [-0.0036, -0.0135, -0.0029,  ..., -0.0027, -0.0072,  0.0076]])),\n",
       "             ('decoder.layers.0.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f234860>),\n",
       "             ('decoder.layers.0.mlp.linear_fc2.weight',\n",
       "              tensor([[ 0.0087, -0.0151, -0.0090,  ...,  0.0079, -0.0039,  0.0134],\n",
       "                      [ 0.0204, -0.0107, -0.0057,  ...,  0.0010,  0.0172,  0.0011],\n",
       "                      [ 0.0082, -0.0075, -0.0023,  ..., -0.0018,  0.0025, -0.0165],\n",
       "                      ...,\n",
       "                      [ 0.0085, -0.0208,  0.0217,  ..., -0.0199,  0.0081, -0.0129],\n",
       "                      [-0.0135, -0.0059, -0.0110,  ...,  0.0093,  0.0015, -0.0131],\n",
       "                      [-0.0029,  0.0069,  0.0085,  ..., -0.0082, -0.0051, -0.0120]])),\n",
       "             ('decoder.layers.0.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f234900>),\n",
       "             ('decoder.layers.1.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f234950>),\n",
       "             ('decoder.layers.1.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0125,  0.0095,  0.0029,  ..., -0.0067, -0.0125,  0.0101],\n",
       "                      [ 0.0046, -0.0110, -0.0021,  ...,  0.0051, -0.0188, -0.0020],\n",
       "                      [-0.0049, -0.0088, -0.0124,  ...,  0.0072, -0.0089, -0.0204],\n",
       "                      ...,\n",
       "                      [ 0.0069, -0.0121,  0.0036,  ...,  0.0117,  0.0162, -0.0070],\n",
       "                      [ 0.0132,  0.0076, -0.0112,  ..., -0.0036, -0.0053, -0.0087],\n",
       "                      [ 0.0115, -0.0020,  0.0059,  ...,  0.0042,  0.0123, -0.0027]])),\n",
       "             ('decoder.layers.1.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2349f0>),\n",
       "             ('decoder.layers.1.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.1006, 0.0845, 0.0771,  ..., 0.3730, 0.0820, 0.0850])),\n",
       "             ('decoder.layers.1.self_attention.linear_qkv.weight',\n",
       "              tensor([[-0.0312, -0.0181, -0.0189,  ..., -0.0281,  0.0028, -0.0088],\n",
       "                      [-0.0029,  0.0005, -0.0078,  ..., -0.0229,  0.0010,  0.0079],\n",
       "                      [-0.0032,  0.0089, -0.0236,  ..., -0.0123, -0.0021, -0.0083],\n",
       "                      ...,\n",
       "                      [ 0.0053, -0.0034,  0.0022,  ...,  0.0004, -0.0034, -0.0243],\n",
       "                      [ 0.0153, -0.0057, -0.0093,  ..., -0.0085,  0.0170, -0.0008],\n",
       "                      [ 0.0079, -0.0093, -0.0061,  ..., -0.0026, -0.0033,  0.0025]])),\n",
       "             ('decoder.layers.1.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f234ae0>),\n",
       "             ('decoder.layers.1.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.1855, 0.1816, 0.1826,  ..., 0.1201, 0.1885, 0.1875])),\n",
       "             ('decoder.layers.1.mlp.linear_fc1.weight',\n",
       "              tensor([[-0.0243, -0.0076, -0.0015,  ..., -0.0034,  0.0098, -0.0137],\n",
       "                      [-0.0160,  0.0120, -0.0245,  ...,  0.0105, -0.0114, -0.0148],\n",
       "                      [ 0.0036, -0.0071, -0.0016,  ...,  0.0173, -0.0056, -0.0231],\n",
       "                      ...,\n",
       "                      [ 0.0128,  0.0030,  0.0002,  ...,  0.0005,  0.0028, -0.0150],\n",
       "                      [ 0.0006,  0.0065, -0.0052,  ..., -0.0060,  0.0056, -0.0006],\n",
       "                      [-0.0084, -0.0215, -0.0143,  ..., -0.0053, -0.0171, -0.0192]])),\n",
       "             ('decoder.layers.1.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d93575300>),\n",
       "             ('decoder.layers.1.mlp.linear_fc2.weight',\n",
       "              tensor([[-2.1820e-03, -1.2451e-02, -8.9111e-03,  ...,  5.9204e-03,\n",
       "                       -7.1106e-03,  5.2185e-03],\n",
       "                      [-5.9814e-03,  1.0254e-02, -5.5847e-03,  ..., -1.0132e-02,\n",
       "                        1.0071e-02, -7.0190e-03],\n",
       "                      [ 2.6703e-03,  1.4282e-02,  5.9509e-03,  ..., -9.3994e-03,\n",
       "                       -7.9346e-03,  6.0730e-03],\n",
       "                      ...,\n",
       "                      [-8.8501e-03, -1.2878e-02, -1.0452e-03,  ..., -9.0332e-03,\n",
       "                       -1.1475e-02,  1.2085e-02],\n",
       "                      [-2.9297e-03,  7.4768e-03, -1.3580e-03,  ..., -2.6512e-04,\n",
       "                        4.9591e-04, -1.2207e-02],\n",
       "                      [-1.4832e-02,  1.5625e-02,  5.1270e-03,  ..., -5.9204e-03,\n",
       "                       -1.4221e-02, -1.2890e-06]])),\n",
       "             ('decoder.layers.1.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f213d80>),\n",
       "             ('decoder.layers.2.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f213740>),\n",
       "             ('decoder.layers.2.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0096,  0.0014, -0.0120,  ..., -0.0110, -0.0102,  0.0061],\n",
       "                      [ 0.0049,  0.0011,  0.0023,  ...,  0.0042,  0.0023, -0.0042],\n",
       "                      [ 0.0121, -0.0083, -0.0087,  ...,  0.0042,  0.0198, -0.0051],\n",
       "                      ...,\n",
       "                      [ 0.0014,  0.0183,  0.0087,  ...,  0.0134, -0.0085,  0.0076],\n",
       "                      [-0.0170,  0.0051, -0.0063,  ...,  0.0016, -0.0060, -0.0002],\n",
       "                      [ 0.0036,  0.0095,  0.0055,  ..., -0.0044,  0.0056,  0.0049]])),\n",
       "             ('decoder.layers.2.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f234bd0>),\n",
       "             ('decoder.layers.2.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.3145, 0.3145, 0.2969,  ..., 0.7930, 0.2910, 0.2969])),\n",
       "             ('decoder.layers.2.self_attention.linear_qkv.weight',\n",
       "              tensor([[-0.0248,  0.0095,  0.0024,  ...,  0.0082,  0.0198, -0.0088],\n",
       "                      [-0.0161, -0.0220, -0.0201,  ..., -0.0097,  0.0036, -0.0034],\n",
       "                      [-0.0030, -0.0021, -0.0011,  ...,  0.0270, -0.0092,  0.0054],\n",
       "                      ...,\n",
       "                      [ 0.0004,  0.0065,  0.0011,  ..., -0.0040, -0.0153,  0.0053],\n",
       "                      [ 0.0093,  0.0020, -0.0041,  ..., -0.0035, -0.0023, -0.0071],\n",
       "                      [-0.0060,  0.0119, -0.0152,  ..., -0.0020, -0.0058,  0.0032]])),\n",
       "             ('decoder.layers.2.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f234cc0>),\n",
       "             ('decoder.layers.2.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.2266, 0.2256, 0.2236,  ..., 0.1367, 0.2344, 0.2334])),\n",
       "             ('decoder.layers.2.mlp.linear_fc1.weight',\n",
       "              tensor([[ 2.9144e-03,  1.3977e-02,  5.4932e-03,  ...,  2.9907e-03,\n",
       "                        1.6479e-02,  2.0630e-02],\n",
       "                      [-1.3809e-03,  1.3245e-02,  5.7373e-03,  ..., -2.2888e-03,\n",
       "                       -3.0398e-05, -5.1575e-03],\n",
       "                      [ 1.0559e-02,  1.0300e-03,  2.0874e-02,  ..., -8.0566e-03,\n",
       "                        9.7046e-03, -3.2806e-04],\n",
       "                      ...,\n",
       "                      [-4.3945e-03, -2.9907e-03,  1.8799e-02,  ...,  5.4626e-03,\n",
       "                        9.8267e-03, -1.5869e-02],\n",
       "                      [ 1.1230e-02,  1.3245e-02,  5.8289e-03,  ...,  2.0142e-02,\n",
       "                        1.3367e-02, -7.6294e-04],\n",
       "                      [ 6.0654e-04,  2.3193e-03, -4.3640e-03,  ..., -3.0823e-03,\n",
       "                        2.1240e-02, -6.1340e-03]])),\n",
       "             ('decoder.layers.2.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f234db0>),\n",
       "             ('decoder.layers.2.mlp.linear_fc2.weight',\n",
       "              tensor([[ 0.0179,  0.0063,  0.0030,  ...,  0.0134, -0.0051, -0.0048],\n",
       "                      [ 0.0188, -0.0093, -0.0040,  ..., -0.0091, -0.0043, -0.0107],\n",
       "                      [-0.0011, -0.0026, -0.0204,  ..., -0.0179, -0.0024, -0.0276],\n",
       "                      ...,\n",
       "                      [ 0.0085,  0.0210,  0.0096,  ..., -0.0099,  0.0075,  0.0064],\n",
       "                      [ 0.0072, -0.0060, -0.0203,  ..., -0.0136,  0.0015,  0.0165],\n",
       "                      [ 0.0176, -0.0103,  0.0030,  ...,  0.0096, -0.0018,  0.0127]])),\n",
       "             ('decoder.layers.2.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f234e50>),\n",
       "             ('decoder.layers.3.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f234ea0>),\n",
       "             ('decoder.layers.3.self_attention.linear_proj.weight',\n",
       "              tensor([[-0.0066,  0.0030,  0.0096,  ...,  0.0017,  0.0098,  0.0096],\n",
       "                      [ 0.0023,  0.0053, -0.0018,  ..., -0.0030, -0.0054,  0.0101],\n",
       "                      [-0.0062, -0.0090, -0.0134,  ...,  0.0034,  0.0015,  0.0090],\n",
       "                      ...,\n",
       "                      [ 0.0024, -0.0093, -0.0173,  ..., -0.0121,  0.0098, -0.0040],\n",
       "                      [-0.0055,  0.0107, -0.0060,  ..., -0.0052, -0.0008,  0.0109],\n",
       "                      [ 0.0056,  0.0018, -0.0088,  ..., -0.0126, -0.0036,  0.0136]])),\n",
       "             ('decoder.layers.3.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f234f40>),\n",
       "             ('decoder.layers.3.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.3809, 0.3789, 0.3613,  ..., 0.4473, 0.3691, 0.3555])),\n",
       "             ('decoder.layers.3.self_attention.linear_qkv.weight',\n",
       "              tensor([[ 0.0092,  0.0076,  0.0015,  ...,  0.0166,  0.0034,  0.0076],\n",
       "                      [ 0.0125,  0.0153,  0.0129,  ..., -0.0004,  0.0128,  0.0056],\n",
       "                      [ 0.0192,  0.0037, -0.0167,  ...,  0.0115,  0.0078, -0.0008],\n",
       "                      ...,\n",
       "                      [ 0.0091,  0.0006,  0.0067,  ..., -0.0012,  0.0012,  0.0021],\n",
       "                      [ 0.0103, -0.0025,  0.0018,  ..., -0.0063,  0.0052, -0.0006],\n",
       "                      [-0.0031,  0.0001, -0.0077,  ...,  0.0025, -0.0118, -0.0160]])),\n",
       "             ('decoder.layers.3.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235030>),\n",
       "             ('decoder.layers.3.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.2520, 0.2480, 0.2539,  ..., 0.1699, 0.2656, 0.2617])),\n",
       "             ('decoder.layers.3.mlp.linear_fc1.weight',\n",
       "              tensor([[ 5.4626e-03, -1.2207e-02,  1.3000e-02,  ...,  1.3306e-02,\n",
       "                       -1.0559e-02, -1.7212e-02],\n",
       "                      [-7.6599e-03, -7.7515e-03, -1.0300e-03,  ..., -4.3030e-03,\n",
       "                       -2.2705e-02, -1.1230e-02],\n",
       "                      [-2.5879e-02, -8.7280e-03, -9.3994e-03,  ..., -5.0049e-03,\n",
       "                        4.0894e-03,  1.1597e-02],\n",
       "                      ...,\n",
       "                      [-2.4261e-03, -1.0620e-02,  1.2283e-03,  ..., -2.7618e-03,\n",
       "                        1.5320e-02, -1.3123e-02],\n",
       "                      [ 6.8054e-03, -2.8534e-03, -1.0620e-02,  ..., -2.1729e-02,\n",
       "                       -1.5747e-02, -3.8862e-05],\n",
       "                      [-1.0254e-02,  7.2937e-03,  1.3428e-02,  ..., -1.9836e-03,\n",
       "                       -8.3618e-03, -1.9684e-03]])),\n",
       "             ('decoder.layers.3.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235120>),\n",
       "             ('decoder.layers.3.mlp.linear_fc2.weight',\n",
       "              tensor([[ 0.0051,  0.0005, -0.0082,  ...,  0.0173, -0.0031, -0.0018],\n",
       "                      [ 0.0046, -0.0010, -0.0005,  ..., -0.0165,  0.0009,  0.0145],\n",
       "                      [ 0.0122, -0.0007,  0.0060,  ..., -0.0210,  0.0053, -0.0098],\n",
       "                      ...,\n",
       "                      [-0.0055, -0.0004,  0.0038,  ..., -0.0146, -0.0140, -0.0029],\n",
       "                      [ 0.0137, -0.0276,  0.0044,  ...,  0.0016,  0.0034, -0.0087],\n",
       "                      [ 0.0022,  0.0134, -0.0059,  ...,  0.0234, -0.0064, -0.0038]])),\n",
       "             ('decoder.layers.3.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2351c0>),\n",
       "             ('decoder.layers.4.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235210>),\n",
       "             ('decoder.layers.4.self_attention.linear_proj.weight',\n",
       "              tensor([[ 1.5015e-02,  4.5471e-03, -2.2583e-03,  ...,  7.0190e-03,\n",
       "                       -8.3618e-03,  3.0518e-03],\n",
       "                      [ 8.8501e-03,  3.6774e-03,  1.3245e-02,  ...,  5.7678e-03,\n",
       "                        1.0620e-02, -3.1281e-03],\n",
       "                      [ 1.6556e-03,  7.9346e-03,  9.0942e-03,  ..., -6.6757e-04,\n",
       "                        1.0925e-02,  3.2806e-03],\n",
       "                      ...,\n",
       "                      [-6.4392e-03,  8.4229e-03,  2.9373e-04,  ...,  8.6670e-03,\n",
       "                       -3.5095e-03, -4.4556e-03],\n",
       "                      [-8.0566e-03, -1.2085e-02, -3.4332e-03,  ...,  7.4387e-04,\n",
       "                       -2.9907e-03,  1.5991e-02],\n",
       "                      [ 7.5073e-03, -2.3804e-03,  7.7820e-03,  ..., -3.7384e-03,\n",
       "                        9.2773e-03, -3.1948e-05]])),\n",
       "             ('decoder.layers.4.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2352b0>),\n",
       "             ('decoder.layers.4.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.3477, 0.3457, 0.3105,  ..., 0.4512, 0.2969, 0.3047])),\n",
       "             ('decoder.layers.4.self_attention.linear_qkv.weight',\n",
       "              tensor([[-0.0052, -0.0096, -0.0068,  ...,  0.0015,  0.0238, -0.0254],\n",
       "                      [-0.0267, -0.0044, -0.0047,  ..., -0.0126,  0.0103, -0.0064],\n",
       "                      [-0.0100, -0.0118,  0.0295,  ...,  0.0116, -0.0153,  0.0098],\n",
       "                      ...,\n",
       "                      [ 0.0126,  0.0177, -0.0154,  ...,  0.0106, -0.0031, -0.0039],\n",
       "                      [-0.0154,  0.0065, -0.0027,  ..., -0.0018,  0.0102,  0.0132],\n",
       "                      [ 0.0082, -0.0031,  0.0150,  ..., -0.0037,  0.0050, -0.0151]])),\n",
       "             ('decoder.layers.4.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2353a0>),\n",
       "             ('decoder.layers.4.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.2754, 0.2734, 0.2754,  ..., 0.2080, 0.2812, 0.2832])),\n",
       "             ('decoder.layers.4.mlp.linear_fc1.weight',\n",
       "              tensor([[-0.0113,  0.0070,  0.0110,  ...,  0.0084,  0.0082, -0.0250],\n",
       "                      [ 0.0050, -0.0095, -0.0337,  ...,  0.0087, -0.0105,  0.0023],\n",
       "                      [ 0.0187, -0.0032, -0.0139,  ..., -0.0036,  0.0088, -0.0005],\n",
       "                      ...,\n",
       "                      [-0.0016,  0.0058,  0.0006,  ...,  0.0057, -0.0045, -0.0054],\n",
       "                      [ 0.0017,  0.0148,  0.0112,  ...,  0.0045, -0.0132,  0.0015],\n",
       "                      [-0.0058, -0.0005, -0.0066,  ...,  0.0103, -0.0192, -0.0076]])),\n",
       "             ('decoder.layers.4.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235490>),\n",
       "             ('decoder.layers.4.mlp.linear_fc2.weight',\n",
       "              tensor([[-0.0130,  0.0048, -0.0094,  ..., -0.0090, -0.0011, -0.0150],\n",
       "                      [-0.0053, -0.0044,  0.0046,  ...,  0.0135,  0.0148,  0.0103],\n",
       "                      [-0.0035,  0.0066, -0.0117,  ...,  0.0010,  0.0043, -0.0232],\n",
       "                      ...,\n",
       "                      [ 0.0046, -0.0039,  0.0156,  ...,  0.0096,  0.0102,  0.0092],\n",
       "                      [ 0.0194,  0.0085,  0.0102,  ..., -0.0065, -0.0007,  0.0136],\n",
       "                      [ 0.0096, -0.0113, -0.0145,  ...,  0.0018, -0.0078,  0.0087]])),\n",
       "             ('decoder.layers.4.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235530>),\n",
       "             ('decoder.layers.5.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235580>),\n",
       "             ('decoder.layers.5.self_attention.linear_proj.weight',\n",
       "              tensor([[-0.0181, -0.0109, -0.0190,  ..., -0.0085, -0.0073, -0.0016],\n",
       "                      [-0.0134,  0.0133, -0.0166,  ..., -0.0003, -0.0052,  0.0013],\n",
       "                      [ 0.0288,  0.0026, -0.0159,  ..., -0.0012, -0.0056, -0.0042],\n",
       "                      ...,\n",
       "                      [-0.0090, -0.0002,  0.0015,  ...,  0.0035,  0.0023,  0.0036],\n",
       "                      [-0.0074, -0.0005, -0.0012,  ...,  0.0046, -0.0087, -0.0005],\n",
       "                      [-0.0015,  0.0133,  0.0007,  ...,  0.0170,  0.0110,  0.0099]])),\n",
       "             ('decoder.layers.5.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235620>),\n",
       "             ('decoder.layers.5.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4004, 0.3984, 0.3535,  ..., 0.5898, 0.3320, 0.3223])),\n",
       "             ('decoder.layers.5.self_attention.linear_qkv.weight',\n",
       "              tensor([[ 0.0068, -0.0071,  0.0026,  ..., -0.0045, -0.0080,  0.0068],\n",
       "                      [ 0.0173, -0.0001,  0.0129,  ..., -0.0146, -0.0200, -0.0140],\n",
       "                      [-0.0096, -0.0080, -0.0057,  ...,  0.0068,  0.0125,  0.0023],\n",
       "                      ...,\n",
       "                      [-0.0035, -0.0013, -0.0045,  ...,  0.0004, -0.0029, -0.0023],\n",
       "                      [-0.0004,  0.0101,  0.0014,  ...,  0.0040,  0.0029,  0.0098],\n",
       "                      [-0.0034, -0.0021,  0.0069,  ...,  0.0045, -0.0044, -0.0039]])),\n",
       "             ('decoder.layers.5.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235710>),\n",
       "             ('decoder.layers.5.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.2910, 0.2910, 0.2969,  ..., 0.2266, 0.3027, 0.2988])),\n",
       "             ('decoder.layers.5.mlp.linear_fc1.weight',\n",
       "              tensor([[-3.7842e-02,  8.5449e-03, -1.3184e-02,  ...,  2.2705e-02,\n",
       "                        1.9897e-02,  6.4087e-03],\n",
       "                      [ 5.5542e-03,  2.8076e-03, -1.1414e-02,  ..., -7.8735e-03,\n",
       "                       -1.4343e-02, -1.2817e-02],\n",
       "                      [-6.1340e-03,  1.8799e-02,  4.3106e-04,  ...,  6.9046e-04,\n",
       "                       -6.9275e-03, -1.7212e-02],\n",
       "                      ...,\n",
       "                      [-7.0801e-03, -2.2583e-02, -5.6458e-03,  ...,  4.1504e-03,\n",
       "                        2.2705e-02,  5.7983e-03],\n",
       "                      [-1.1719e-02,  1.2695e-02, -2.9907e-03,  ...,  5.7220e-05,\n",
       "                       -1.2573e-02, -4.1504e-03],\n",
       "                      [ 4.3106e-04, -1.4832e-02,  4.3030e-03,  ..., -5.0964e-03,\n",
       "                        1.4648e-02, -1.8311e-02]])),\n",
       "             ('decoder.layers.5.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235800>),\n",
       "             ('decoder.layers.5.mlp.linear_fc2.weight',\n",
       "              tensor([[ 0.0058,  0.0010,  0.0029,  ..., -0.0366, -0.0147, -0.0060],\n",
       "                      [-0.0037, -0.0030, -0.0019,  ..., -0.0042,  0.0064, -0.0114],\n",
       "                      [ 0.0167, -0.0077, -0.0081,  ...,  0.0075, -0.0091, -0.0032],\n",
       "                      ...,\n",
       "                      [-0.0067, -0.0034,  0.0023,  ...,  0.0101, -0.0092, -0.0067],\n",
       "                      [-0.0042,  0.0073, -0.0118,  ...,  0.0220, -0.0064,  0.0107],\n",
       "                      [-0.0059,  0.0193,  0.0023,  ...,  0.0036,  0.0002, -0.0096]])),\n",
       "             ('decoder.layers.5.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2358a0>),\n",
       "             ('decoder.layers.6.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2358f0>),\n",
       "             ('decoder.layers.6.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0129,  0.0192,  0.0019,  ...,  0.0020,  0.0090, -0.0054],\n",
       "                      [ 0.0060, -0.0025,  0.0097,  ...,  0.0097,  0.0012,  0.0014],\n",
       "                      [-0.0094, -0.0080,  0.0162,  ...,  0.0016,  0.0052, -0.0109],\n",
       "                      ...,\n",
       "                      [-0.0126, -0.0028,  0.0109,  ...,  0.0162,  0.0012,  0.0025],\n",
       "                      [-0.0073,  0.0121,  0.0005,  ...,  0.0006, -0.0007,  0.0022],\n",
       "                      [ 0.0166,  0.0061,  0.0003,  ...,  0.0028,  0.0043, -0.0048]])),\n",
       "             ('decoder.layers.6.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235990>),\n",
       "             ('decoder.layers.6.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4004, 0.4160, 0.3945,  ..., 0.4648, 0.3242, 0.3164])),\n",
       "             ('decoder.layers.6.self_attention.linear_qkv.weight',\n",
       "              tensor([[-0.0098,  0.0156, -0.0015,  ..., -0.0002, -0.0009, -0.0031],\n",
       "                      [-0.0142, -0.0012,  0.0046,  ..., -0.0134, -0.0005,  0.0078],\n",
       "                      [ 0.0113, -0.0079,  0.0070,  ..., -0.0215, -0.0156, -0.0142],\n",
       "                      ...,\n",
       "                      [ 0.0017, -0.0006,  0.0017,  ...,  0.0115, -0.0036,  0.0029],\n",
       "                      [-0.0056,  0.0115, -0.0015,  ..., -0.0061, -0.0063, -0.0005],\n",
       "                      [-0.0145, -0.0027,  0.0042,  ..., -0.0036, -0.0041,  0.0017]])),\n",
       "             ('decoder.layers.6.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235a80>),\n",
       "             ('decoder.layers.6.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.2988, 0.3008, 0.3105,  ..., 0.2500, 0.3145, 0.3027])),\n",
       "             ('decoder.layers.6.mlp.linear_fc1.weight',\n",
       "              tensor([[-5.4321e-03, -9.5825e-03, -1.1597e-03,  ..., -1.1658e-02,\n",
       "                        7.2479e-04,  5.3101e-03],\n",
       "                      [ 1.5137e-02,  1.1841e-02,  2.4414e-02,  ..., -1.0132e-02,\n",
       "                        2.2217e-02, -3.0396e-02],\n",
       "                      [ 2.8839e-03, -4.1809e-03,  7.9727e-04,  ..., -1.3916e-02,\n",
       "                        5.8899e-03,  6.6833e-03],\n",
       "                      ...,\n",
       "                      [-9.0408e-04, -5.6028e-05, -4.2725e-03,  ...,  9.6130e-04,\n",
       "                        6.0425e-03, -3.3417e-03],\n",
       "                      [-3.6316e-03,  2.5940e-03,  1.2207e-02,  ..., -1.2634e-02,\n",
       "                        9.3994e-03,  1.1047e-02],\n",
       "                      [-1.4709e-02, -2.6245e-02, -3.3188e-04,  ..., -7.7820e-03,\n",
       "                        1.0620e-02, -7.9956e-03]])),\n",
       "             ('decoder.layers.6.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235b70>),\n",
       "             ('decoder.layers.6.mlp.linear_fc2.weight',\n",
       "              tensor([[ 8.1787e-03,  8.4305e-04, -1.3351e-03,  ...,  5.2795e-03,\n",
       "                       -1.4160e-02,  4.8523e-03],\n",
       "                      [-1.6357e-02, -6.4697e-03,  6.2256e-03,  ...,  1.3367e-02,\n",
       "                        5.8594e-03, -4.7913e-03],\n",
       "                      [-1.2817e-02, -4.6997e-03,  3.0708e-04,  ...,  6.8970e-03,\n",
       "                        2.0027e-05, -1.1169e-02],\n",
       "                      ...,\n",
       "                      [-1.6602e-02, -1.8311e-03, -7.9346e-03,  ...,  9.1553e-03,\n",
       "                       -1.1658e-02, -9.1553e-03],\n",
       "                      [-1.9043e-02,  1.4099e-02,  7.6599e-03,  ...,  6.8665e-04,\n",
       "                        8.3008e-03,  1.0132e-02],\n",
       "                      [-1.1292e-03, -8.6670e-03,  9.9487e-03,  ...,  8.4229e-03,\n",
       "                       -1.0498e-02,  4.3335e-03]])),\n",
       "             ('decoder.layers.6.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235c10>),\n",
       "             ('decoder.layers.7.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235c60>),\n",
       "             ('decoder.layers.7.self_attention.linear_proj.weight',\n",
       "              tensor([[-0.0017,  0.0184,  0.0101,  ..., -0.0053, -0.0055,  0.0132],\n",
       "                      [ 0.0005, -0.0094, -0.0034,  ..., -0.0071,  0.0036, -0.0047],\n",
       "                      [ 0.0063, -0.0025,  0.0096,  ..., -0.0023,  0.0035,  0.0222],\n",
       "                      ...,\n",
       "                      [ 0.0036,  0.0014, -0.0070,  ..., -0.0017,  0.0157,  0.0031],\n",
       "                      [ 0.0096, -0.0011, -0.0021,  ...,  0.0116,  0.0046,  0.0074],\n",
       "                      [-0.0117,  0.0103, -0.0035,  ..., -0.0051,  0.0029,  0.0022]])),\n",
       "             ('decoder.layers.7.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235d00>),\n",
       "             ('decoder.layers.7.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4043, 0.4551, 0.4141,  ..., 0.4238, 0.3340, 0.3223])),\n",
       "             ('decoder.layers.7.self_attention.linear_qkv.weight',\n",
       "              tensor([[ 5.2795e-03,  4.6997e-03, -4.2114e-03,  ..., -1.2207e-02,\n",
       "                       -2.8992e-03, -5.3101e-03],\n",
       "                      [-8.3618e-03,  7.0190e-03, -1.7090e-02,  ..., -1.5564e-02,\n",
       "                       -4.9133e-03, -4.6692e-03],\n",
       "                      [ 3.1281e-03, -1.3550e-02,  6.7520e-04,  ..., -8.7891e-03,\n",
       "                        6.9580e-03,  4.6692e-03],\n",
       "                      ...,\n",
       "                      [ 8.2397e-03,  5.4016e-03,  9.8705e-05,  ..., -1.1673e-03,\n",
       "                        2.4567e-03, -1.0071e-02],\n",
       "                      [ 1.6251e-03, -7.2937e-03,  1.1475e-02,  ...,  4.9133e-03,\n",
       "                       -3.4027e-03,  9.4604e-03],\n",
       "                      [-1.0376e-02, -8.1787e-03, -1.2390e-02,  ...,  5.8594e-03,\n",
       "                        1.0071e-02, -5.0964e-03]])),\n",
       "             ('decoder.layers.7.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235df0>),\n",
       "             ('decoder.layers.7.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.3027, 0.3047, 0.3125,  ..., 0.2578, 0.3125, 0.2949])),\n",
       "             ('decoder.layers.7.mlp.linear_fc1.weight',\n",
       "              tensor([[ 0.0259,  0.0004,  0.0115,  ..., -0.0153, -0.0053,  0.0100],\n",
       "                      [-0.0069,  0.0118,  0.0209,  ..., -0.0143, -0.0025,  0.0168],\n",
       "                      [ 0.0001,  0.0058,  0.0006,  ...,  0.0105,  0.0054, -0.0008],\n",
       "                      ...,\n",
       "                      [-0.0126, -0.0061, -0.0025,  ...,  0.0197,  0.0024,  0.0086],\n",
       "                      [-0.0023, -0.0125,  0.0077,  ..., -0.0005,  0.0208,  0.0043],\n",
       "                      [-0.0088, -0.0201, -0.0081,  ...,  0.0027, -0.0149,  0.0040]])),\n",
       "             ('decoder.layers.7.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235ee0>),\n",
       "             ('decoder.layers.7.mlp.linear_fc2.weight',\n",
       "              tensor([[-0.0065, -0.0227,  0.0066,  ..., -0.0081,  0.0084, -0.0053],\n",
       "                      [-0.0082,  0.0029, -0.0065,  ...,  0.0146, -0.0052, -0.0154],\n",
       "                      [-0.0055,  0.0076, -0.0035,  ..., -0.0037,  0.0096,  0.0167],\n",
       "                      ...,\n",
       "                      [ 0.0123, -0.0113, -0.0011,  ...,  0.0031,  0.0036, -0.0099],\n",
       "                      [-0.0053, -0.0173, -0.0018,  ...,  0.0179, -0.0005, -0.0082],\n",
       "                      [-0.0096,  0.0195, -0.0007,  ...,  0.0086,  0.0083, -0.0023]])),\n",
       "             ('decoder.layers.7.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235f80>),\n",
       "             ('decoder.layers.8.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f235fd0>),\n",
       "             ('decoder.layers.8.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0096,  0.0090, -0.0067,  ...,  0.0066,  0.0104,  0.0112],\n",
       "                      [-0.0251,  0.0053, -0.0168,  ..., -0.0167,  0.0042, -0.0056],\n",
       "                      [ 0.0027,  0.0112, -0.0061,  ...,  0.0015,  0.0044,  0.0060],\n",
       "                      ...,\n",
       "                      [ 0.0020,  0.0012,  0.0050,  ...,  0.0017, -0.0081,  0.0015],\n",
       "                      [-0.0091,  0.0078,  0.0040,  ..., -0.0052, -0.0016, -0.0040],\n",
       "                      [ 0.0049,  0.0060,  0.0051,  ..., -0.0025, -0.0097, -0.0023]])),\n",
       "             ('decoder.layers.8.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236070>),\n",
       "             ('decoder.layers.8.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4258, 0.4668, 0.4512,  ..., 0.4316, 0.3281, 0.3379])),\n",
       "             ('decoder.layers.8.self_attention.linear_qkv.weight',\n",
       "              tensor([[-0.0074, -0.0166,  0.0028,  ...,  0.0008, -0.0066,  0.0116],\n",
       "                      [ 0.0112, -0.0211,  0.0008,  ...,  0.0194, -0.0116, -0.0056],\n",
       "                      [ 0.0009, -0.0139, -0.0002,  ...,  0.0126, -0.0183, -0.0157],\n",
       "                      ...,\n",
       "                      [ 0.0039,  0.0003,  0.0048,  ...,  0.0057, -0.0054,  0.0043],\n",
       "                      [ 0.0046,  0.0030, -0.0013,  ..., -0.0040, -0.0008, -0.0188],\n",
       "                      [ 0.0109, -0.0137,  0.0032,  ..., -0.0017,  0.0015, -0.0070]])),\n",
       "             ('decoder.layers.8.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236160>),\n",
       "             ('decoder.layers.8.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.3047, 0.3047, 0.3125,  ..., 0.2676, 0.3125, 0.3047])),\n",
       "             ('decoder.layers.8.mlp.linear_fc1.weight',\n",
       "              tensor([[-0.0211, -0.0114,  0.0132,  ..., -0.0098, -0.0075,  0.0074],\n",
       "                      [-0.0282, -0.0011, -0.0181,  ...,  0.0099, -0.0062,  0.0201],\n",
       "                      [-0.0057, -0.0017,  0.0035,  ...,  0.0045,  0.0020, -0.0106],\n",
       "                      ...,\n",
       "                      [-0.0063,  0.0093,  0.0139,  ..., -0.0029, -0.0168, -0.0059],\n",
       "                      [ 0.0055, -0.0037, -0.0079,  ...,  0.0114, -0.0011, -0.0018],\n",
       "                      [ 0.0143,  0.0132,  0.0022,  ..., -0.0034,  0.0056, -0.0083]])),\n",
       "             ('decoder.layers.8.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236250>),\n",
       "             ('decoder.layers.8.mlp.linear_fc2.weight',\n",
       "              tensor([[-0.0001, -0.0043,  0.0062,  ..., -0.0014,  0.0094, -0.0051],\n",
       "                      [ 0.0112,  0.0069, -0.0071,  ...,  0.0012,  0.0075, -0.0071],\n",
       "                      [-0.0095,  0.0160,  0.0021,  ...,  0.0053, -0.0111, -0.0045],\n",
       "                      ...,\n",
       "                      [ 0.0014, -0.0178, -0.0236,  ...,  0.0047,  0.0184,  0.0083],\n",
       "                      [ 0.0086,  0.0120, -0.0154,  ..., -0.0120,  0.0032,  0.0120],\n",
       "                      [-0.0008,  0.0065,  0.0099,  ...,  0.0033, -0.0116, -0.0172]])),\n",
       "             ('decoder.layers.8.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2362f0>),\n",
       "             ('decoder.layers.9.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236340>),\n",
       "             ('decoder.layers.9.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0029,  0.0156, -0.0114,  ...,  0.0123,  0.0037, -0.0040],\n",
       "                      [-0.0055,  0.0154, -0.0038,  ...,  0.0083,  0.0059,  0.0135],\n",
       "                      [-0.0254, -0.0264,  0.0038,  ..., -0.0040, -0.0042,  0.0046],\n",
       "                      ...,\n",
       "                      [ 0.0035,  0.0073,  0.0064,  ..., -0.0013,  0.0009, -0.0033],\n",
       "                      [-0.0030, -0.0140,  0.0074,  ..., -0.0023, -0.0033,  0.0025],\n",
       "                      [ 0.0012, -0.0051,  0.0048,  ..., -0.0070, -0.0040,  0.0107]])),\n",
       "             ('decoder.layers.9.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2363e0>),\n",
       "             ('decoder.layers.9.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4180, 0.4863, 0.4668,  ..., 0.3984, 0.3340, 0.3223])),\n",
       "             ('decoder.layers.9.self_attention.linear_qkv.weight',\n",
       "              tensor([[ 0.0052,  0.0275,  0.0001,  ...,  0.0025,  0.0208,  0.0255],\n",
       "                      [ 0.0050,  0.0103,  0.0011,  ..., -0.0168, -0.0104,  0.0143],\n",
       "                      [-0.0238, -0.0129,  0.0055,  ...,  0.0079, -0.0396, -0.0049],\n",
       "                      ...,\n",
       "                      [ 0.0103,  0.0039, -0.0089,  ...,  0.0002,  0.0023, -0.0145],\n",
       "                      [-0.0052, -0.0010, -0.0002,  ..., -0.0038,  0.0006,  0.0039],\n",
       "                      [-0.0008,  0.0116,  0.0027,  ..., -0.0032,  0.0004,  0.0110]])),\n",
       "             ('decoder.layers.9.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2364d0>),\n",
       "             ('decoder.layers.9.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.3027, 0.3027, 0.3105,  ..., 0.2734, 0.3223, 0.3027])),\n",
       "             ('decoder.layers.9.mlp.linear_fc1.weight',\n",
       "              tensor([[ 0.0008,  0.0184,  0.0170,  ...,  0.0003,  0.0042, -0.0008],\n",
       "                      [ 0.0024,  0.0042, -0.0033,  ..., -0.0065,  0.0024, -0.0044],\n",
       "                      [ 0.0156,  0.0030, -0.0069,  ...,  0.0300,  0.0226,  0.0027],\n",
       "                      ...,\n",
       "                      [ 0.0038, -0.0012, -0.0044,  ..., -0.0066,  0.0011,  0.0209],\n",
       "                      [-0.0057,  0.0007,  0.0033,  ..., -0.0077, -0.0042, -0.0129],\n",
       "                      [-0.0114, -0.0039,  0.0089,  ...,  0.0095, -0.0109,  0.0059]])),\n",
       "             ('decoder.layers.9.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2365c0>),\n",
       "             ('decoder.layers.9.mlp.linear_fc2.weight',\n",
       "              tensor([[ 1.8433e-02,  1.4954e-02,  1.9226e-03,  ...,  9.4604e-04,\n",
       "                       -7.6294e-03,  2.4719e-03],\n",
       "                      [ 6.3782e-03,  8.3008e-03,  1.7452e-04,  ...,  1.8692e-03,\n",
       "                       -1.6846e-02, -1.4771e-02],\n",
       "                      [ 2.5269e-02,  8.9722e-03,  5.3406e-03,  ..., -1.8539e-03,\n",
       "                        8.0566e-03,  1.3542e-04],\n",
       "                      ...,\n",
       "                      [ 1.0437e-02,  5.4626e-03,  1.2085e-02,  ...,  8.7891e-03,\n",
       "                       -7.8735e-03,  5.0049e-03],\n",
       "                      [-1.1047e-02, -1.2512e-02,  6.1340e-03,  ...,  1.0132e-02,\n",
       "                       -1.7334e-02, -1.9455e-04],\n",
       "                      [-3.1710e-05,  1.1902e-02, -1.9531e-03,  ..., -2.1362e-02,\n",
       "                       -1.1475e-02,  1.7944e-02]])),\n",
       "             ('decoder.layers.9.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236660>),\n",
       "             ('decoder.layers.10.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2366b0>),\n",
       "             ('decoder.layers.10.self_attention.linear_proj.weight',\n",
       "              tensor([[-0.0100,  0.0029, -0.0039,  ...,  0.0040, -0.0011, -0.0092],\n",
       "                      [ 0.0091, -0.0033, -0.0017,  ..., -0.0026, -0.0037,  0.0128],\n",
       "                      [ 0.0118, -0.0013, -0.0064,  ...,  0.0125, -0.0053, -0.0024],\n",
       "                      ...,\n",
       "                      [ 0.0145,  0.0073,  0.0103,  ...,  0.0044,  0.0047,  0.0058],\n",
       "                      [-0.0021,  0.0045, -0.0064,  ...,  0.0051,  0.0036,  0.0039],\n",
       "                      [-0.0114, -0.0013, -0.0011,  ...,  0.0085,  0.0072, -0.0030]])),\n",
       "             ('decoder.layers.10.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236750>),\n",
       "             ('decoder.layers.10.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4277, 0.4922, 0.4746,  ..., 0.4043, 0.3379, 0.3398])),\n",
       "             ('decoder.layers.10.self_attention.linear_qkv.weight',\n",
       "              tensor([[-0.0195,  0.0122, -0.0154,  ..., -0.0003,  0.0058,  0.0007],\n",
       "                      [-0.0403,  0.0096, -0.0281,  ...,  0.0051, -0.0222, -0.0381],\n",
       "                      [-0.0050,  0.0190, -0.0074,  ..., -0.0007, -0.0177,  0.0084],\n",
       "                      ...,\n",
       "                      [-0.0156,  0.0025,  0.0008,  ..., -0.0026,  0.0077,  0.0015],\n",
       "                      [-0.0065,  0.0056, -0.0062,  ..., -0.0024, -0.0011,  0.0073],\n",
       "                      [-0.0018,  0.0074, -0.0024,  ...,  0.0038, -0.0057,  0.0013]])),\n",
       "             ('decoder.layers.10.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236840>),\n",
       "             ('decoder.layers.10.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.3125, 0.3047, 0.3164,  ..., 0.2773, 0.3125, 0.3027])),\n",
       "             ('decoder.layers.10.mlp.linear_fc1.weight',\n",
       "              tensor([[-0.0046, -0.0064, -0.0020,  ...,  0.0096,  0.0101,  0.0067],\n",
       "                      [ 0.0049, -0.0004, -0.0327,  ...,  0.0123,  0.0084, -0.0120],\n",
       "                      [-0.0113,  0.0060,  0.0038,  ...,  0.0040,  0.0305, -0.0031],\n",
       "                      ...,\n",
       "                      [ 0.0029,  0.0128, -0.0052,  ...,  0.0201, -0.0069,  0.0112],\n",
       "                      [-0.0008,  0.0225, -0.0057,  ..., -0.0154, -0.0120,  0.0043],\n",
       "                      [ 0.0132,  0.0044,  0.0008,  ..., -0.0059, -0.0083, -0.0038]])),\n",
       "             ('decoder.layers.10.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236930>),\n",
       "             ('decoder.layers.10.mlp.linear_fc2.weight',\n",
       "              tensor([[ 0.0087, -0.0116,  0.0093,  ..., -0.0098, -0.0078,  0.0068],\n",
       "                      [-0.0059,  0.0089,  0.0061,  ...,  0.0025,  0.0203, -0.0133],\n",
       "                      [-0.0131,  0.0167,  0.0060,  ..., -0.0065, -0.0233, -0.0004],\n",
       "                      ...,\n",
       "                      [ 0.0049, -0.0145, -0.0013,  ..., -0.0186, -0.0131,  0.0057],\n",
       "                      [ 0.0093, -0.0025,  0.0080,  ..., -0.0063, -0.0036, -0.0084],\n",
       "                      [ 0.0131,  0.0210,  0.0031,  ..., -0.0077, -0.0133, -0.0090]])),\n",
       "             ('decoder.layers.10.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2369d0>),\n",
       "             ('decoder.layers.11.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236a20>),\n",
       "             ('decoder.layers.11.self_attention.linear_proj.weight',\n",
       "              tensor([[-1.3489e-02,  2.1820e-03,  3.0212e-03,  ...,  1.6357e-02,\n",
       "                       -9.0942e-03, -6.9885e-03],\n",
       "                      [ 4.6692e-03,  8.0566e-03,  1.4160e-02,  ..., -7.6294e-03,\n",
       "                        1.0559e-02, -7.6771e-05],\n",
       "                      [ 9.9945e-04, -1.3245e-02, -4.7913e-03,  ...,  7.5684e-03,\n",
       "                       -1.2665e-03, -1.0681e-02],\n",
       "                      ...,\n",
       "                      [-3.4790e-03, -5.8899e-03, -7.7209e-03,  ...,  1.5381e-02,\n",
       "                       -6.9427e-04,  7.2861e-04],\n",
       "                      [-3.2654e-03,  7.5912e-04, -2.0630e-02,  ...,  4.6692e-03,\n",
       "                       -6.6223e-03,  4.6349e-04],\n",
       "                      [ 1.8066e-02,  4.4556e-03, -7.7820e-03,  ...,  3.6011e-03,\n",
       "                       -1.0864e-02,  1.8921e-03]])),\n",
       "             ('decoder.layers.11.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236ac0>),\n",
       "             ('decoder.layers.11.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4629, 0.5039, 0.4824,  ..., 0.3789, 0.3496, 0.3418])),\n",
       "             ('decoder.layers.11.self_attention.linear_qkv.weight',\n",
       "              tensor([[-0.0096,  0.0144,  0.0040,  ..., -0.0082, -0.0140, -0.0215],\n",
       "                      [ 0.0013, -0.0046,  0.0087,  ..., -0.0142, -0.0070,  0.0023],\n",
       "                      [ 0.0239, -0.0195, -0.0043,  ...,  0.0231,  0.0076,  0.0132],\n",
       "                      ...,\n",
       "                      [-0.0014, -0.0054,  0.0040,  ..., -0.0016, -0.0014,  0.0056],\n",
       "                      [ 0.0032, -0.0017,  0.0039,  ..., -0.0071, -0.0037, -0.0090],\n",
       "                      [-0.0017,  0.0101, -0.0141,  ...,  0.0090, -0.0032, -0.0114]])),\n",
       "             ('decoder.layers.11.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236bb0>),\n",
       "             ('decoder.layers.11.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.3164, 0.3047, 0.3164,  ..., 0.2871, 0.3086, 0.2949])),\n",
       "             ('decoder.layers.11.mlp.linear_fc1.weight',\n",
       "              tensor([[-0.0005,  0.0067,  0.0014,  ..., -0.0008,  0.0020, -0.0017],\n",
       "                      [-0.0011,  0.0015, -0.0208,  ...,  0.0106,  0.0063,  0.0006],\n",
       "                      [ 0.0052, -0.0059,  0.0134,  ..., -0.0115, -0.0123,  0.0066],\n",
       "                      ...,\n",
       "                      [ 0.0030, -0.0129,  0.0035,  ..., -0.0047, -0.0081, -0.0064],\n",
       "                      [ 0.0054, -0.0096,  0.0093,  ..., -0.0206,  0.0145,  0.0120],\n",
       "                      [-0.0098,  0.0014,  0.0121,  ..., -0.0071, -0.0031,  0.0175]])),\n",
       "             ('decoder.layers.11.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236ca0>),\n",
       "             ('decoder.layers.11.mlp.linear_fc2.weight',\n",
       "              tensor([[ 0.0037, -0.0074, -0.0059,  ..., -0.0229, -0.0067,  0.0142],\n",
       "                      [-0.0103,  0.0002, -0.0026,  ...,  0.0008, -0.0049,  0.0154],\n",
       "                      [-0.0012,  0.0107,  0.0017,  ..., -0.0039,  0.0041, -0.0192],\n",
       "                      ...,\n",
       "                      [-0.0118,  0.0013,  0.0052,  ..., -0.0035,  0.0099,  0.0055],\n",
       "                      [-0.0093, -0.0085, -0.0107,  ...,  0.0106,  0.0041, -0.0089],\n",
       "                      [-0.0036,  0.0046, -0.0011,  ...,  0.0075,  0.0016, -0.0071]])),\n",
       "             ('decoder.layers.11.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236d40>),\n",
       "             ('decoder.layers.12.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236d90>),\n",
       "             ('decoder.layers.12.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0105, -0.0175,  0.0053,  ...,  0.0028,  0.0262,  0.0072],\n",
       "                      [ 0.0013, -0.0018,  0.0101,  ..., -0.0032,  0.0128,  0.0023],\n",
       "                      [-0.0078,  0.0162, -0.0027,  ..., -0.0037, -0.0068,  0.0087],\n",
       "                      ...,\n",
       "                      [-0.0001,  0.0055,  0.0064,  ...,  0.0074, -0.0195, -0.0124],\n",
       "                      [ 0.0082, -0.0234, -0.0013,  ..., -0.0211,  0.0006, -0.0082],\n",
       "                      [-0.0079,  0.0065, -0.0087,  ...,  0.0075, -0.0081,  0.0249]])),\n",
       "             ('decoder.layers.12.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236e30>),\n",
       "             ('decoder.layers.12.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4355, 0.4570, 0.4355,  ..., 0.3516, 0.3262, 0.3301])),\n",
       "             ('decoder.layers.12.self_attention.linear_qkv.weight',\n",
       "              tensor([[ 0.0033,  0.0079,  0.0029,  ...,  0.0190, -0.0141,  0.0128],\n",
       "                      [ 0.0154, -0.0145,  0.0108,  ...,  0.0052, -0.0014, -0.0092],\n",
       "                      [-0.0024,  0.0159, -0.0003,  ..., -0.0080,  0.0126,  0.0231],\n",
       "                      ...,\n",
       "                      [-0.0011, -0.0008, -0.0012,  ...,  0.0076,  0.0077, -0.0037],\n",
       "                      [-0.0047, -0.0040,  0.0020,  ..., -0.0084,  0.0031, -0.0012],\n",
       "                      [-0.0057,  0.0005, -0.0063,  ..., -0.0183, -0.0038, -0.0028]])),\n",
       "             ('decoder.layers.12.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f236f20>),\n",
       "             ('decoder.layers.12.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.3066, 0.3027, 0.3105,  ..., 0.2891, 0.3066, 0.2930])),\n",
       "             ('decoder.layers.12.mlp.linear_fc1.weight',\n",
       "              tensor([[-0.0094, -0.0015, -0.0035,  ..., -0.0039, -0.0239, -0.0013],\n",
       "                      [-0.0038, -0.0085, -0.0037,  ..., -0.0034,  0.0033,  0.0046],\n",
       "                      [-0.0104, -0.0135,  0.0066,  ..., -0.0140, -0.0020, -0.0058],\n",
       "                      ...,\n",
       "                      [-0.0118, -0.0039, -0.0013,  ..., -0.0165,  0.0175, -0.0084],\n",
       "                      [-0.0026, -0.0101,  0.0077,  ..., -0.0282, -0.0058,  0.0041],\n",
       "                      [-0.0026, -0.0018,  0.0147,  ..., -0.0095, -0.0020,  0.0164]])),\n",
       "             ('decoder.layers.12.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237010>),\n",
       "             ('decoder.layers.12.mlp.linear_fc2.weight',\n",
       "              tensor([[ 0.0095, -0.0103,  0.0050,  ..., -0.0018, -0.0058, -0.0072],\n",
       "                      [-0.0167,  0.0026, -0.0103,  ..., -0.0070,  0.0100,  0.0054],\n",
       "                      [-0.0026, -0.0231, -0.0072,  ...,  0.0090,  0.0208,  0.0036],\n",
       "                      ...,\n",
       "                      [-0.0011,  0.0088,  0.0050,  ..., -0.0083, -0.0222, -0.0173],\n",
       "                      [-0.0141, -0.0167, -0.0203,  ...,  0.0146, -0.0101,  0.0048],\n",
       "                      [ 0.0037,  0.0109, -0.0303,  ..., -0.0214, -0.0020,  0.0079]])),\n",
       "             ('decoder.layers.12.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2370b0>),\n",
       "             ('decoder.layers.13.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237100>),\n",
       "             ('decoder.layers.13.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0194,  0.0181,  0.0006,  ...,  0.0140,  0.0010,  0.0053],\n",
       "                      [-0.0092, -0.0036,  0.0098,  ...,  0.0036,  0.0045, -0.0125],\n",
       "                      [-0.0170,  0.0046, -0.0033,  ...,  0.0006,  0.0046, -0.0076],\n",
       "                      ...,\n",
       "                      [ 0.0002, -0.0118,  0.0014,  ..., -0.0070, -0.0057,  0.0085],\n",
       "                      [ 0.0004, -0.0019, -0.0076,  ...,  0.0091,  0.0010,  0.0105],\n",
       "                      [ 0.0032, -0.0097,  0.0007,  ..., -0.0050, -0.0077,  0.0014]])),\n",
       "             ('decoder.layers.13.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2371a0>),\n",
       "             ('decoder.layers.13.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4648, 0.5352, 0.5000,  ..., 0.3906, 0.3789, 0.3770])),\n",
       "             ('decoder.layers.13.self_attention.linear_qkv.weight',\n",
       "              tensor([[-0.0099, -0.0032,  0.0062,  ..., -0.0120,  0.0001, -0.0119],\n",
       "                      [ 0.0096,  0.0051, -0.0060,  ...,  0.0081, -0.0013,  0.0077],\n",
       "                      [-0.0115,  0.0036, -0.0033,  ..., -0.0017,  0.0011, -0.0122],\n",
       "                      ...,\n",
       "                      [ 0.0106,  0.0037,  0.0079,  ..., -0.0036, -0.0049, -0.0183],\n",
       "                      [ 0.0028,  0.0081,  0.0029,  ..., -0.0045, -0.0035, -0.0093],\n",
       "                      [-0.0103, -0.0065,  0.0086,  ...,  0.0108,  0.0134,  0.0133]])),\n",
       "             ('decoder.layers.13.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237290>),\n",
       "             ('decoder.layers.13.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.3164, 0.3145, 0.3203,  ..., 0.3086, 0.3262, 0.3125])),\n",
       "             ('decoder.layers.13.mlp.linear_fc1.weight',\n",
       "              tensor([[ 0.0055,  0.0243, -0.0154,  ..., -0.0046, -0.0088,  0.0251],\n",
       "                      [-0.0034, -0.0046,  0.0010,  ...,  0.0237,  0.0027,  0.0070],\n",
       "                      [ 0.0095,  0.0130,  0.0087,  ...,  0.0082, -0.0058,  0.0160],\n",
       "                      ...,\n",
       "                      [-0.0004, -0.0122, -0.0200,  ...,  0.0126, -0.0071,  0.0007],\n",
       "                      [-0.0017,  0.0004, -0.0027,  ..., -0.0123,  0.0050,  0.0068],\n",
       "                      [-0.0225,  0.0014, -0.0083,  ...,  0.0060,  0.0073,  0.0051]])),\n",
       "             ('decoder.layers.13.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237380>),\n",
       "             ('decoder.layers.13.mlp.linear_fc2.weight',\n",
       "              tensor([[-4.3335e-03,  1.0010e-02, -1.1108e-02,  ...,  4.4107e-05,\n",
       "                        1.6403e-03,  4.8218e-03],\n",
       "                      [-1.1536e-02, -1.2939e-02, -4.1199e-03,  ..., -2.3956e-03,\n",
       "                        2.7313e-03,  6.2866e-03],\n",
       "                      [-8.1787e-03, -5.2490e-03, -9.8419e-04,  ...,  8.6060e-03,\n",
       "                       -5.6763e-03, -3.1433e-03],\n",
       "                      ...,\n",
       "                      [-1.3550e-02, -2.6367e-02, -2.8534e-03,  ...,  4.6387e-03,\n",
       "                       -3.9673e-03,  7.7438e-04],\n",
       "                      [ 1.4465e-02,  7.0190e-03, -5.3406e-03,  ...,  9.7656e-03,\n",
       "                        6.9885e-03,  1.4282e-02],\n",
       "                      [-1.4648e-02, -2.5269e-02,  8.0566e-03,  ...,  2.8038e-04,\n",
       "                       -9.7656e-03,  1.4160e-02]])),\n",
       "             ('decoder.layers.13.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237420>),\n",
       "             ('decoder.layers.14.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237470>),\n",
       "             ('decoder.layers.14.self_attention.linear_proj.weight',\n",
       "              tensor([[-1.3306e-02, -9.7656e-03, -2.0885e-04,  ...,  1.1047e-02,\n",
       "                        3.7384e-03, -5.0049e-03],\n",
       "                      [ 1.7700e-02,  7.9346e-03, -1.3245e-02,  ..., -7.2937e-03,\n",
       "                       -8.7357e-04,  8.5235e-06],\n",
       "                      [-1.1902e-03,  3.9673e-03,  8.1253e-04,  ...,  1.6602e-02,\n",
       "                       -1.0437e-02,  4.9744e-03],\n",
       "                      ...,\n",
       "                      [ 3.4790e-03,  1.3275e-03, -6.0120e-03,  ...,  9.6436e-03,\n",
       "                       -6.1035e-03, -8.8501e-03],\n",
       "                      [-7.5378e-03, -5.0354e-03,  8.0566e-03,  ..., -6.1951e-03,\n",
       "                        1.0925e-02,  1.2329e-02],\n",
       "                      [ 1.9897e-02, -3.6621e-03,  2.4986e-04,  ..., -8.3542e-04,\n",
       "                       -2.6550e-03, -1.7944e-02]])),\n",
       "             ('decoder.layers.14.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237510>),\n",
       "             ('decoder.layers.14.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4551, 0.4883, 0.4980,  ..., 0.3945, 0.3789, 0.3750])),\n",
       "             ('decoder.layers.14.self_attention.linear_qkv.weight',\n",
       "              tensor([[ 0.0054,  0.0109,  0.0177,  ..., -0.0011, -0.0032,  0.0026],\n",
       "                      [ 0.0014, -0.0280,  0.0054,  ...,  0.0166, -0.0125,  0.0127],\n",
       "                      [-0.0005,  0.0110,  0.0054,  ...,  0.0025,  0.0031, -0.0050],\n",
       "                      ...,\n",
       "                      [ 0.0053,  0.0104,  0.0040,  ...,  0.0008, -0.0082,  0.0293],\n",
       "                      [-0.0106,  0.0024,  0.0051,  ...,  0.0048, -0.0012, -0.0122],\n",
       "                      [-0.0053,  0.0020,  0.0079,  ..., -0.0033, -0.0139,  0.0093]])),\n",
       "             ('decoder.layers.14.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237600>),\n",
       "             ('decoder.layers.14.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.3359, 0.3262, 0.3359,  ..., 0.3223, 0.3457, 0.3359])),\n",
       "             ('decoder.layers.14.mlp.linear_fc1.weight',\n",
       "              tensor([[-0.0056,  0.0272, -0.0054,  ...,  0.0003,  0.0134,  0.0074],\n",
       "                      [ 0.0161,  0.0043, -0.0004,  ...,  0.0143,  0.0004, -0.0018],\n",
       "                      [-0.0027, -0.0136,  0.0072,  ...,  0.0053,  0.0100, -0.0102],\n",
       "                      ...,\n",
       "                      [ 0.0139, -0.0061,  0.0070,  ..., -0.0214, -0.0021,  0.0281],\n",
       "                      [ 0.0240, -0.0026,  0.0005,  ..., -0.0022, -0.0011,  0.0073],\n",
       "                      [ 0.0048,  0.0103, -0.0075,  ..., -0.0007,  0.0033, -0.0040]])),\n",
       "             ('decoder.layers.14.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2376f0>),\n",
       "             ('decoder.layers.14.mlp.linear_fc2.weight',\n",
       "              tensor([[ 0.0010, -0.0182,  0.0077,  ...,  0.0007,  0.0076, -0.0099],\n",
       "                      [-0.0278, -0.0125,  0.0176,  ...,  0.0019,  0.0074,  0.0189],\n",
       "                      [ 0.0107, -0.0091, -0.0017,  ..., -0.0067,  0.0239, -0.0074],\n",
       "                      ...,\n",
       "                      [ 0.0113,  0.0125, -0.0068,  ..., -0.0098, -0.0144,  0.0054],\n",
       "                      [-0.0037,  0.0045, -0.0112,  ..., -0.0164, -0.0023,  0.0078],\n",
       "                      [-0.0016,  0.0205, -0.0082,  ...,  0.0222, -0.0190, -0.0053]])),\n",
       "             ('decoder.layers.14.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237790>),\n",
       "             ('decoder.layers.15.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2377e0>),\n",
       "             ('decoder.layers.15.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0126,  0.0017,  0.0011,  ...,  0.0010, -0.0103, -0.0084],\n",
       "                      [ 0.0027,  0.0085, -0.0016,  ...,  0.0030, -0.0005, -0.0173],\n",
       "                      [ 0.0134,  0.0063,  0.0014,  ..., -0.0015,  0.0002,  0.0072],\n",
       "                      ...,\n",
       "                      [ 0.0130, -0.0028, -0.0041,  ..., -0.0002, -0.0124,  0.0014],\n",
       "                      [-0.0018, -0.0151, -0.0045,  ...,  0.0046,  0.0109, -0.0023],\n",
       "                      [-0.0051,  0.0015, -0.0019,  ..., -0.0066, -0.0067, -0.0003]])),\n",
       "             ('decoder.layers.15.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237880>),\n",
       "             ('decoder.layers.15.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4551, 0.4590, 0.4609,  ..., 0.4258, 0.3965, 0.4082])),\n",
       "             ('decoder.layers.15.self_attention.linear_qkv.weight',\n",
       "              tensor([[-2.1362e-03, -2.0142e-02,  2.1362e-02,  ...,  2.6978e-02,\n",
       "                        1.5259e-02, -1.8677e-02],\n",
       "                      [ 3.2959e-02, -1.7090e-02,  4.4434e-02,  ...,  7.9346e-03,\n",
       "                        9.8877e-03, -7.5989e-03],\n",
       "                      [-9.2030e-05, -9.4604e-03,  6.0730e-03,  ..., -2.5879e-02,\n",
       "                        1.1841e-02,  8.2397e-03],\n",
       "                      ...,\n",
       "                      [-6.8970e-03,  3.7842e-03, -2.9297e-03,  ...,  3.6621e-03,\n",
       "                       -5.7678e-03, -1.4305e-04],\n",
       "                      [-8.3618e-03, -7.4463e-03,  3.1891e-03,  ..., -8.9264e-04,\n",
       "                        5.0049e-03, -1.5747e-02],\n",
       "                      [-2.7924e-03,  2.7008e-03, -8.1787e-03,  ..., -7.0190e-03,\n",
       "                        3.7842e-03, -5.0964e-03]])),\n",
       "             ('decoder.layers.15.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237970>),\n",
       "             ('decoder.layers.15.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.3457, 0.3496, 0.3555,  ..., 0.3398, 0.3633, 0.3516])),\n",
       "             ('decoder.layers.15.mlp.linear_fc1.weight',\n",
       "              tensor([[-2.6855e-03, -3.7994e-03, -1.1826e-03,  ..., -3.4485e-03,\n",
       "                       -5.2490e-03, -1.3000e-02],\n",
       "                      [ 1.4526e-02,  7.6294e-03,  5.2214e-05,  ...,  2.3041e-03,\n",
       "                       -1.7944e-02, -1.7471e-03],\n",
       "                      [-3.4943e-03,  2.7618e-03, -3.3875e-03,  ...,  1.3123e-02,\n",
       "                        6.4392e-03, -1.2024e-02],\n",
       "                      ...,\n",
       "                      [-2.4261e-03, -2.2430e-03, -1.0147e-03,  ...,  2.0905e-03,\n",
       "                        5.2795e-03, -3.1891e-03],\n",
       "                      [ 6.9275e-03, -3.9978e-03,  3.7003e-04,  ..., -3.5553e-03,\n",
       "                       -2.1240e-02,  3.3936e-02],\n",
       "                      [-2.6398e-03, -8.9645e-04, -1.8188e-02,  ..., -7.9956e-03,\n",
       "                       -3.2501e-03,  3.7231e-03]])),\n",
       "             ('decoder.layers.15.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237a60>),\n",
       "             ('decoder.layers.15.mlp.linear_fc2.weight',\n",
       "              tensor([[ 0.0015,  0.0070,  0.0006,  ...,  0.0197,  0.0156, -0.0040],\n",
       "                      [ 0.0039, -0.0108, -0.0061,  ...,  0.0253,  0.0073,  0.0036],\n",
       "                      [-0.0157, -0.0110, -0.0102,  ...,  0.0055,  0.0096, -0.0057],\n",
       "                      ...,\n",
       "                      [ 0.0167, -0.0143,  0.0020,  ...,  0.0050, -0.0062,  0.0039],\n",
       "                      [ 0.0143, -0.0064, -0.0046,  ..., -0.0050,  0.0076, -0.0054],\n",
       "                      [-0.0107,  0.0078,  0.0057,  ...,  0.0164,  0.0115, -0.0074]])),\n",
       "             ('decoder.layers.15.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237ab0>),\n",
       "             ('decoder.layers.16.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237b00>),\n",
       "             ('decoder.layers.16.self_attention.linear_proj.weight',\n",
       "              tensor([[-0.0232,  0.0126,  0.0048,  ...,  0.0089,  0.0049,  0.0008],\n",
       "                      [-0.0413, -0.0055, -0.0079,  ..., -0.0027,  0.0094, -0.0056],\n",
       "                      [ 0.0215,  0.0028, -0.0068,  ..., -0.0006,  0.0071, -0.0029],\n",
       "                      ...,\n",
       "                      [ 0.0041, -0.0172, -0.0209,  ..., -0.0009,  0.0076,  0.0057],\n",
       "                      [-0.0128, -0.0146, -0.0078,  ..., -0.0077,  0.0048,  0.0069],\n",
       "                      [-0.0031, -0.0019, -0.0168,  ..., -0.0042, -0.0043, -0.0004]])),\n",
       "             ('decoder.layers.16.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237ba0>),\n",
       "             ('decoder.layers.16.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4355, 0.4375, 0.4355,  ..., 0.4355, 0.4219, 0.4004])),\n",
       "             ('decoder.layers.16.self_attention.linear_qkv.weight',\n",
       "              tensor([[-0.0024,  0.0074,  0.0005,  ..., -0.0055,  0.0060,  0.0069],\n",
       "                      [ 0.0021,  0.0096, -0.0036,  ...,  0.0017,  0.0053,  0.0085],\n",
       "                      [ 0.0066, -0.0008, -0.0024,  ..., -0.0055,  0.0095,  0.0026],\n",
       "                      ...,\n",
       "                      [ 0.0028, -0.0211, -0.0109,  ..., -0.0002,  0.0043, -0.0045],\n",
       "                      [-0.0044,  0.0001, -0.0018,  ...,  0.0028,  0.0080, -0.0096],\n",
       "                      [ 0.0036, -0.0030, -0.0001,  ..., -0.0008, -0.0083,  0.0066]])),\n",
       "             ('decoder.layers.16.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237c90>),\n",
       "             ('decoder.layers.16.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.3652, 0.3711, 0.3770,  ..., 0.3633, 0.3848, 0.3750])),\n",
       "             ('decoder.layers.16.mlp.linear_fc1.weight',\n",
       "              tensor([[ 0.0179,  0.0004, -0.0067,  ..., -0.0161,  0.0344, -0.0085],\n",
       "                      [-0.0270, -0.0025, -0.0143,  ...,  0.0140,  0.0192, -0.0322],\n",
       "                      [-0.0070,  0.0361,  0.0066,  ...,  0.0038,  0.0053, -0.0118],\n",
       "                      ...,\n",
       "                      [ 0.0145, -0.0031,  0.0075,  ..., -0.0064, -0.0073,  0.0090],\n",
       "                      [ 0.0109, -0.0068,  0.0142,  ..., -0.0010, -0.0092,  0.0048],\n",
       "                      [ 0.0240,  0.0204, -0.0108,  ...,  0.0195, -0.0124, -0.0104]])),\n",
       "             ('decoder.layers.16.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237d80>),\n",
       "             ('decoder.layers.16.mlp.linear_fc2.weight',\n",
       "              tensor([[-1.3916e-02,  8.4229e-03,  4.0894e-03,  ..., -2.6550e-03,\n",
       "                       -3.8605e-03,  3.1128e-02],\n",
       "                      [ 1.1749e-03, -3.5553e-03, -1.2131e-03,  ..., -1.2573e-02,\n",
       "                       -2.3499e-03,  1.3489e-02],\n",
       "                      [-1.5106e-03, -3.6478e-05,  2.2461e-02,  ...,  1.8921e-02,\n",
       "                        1.6785e-03,  9.0027e-04],\n",
       "                      ...,\n",
       "                      [ 3.0670e-03, -9.6436e-03,  1.9989e-03,  ..., -1.4099e-02,\n",
       "                       -1.6113e-02,  1.2146e-02],\n",
       "                      [ 4.8828e-03, -1.2817e-03, -8.5449e-03,  ..., -1.9455e-04,\n",
       "                        1.7395e-03,  2.8381e-03],\n",
       "                      [-1.7090e-03, -1.3199e-03,  1.0071e-02,  ..., -4.2114e-03,\n",
       "                       -6.4392e-03,  6.6223e-03]])),\n",
       "             ('decoder.layers.16.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237e20>),\n",
       "             ('decoder.layers.17.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237e70>),\n",
       "             ('decoder.layers.17.self_attention.linear_proj.weight',\n",
       "              tensor([[-0.0087,  0.0126,  0.0284,  ..., -0.0034,  0.0193, -0.0126],\n",
       "                      [ 0.0146,  0.0053, -0.0095,  ..., -0.0003,  0.0043, -0.0143],\n",
       "                      [ 0.0087,  0.0063,  0.0071,  ..., -0.0021,  0.0025,  0.0035],\n",
       "                      ...,\n",
       "                      [-0.0103,  0.0069, -0.0112,  ..., -0.0033,  0.0034,  0.0081],\n",
       "                      [ 0.0056,  0.0015,  0.0081,  ..., -0.0104, -0.0023,  0.0131],\n",
       "                      [-0.0115,  0.0018, -0.0011,  ..., -0.0014, -0.0064,  0.0178]])),\n",
       "             ('decoder.layers.17.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f237f10>),\n",
       "             ('decoder.layers.17.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4648, 0.4648, 0.4355,  ..., 0.4238, 0.4141, 0.4180])),\n",
       "             ('decoder.layers.17.self_attention.linear_qkv.weight',\n",
       "              tensor([[ 0.0223,  0.0012,  0.0179,  ..., -0.0034, -0.0073, -0.0013],\n",
       "                      [-0.0061,  0.0107,  0.0007,  ..., -0.0162, -0.0018, -0.0090],\n",
       "                      [-0.0074, -0.0068,  0.0011,  ...,  0.0112, -0.0152,  0.0084],\n",
       "                      ...,\n",
       "                      [-0.0034, -0.0096,  0.0035,  ...,  0.0017,  0.0054,  0.0081],\n",
       "                      [ 0.0175,  0.0060, -0.0099,  ..., -0.0030,  0.0082,  0.0021],\n",
       "                      [-0.0151, -0.0027, -0.0082,  ..., -0.0041,  0.0003, -0.0075]])),\n",
       "             ('decoder.layers.17.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248040>),\n",
       "             ('decoder.layers.17.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.3828, 0.3848, 0.3906,  ..., 0.3730, 0.4062, 0.3867])),\n",
       "             ('decoder.layers.17.mlp.linear_fc1.weight',\n",
       "              tensor([[ 2.3682e-02,  1.9287e-02, -1.6403e-04,  ..., -1.0376e-02,\n",
       "                       -6.7139e-03,  1.0010e-02],\n",
       "                      [-5.0293e-02,  7.0801e-03,  1.5259e-02,  ..., -3.9551e-02,\n",
       "                        1.9653e-02,  3.7994e-03],\n",
       "                      [ 2.4414e-03, -2.8931e-02, -3.0060e-03,  ...,  7.7209e-03,\n",
       "                       -2.9755e-03, -9.2163e-03],\n",
       "                      ...,\n",
       "                      [-1.1292e-02, -1.4404e-02, -8.8811e-06,  ..., -1.8921e-03,\n",
       "                        2.3682e-02,  4.0588e-03],\n",
       "                      [-1.1230e-02,  1.8188e-02,  3.0518e-03,  ..., -5.5542e-03,\n",
       "                        4.8523e-03, -3.3417e-03],\n",
       "                      [ 1.0376e-02,  2.7466e-03, -9.5825e-03,  ...,  1.1353e-02,\n",
       "                       -8.1787e-03, -4.2725e-03]])),\n",
       "             ('decoder.layers.17.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248130>),\n",
       "             ('decoder.layers.17.mlp.linear_fc2.weight',\n",
       "              tensor([[-0.0137,  0.0062,  0.0059,  ..., -0.0029,  0.0045,  0.0160],\n",
       "                      [-0.0160, -0.0021, -0.0051,  ..., -0.0106, -0.0070, -0.0040],\n",
       "                      [-0.0117, -0.0125,  0.0139,  ..., -0.0095, -0.0181, -0.0170],\n",
       "                      ...,\n",
       "                      [ 0.0117, -0.0031, -0.0012,  ..., -0.0010, -0.0113,  0.0014],\n",
       "                      [-0.0017, -0.0125, -0.0018,  ...,  0.0049, -0.0011, -0.0284],\n",
       "                      [-0.0195,  0.0072, -0.0187,  ..., -0.0035, -0.0001, -0.0023]])),\n",
       "             ('decoder.layers.17.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2481d0>),\n",
       "             ('decoder.layers.18.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248220>),\n",
       "             ('decoder.layers.18.self_attention.linear_proj.weight',\n",
       "              tensor([[-0.0073,  0.0193,  0.0024,  ..., -0.0043,  0.0018, -0.0109],\n",
       "                      [ 0.0004,  0.0051, -0.0066,  ...,  0.0081, -0.0084, -0.0124],\n",
       "                      [ 0.0058, -0.0060, -0.0049,  ...,  0.0109,  0.0116,  0.0017],\n",
       "                      ...,\n",
       "                      [-0.0096, -0.0146,  0.0051,  ..., -0.0147,  0.0130, -0.0086],\n",
       "                      [ 0.0060,  0.0005,  0.0101,  ..., -0.0137, -0.0055,  0.0151],\n",
       "                      [-0.0044, -0.0029, -0.0092,  ..., -0.0023,  0.0048, -0.0095]])),\n",
       "             ('decoder.layers.18.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2482c0>),\n",
       "             ('decoder.layers.18.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4883, 0.4746, 0.4531,  ..., 0.4453, 0.4512, 0.4473])),\n",
       "             ('decoder.layers.18.self_attention.linear_qkv.weight',\n",
       "              tensor([[ 0.0205, -0.0024,  0.0062,  ..., -0.0048, -0.0030,  0.0199],\n",
       "                      [ 0.0002,  0.0046,  0.0068,  ...,  0.0036, -0.0113, -0.0060],\n",
       "                      [ 0.0168, -0.0011, -0.0253,  ...,  0.0041,  0.0011, -0.0107],\n",
       "                      ...,\n",
       "                      [-0.0087, -0.0050,  0.0006,  ...,  0.0033, -0.0042, -0.0096],\n",
       "                      [ 0.0176, -0.0020,  0.0007,  ...,  0.0033,  0.0010, -0.0028],\n",
       "                      [-0.0026, -0.0083,  0.0026,  ...,  0.0081,  0.0080, -0.0029]])),\n",
       "             ('decoder.layers.18.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2483b0>),\n",
       "             ('decoder.layers.18.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.3945, 0.3984, 0.4082,  ..., 0.3906, 0.4199, 0.4023])),\n",
       "             ('decoder.layers.18.mlp.linear_fc1.weight',\n",
       "              tensor([[ 0.0043,  0.0034,  0.0020,  ..., -0.0087, -0.0289,  0.0137],\n",
       "                      [-0.0115, -0.0240,  0.0170,  ...,  0.0181,  0.0258, -0.0116],\n",
       "                      [ 0.0157,  0.0258,  0.0024,  ..., -0.0142,  0.0031,  0.0170],\n",
       "                      ...,\n",
       "                      [ 0.0014,  0.0031,  0.0043,  ..., -0.0106, -0.0052, -0.0037],\n",
       "                      [ 0.0100,  0.0045,  0.0076,  ...,  0.0027,  0.0047,  0.0071],\n",
       "                      [-0.0090,  0.0092,  0.0092,  ..., -0.0166, -0.0114, -0.0011]])),\n",
       "             ('decoder.layers.18.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2484a0>),\n",
       "             ('decoder.layers.18.mlp.linear_fc2.weight',\n",
       "              tensor([[-0.0134,  0.0146,  0.0034,  ...,  0.0041, -0.0006, -0.0183],\n",
       "                      [ 0.0111,  0.0025, -0.0047,  ...,  0.0124,  0.0182, -0.0029],\n",
       "                      [-0.0009,  0.0046,  0.0182,  ...,  0.0025,  0.0097,  0.0007],\n",
       "                      ...,\n",
       "                      [ 0.0047,  0.0223,  0.0009,  ..., -0.0126, -0.0043, -0.0027],\n",
       "                      [ 0.0137,  0.0042, -0.0049,  ..., -0.0036,  0.0010,  0.0001],\n",
       "                      [-0.0283,  0.0103, -0.0041,  ..., -0.0114, -0.0082, -0.0109]])),\n",
       "             ('decoder.layers.18.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248540>),\n",
       "             ('decoder.layers.19.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248590>),\n",
       "             ('decoder.layers.19.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0131,  0.0090, -0.0054,  ...,  0.0009, -0.0088,  0.0184],\n",
       "                      [ 0.0017, -0.0051,  0.0055,  ...,  0.0237, -0.0028, -0.0024],\n",
       "                      [-0.0012,  0.0015,  0.0074,  ..., -0.0142, -0.0078,  0.0038],\n",
       "                      ...,\n",
       "                      [ 0.0052, -0.0012,  0.0036,  ...,  0.0128, -0.0016, -0.0008],\n",
       "                      [ 0.0053,  0.0016,  0.0046,  ..., -0.0079,  0.0172, -0.0004],\n",
       "                      [ 0.0113, -0.0007, -0.0065,  ..., -0.0041,  0.0114, -0.0015]])),\n",
       "             ('decoder.layers.19.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248630>),\n",
       "             ('decoder.layers.19.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4492, 0.4707, 0.4590,  ..., 0.4629, 0.4414, 0.4395])),\n",
       "             ('decoder.layers.19.self_attention.linear_qkv.weight',\n",
       "              tensor([[ 0.0152, -0.0009, -0.0107,  ..., -0.0103, -0.0052,  0.0134],\n",
       "                      [ 0.0038, -0.0136,  0.0080,  ...,  0.0175, -0.0171, -0.0059],\n",
       "                      [-0.0197,  0.0014,  0.0069,  ..., -0.0104, -0.0063, -0.0040],\n",
       "                      ...,\n",
       "                      [ 0.0060,  0.0148,  0.0072,  ...,  0.0024, -0.0039,  0.0080],\n",
       "                      [-0.0101,  0.0037,  0.0156,  ..., -0.0050, -0.0070, -0.0025],\n",
       "                      [ 0.0004,  0.0256,  0.0008,  ...,  0.0162, -0.0056,  0.0211]])),\n",
       "             ('decoder.layers.19.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248720>),\n",
       "             ('decoder.layers.19.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.4121, 0.4141, 0.4219,  ..., 0.4062, 0.4277, 0.4180])),\n",
       "             ('decoder.layers.19.mlp.linear_fc1.weight',\n",
       "              tensor([[-8.7891e-03, -1.7822e-02, -8.3618e-03,  ...,  6.8359e-03,\n",
       "                       -2.4261e-03,  3.5553e-03],\n",
       "                      [ 7.2632e-03, -2.1118e-02,  1.0986e-02,  ..., -3.9062e-03,\n",
       "                       -1.2875e-04, -1.6708e-03],\n",
       "                      [ 3.3203e-02,  4.1504e-03,  5.6396e-02,  ...,  5.1270e-02,\n",
       "                        2.1484e-02, -2.9175e-02],\n",
       "                      ...,\n",
       "                      [ 1.7578e-02, -9.0599e-06,  1.5503e-02,  ..., -4.9438e-03,\n",
       "                        7.5378e-03, -1.1475e-02],\n",
       "                      [-5.9509e-03, -6.7139e-03, -3.7079e-03,  ..., -2.2827e-02,\n",
       "                        2.6512e-04,  1.0986e-03],\n",
       "                      [-1.5747e-02, -2.1210e-03, -9.5825e-03,  ..., -3.1471e-04,\n",
       "                        1.4771e-02, -4.8828e-03]])),\n",
       "             ('decoder.layers.19.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248810>),\n",
       "             ('decoder.layers.19.mlp.linear_fc2.weight',\n",
       "              tensor([[ 7.9346e-03, -2.0218e-04, -1.0620e-02,  ...,  5.9814e-03,\n",
       "                        1.5991e-02, -6.1646e-03],\n",
       "                      [-3.2349e-03,  1.3351e-03, -6.7139e-03,  ...,  1.4221e-02,\n",
       "                        2.0752e-02, -7.5531e-04],\n",
       "                      [-1.7090e-02, -2.2602e-04,  7.9346e-03,  ..., -1.1292e-02,\n",
       "                        2.7771e-03,  8.6212e-04],\n",
       "                      ...,\n",
       "                      [ 1.7090e-02, -6.5918e-03, -1.0376e-02,  ..., -1.1292e-03,\n",
       "                        7.4387e-04,  8.6670e-03],\n",
       "                      [ 1.0437e-02,  2.7466e-02,  7.1335e-04,  ...,  1.7624e-03,\n",
       "                        9.8705e-05, -7.9956e-03],\n",
       "                      [ 1.2695e-02, -5.0049e-03,  2.5177e-04,  ..., -8.7280e-03,\n",
       "                       -1.6357e-02,  9.9487e-03]])),\n",
       "             ('decoder.layers.19.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2488b0>),\n",
       "             ('decoder.layers.20.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248900>),\n",
       "             ('decoder.layers.20.self_attention.linear_proj.weight',\n",
       "              tensor([[-0.0293,  0.0066,  0.0026,  ...,  0.0004, -0.0120,  0.0070],\n",
       "                      [ 0.0028, -0.0119, -0.0027,  ..., -0.0071,  0.0077,  0.0007],\n",
       "                      [-0.0014, -0.0142,  0.0134,  ...,  0.0033,  0.0179,  0.0004],\n",
       "                      ...,\n",
       "                      [ 0.0025, -0.0050, -0.0076,  ...,  0.0026,  0.0016,  0.0036],\n",
       "                      [ 0.0039, -0.0044, -0.0044,  ..., -0.0073,  0.0044,  0.0181],\n",
       "                      [ 0.0034,  0.0020,  0.0064,  ...,  0.0020,  0.0007, -0.0219]])),\n",
       "             ('decoder.layers.20.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2489a0>),\n",
       "             ('decoder.layers.20.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4648, 0.4688, 0.4551,  ..., 0.4629, 0.4453, 0.4512])),\n",
       "             ('decoder.layers.20.self_attention.linear_qkv.weight',\n",
       "              tensor([[ 0.0030, -0.0018,  0.0052,  ...,  0.0184, -0.0143, -0.0043],\n",
       "                      [-0.0167,  0.0038, -0.0010,  ...,  0.0147, -0.0118,  0.0084],\n",
       "                      [ 0.0342, -0.0049, -0.0237,  ..., -0.0045,  0.0010,  0.0028],\n",
       "                      ...,\n",
       "                      [ 0.0018,  0.0010,  0.0040,  ...,  0.0019,  0.0047, -0.0098],\n",
       "                      [ 0.0147, -0.0027, -0.0057,  ...,  0.0084, -0.0038,  0.0189],\n",
       "                      [ 0.0039,  0.0132, -0.0137,  ..., -0.0102, -0.0029,  0.0045]])),\n",
       "             ('decoder.layers.20.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248a90>),\n",
       "             ('decoder.layers.20.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.4238, 0.4258, 0.4375,  ..., 0.4160, 0.4395, 0.4355])),\n",
       "             ('decoder.layers.20.mlp.linear_fc1.weight',\n",
       "              tensor([[ 0.0018, -0.0198, -0.0076,  ..., -0.0258,  0.0156,  0.0027],\n",
       "                      [-0.0107, -0.0131, -0.0013,  ..., -0.0026,  0.0055, -0.0029],\n",
       "                      [ 0.0173,  0.0229,  0.0027,  ...,  0.0030,  0.0065, -0.0137],\n",
       "                      ...,\n",
       "                      [ 0.0096, -0.0194, -0.0085,  ...,  0.0013, -0.0189, -0.0120],\n",
       "                      [-0.0073,  0.0155, -0.0084,  ..., -0.0063, -0.0046, -0.0052],\n",
       "                      [-0.0060, -0.0063, -0.0022,  ...,  0.0089,  0.0057, -0.0050]])),\n",
       "             ('decoder.layers.20.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248b80>),\n",
       "             ('decoder.layers.20.mlp.linear_fc2.weight',\n",
       "              tensor([[ 0.0014,  0.0236, -0.0229,  ...,  0.0161, -0.0186,  0.0078],\n",
       "                      [ 0.0107,  0.0035, -0.0052,  ...,  0.0049,  0.0080, -0.0123],\n",
       "                      [-0.0109,  0.0004,  0.0056,  ..., -0.0172, -0.0135, -0.0020],\n",
       "                      ...,\n",
       "                      [-0.0086, -0.0050, -0.0204,  ..., -0.0036,  0.0036,  0.0037],\n",
       "                      [-0.0135, -0.0057,  0.0007,  ...,  0.0010,  0.0097, -0.0060],\n",
       "                      [ 0.0074, -0.0099, -0.0189,  ...,  0.0038,  0.0123,  0.0208]])),\n",
       "             ('decoder.layers.20.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248c20>),\n",
       "             ('decoder.layers.21.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248c70>),\n",
       "             ('decoder.layers.21.self_attention.linear_proj.weight',\n",
       "              tensor([[ 7.9346e-04,  6.1035e-04,  5.4321e-03,  ..., -3.4790e-03,\n",
       "                       -4.1504e-03,  5.5552e-05],\n",
       "                      [-1.3916e-02,  3.0212e-03,  9.8267e-03,  ..., -1.2207e-02,\n",
       "                       -6.5918e-03, -1.3672e-02],\n",
       "                      [ 9.0332e-03, -6.5308e-03, -6.0730e-03,  ..., -1.4893e-02,\n",
       "                       -3.7079e-03, -5.3406e-03],\n",
       "                      ...,\n",
       "                      [ 4.4861e-03,  6.8054e-03,  9.2163e-03,  ..., -2.3956e-03,\n",
       "                        8.0872e-04,  3.7842e-03],\n",
       "                      [ 1.0071e-02,  5.7678e-03,  2.0599e-03,  ...,  6.8054e-03,\n",
       "                        3.2616e-04, -1.5991e-02],\n",
       "                      [-1.7929e-03, -5.7983e-03, -1.9989e-03,  ..., -5.4321e-03,\n",
       "                       -8.7280e-03, -3.2959e-03]])),\n",
       "             ('decoder.layers.21.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248d10>),\n",
       "             ('decoder.layers.21.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4766, 0.4531, 0.4590,  ..., 0.4707, 0.4375, 0.4707])),\n",
       "             ('decoder.layers.21.self_attention.linear_qkv.weight',\n",
       "              tensor([[-0.0006, -0.0002,  0.0106,  ..., -0.0047,  0.0020,  0.0125],\n",
       "                      [-0.0023,  0.0105,  0.0116,  ...,  0.0030, -0.0041,  0.0157],\n",
       "                      [ 0.0025,  0.0134, -0.0098,  ...,  0.0107, -0.0142, -0.0147],\n",
       "                      ...,\n",
       "                      [-0.0120, -0.0033,  0.0044,  ..., -0.0148,  0.0019,  0.0101],\n",
       "                      [ 0.0005,  0.0035, -0.0057,  ...,  0.0044, -0.0015, -0.0068],\n",
       "                      [-0.0099,  0.0056, -0.0003,  ..., -0.0147,  0.0140,  0.0013]])),\n",
       "             ('decoder.layers.21.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248e00>),\n",
       "             ('decoder.layers.21.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.4434, 0.4453, 0.4551,  ..., 0.4375, 0.4512, 0.4551])),\n",
       "             ('decoder.layers.21.mlp.linear_fc1.weight',\n",
       "              tensor([[ 0.0201,  0.0105, -0.0289,  ...,  0.0048,  0.0175,  0.0112],\n",
       "                      [ 0.0125, -0.0042,  0.0002,  ...,  0.0017,  0.0015, -0.0386],\n",
       "                      [ 0.0034,  0.0181, -0.0019,  ..., -0.0117, -0.0172,  0.0325],\n",
       "                      ...,\n",
       "                      [ 0.0106, -0.0190, -0.0048,  ...,  0.0087,  0.0028,  0.0079],\n",
       "                      [-0.0189,  0.0305,  0.0020,  ..., -0.0124, -0.0108, -0.0029],\n",
       "                      [ 0.0129, -0.0089, -0.0043,  ...,  0.0012, -0.0006, -0.0126]])),\n",
       "             ('decoder.layers.21.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248ef0>),\n",
       "             ('decoder.layers.21.mlp.linear_fc2.weight',\n",
       "              tensor([[-0.0033, -0.0195,  0.0045,  ..., -0.0145, -0.0182,  0.0137],\n",
       "                      [ 0.0055, -0.0025,  0.0153,  ...,  0.0123,  0.0164, -0.0159],\n",
       "                      [ 0.0030, -0.0110, -0.0132,  ...,  0.0039,  0.0203, -0.0194],\n",
       "                      ...,\n",
       "                      [ 0.0001, -0.0008,  0.0210,  ..., -0.0050, -0.0046,  0.0043],\n",
       "                      [ 0.0194, -0.0046, -0.0016,  ..., -0.0096, -0.0081,  0.0055],\n",
       "                      [ 0.0117, -0.0110, -0.0104,  ..., -0.0173, -0.0133, -0.0028]])),\n",
       "             ('decoder.layers.21.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248f90>),\n",
       "             ('decoder.layers.22.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f248fe0>),\n",
       "             ('decoder.layers.22.self_attention.linear_proj.weight',\n",
       "              tensor([[-5.5542e-03, -6.6528e-03,  9.4414e-05,  ...,  2.1667e-03,\n",
       "                        2.6733e-02, -9.5825e-03],\n",
       "                      [-9.5215e-03, -1.9165e-02,  1.5991e-02,  ..., -8.7280e-03,\n",
       "                        3.5248e-03, -2.3956e-03],\n",
       "                      [ 1.2085e-02, -7.0496e-03,  1.6724e-02,  ..., -1.9150e-03,\n",
       "                        1.4954e-02, -8.1177e-03],\n",
       "                      ...,\n",
       "                      [-1.6357e-02, -5.1575e-03, -7.3853e-03,  ..., -1.1292e-02,\n",
       "                        6.4087e-03, -8.1787e-03],\n",
       "                      [ 1.7822e-02, -1.6968e-02, -6.8665e-03,  ..., -6.4697e-03,\n",
       "                        1.3580e-03, -1.3367e-02],\n",
       "                      [ 2.1458e-05,  7.9346e-03,  3.5248e-03,  ...,  5.4932e-03,\n",
       "                        1.6846e-02,  7.3242e-04]])),\n",
       "             ('decoder.layers.22.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249080>),\n",
       "             ('decoder.layers.22.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4824, 0.4727, 0.4805,  ..., 0.4844, 0.4453, 0.4824])),\n",
       "             ('decoder.layers.22.self_attention.linear_qkv.weight',\n",
       "              tensor([[-0.0025,  0.0102, -0.0044,  ..., -0.0160, -0.0195, -0.0042],\n",
       "                      [-0.0006,  0.0101,  0.0157,  ..., -0.0038, -0.0032, -0.0069],\n",
       "                      [ 0.0067, -0.0022, -0.0254,  ..., -0.0231, -0.0051,  0.0095],\n",
       "                      ...,\n",
       "                      [-0.0076,  0.0045,  0.0062,  ..., -0.0151,  0.0020, -0.0071],\n",
       "                      [-0.0079,  0.0004,  0.0127,  ...,  0.0091, -0.0107,  0.0135],\n",
       "                      [-0.0069, -0.0093, -0.0121,  ...,  0.0160,  0.0128, -0.0344]])),\n",
       "             ('decoder.layers.22.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249170>),\n",
       "             ('decoder.layers.22.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.4570, 0.4570, 0.4707,  ..., 0.4473, 0.4688, 0.4668])),\n",
       "             ('decoder.layers.22.mlp.linear_fc1.weight',\n",
       "              tensor([[-0.0020,  0.0095,  0.0026,  ...,  0.0108, -0.0183, -0.0242],\n",
       "                      [ 0.0118, -0.0173,  0.0076,  ..., -0.0011,  0.0238, -0.0036],\n",
       "                      [-0.0269, -0.0126, -0.0060,  ...,  0.0018,  0.0144,  0.0148],\n",
       "                      ...,\n",
       "                      [-0.0060,  0.0008,  0.0160,  ..., -0.0061, -0.0093,  0.0069],\n",
       "                      [ 0.0071,  0.0099,  0.0035,  ...,  0.0065,  0.0156,  0.0032],\n",
       "                      [-0.0122,  0.0134,  0.0236,  ...,  0.0160, -0.0002,  0.0016]])),\n",
       "             ('decoder.layers.22.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249260>),\n",
       "             ('decoder.layers.22.mlp.linear_fc2.weight',\n",
       "              tensor([[-8.5449e-03, -1.8616e-03,  5.9509e-03,  ...,  2.6367e-02,\n",
       "                       -1.7822e-02,  9.4604e-03],\n",
       "                      [ 5.7602e-04, -1.5869e-02, -5.1270e-03,  ...,  3.7670e-05,\n",
       "                        2.5177e-03, -1.8921e-03],\n",
       "                      [ 4.4250e-03,  1.0315e-02, -1.4038e-02,  ...,  6.3705e-04,\n",
       "                       -1.3855e-02,  1.6724e-02],\n",
       "                      ...,\n",
       "                      [-4.8828e-03, -4.7302e-03,  1.2085e-02,  ..., -1.3428e-03,\n",
       "                       -1.0132e-02, -1.1902e-03],\n",
       "                      [-1.8539e-03,  4.0283e-03, -1.2817e-02,  ..., -2.9297e-03,\n",
       "                        3.8300e-03,  1.3062e-02],\n",
       "                      [ 1.6113e-02,  1.4343e-02,  7.9956e-03,  ...,  8.3618e-03,\n",
       "                       -1.2741e-03,  1.8845e-03]])),\n",
       "             ('decoder.layers.22.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249300>),\n",
       "             ('decoder.layers.23.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249350>),\n",
       "             ('decoder.layers.23.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0063, -0.0143,  0.0106,  ..., -0.0168, -0.0074, -0.0042],\n",
       "                      [-0.0128, -0.0073,  0.0096,  ...,  0.0025, -0.0011,  0.0026],\n",
       "                      [-0.0023,  0.0078,  0.0111,  ..., -0.0022, -0.0091,  0.0019],\n",
       "                      ...,\n",
       "                      [ 0.0227,  0.0038, -0.0173,  ..., -0.0038, -0.0096,  0.0054],\n",
       "                      [ 0.0057,  0.0001,  0.0172,  ...,  0.0054,  0.0099, -0.0032],\n",
       "                      [ 0.0022,  0.0018,  0.0102,  ..., -0.0064,  0.0045, -0.0058]])),\n",
       "             ('decoder.layers.23.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2493f0>),\n",
       "             ('decoder.layers.23.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4941, 0.4902, 0.4883,  ..., 0.5078, 0.4648, 0.4941])),\n",
       "             ('decoder.layers.23.self_attention.linear_qkv.weight',\n",
       "              tensor([[-0.0032, -0.0096, -0.0040,  ..., -0.0118,  0.0039, -0.0020],\n",
       "                      [ 0.0103, -0.0160,  0.0019,  ...,  0.0107,  0.0082, -0.0093],\n",
       "                      [ 0.0031,  0.0055, -0.0020,  ..., -0.0186, -0.0122,  0.0078],\n",
       "                      ...,\n",
       "                      [-0.0045,  0.0101,  0.0124,  ...,  0.0010,  0.0179,  0.0060],\n",
       "                      [ 0.0001, -0.0029, -0.0016,  ..., -0.0099, -0.0144, -0.0014],\n",
       "                      [ 0.0140, -0.0029, -0.0006,  ...,  0.0150,  0.0154, -0.0170]])),\n",
       "             ('decoder.layers.23.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2494e0>),\n",
       "             ('decoder.layers.23.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.4727, 0.4746, 0.4746,  ..., 0.4668, 0.4805, 0.4844])),\n",
       "             ('decoder.layers.23.mlp.linear_fc1.weight',\n",
       "              tensor([[-0.0016, -0.0053, -0.0205,  ..., -0.0164,  0.0036, -0.0117],\n",
       "                      [ 0.0083,  0.0024,  0.0050,  ...,  0.0222, -0.0132, -0.0072],\n",
       "                      [-0.0197, -0.0004,  0.0058,  ..., -0.0092,  0.0222,  0.0034],\n",
       "                      ...,\n",
       "                      [ 0.0101, -0.0022,  0.0090,  ...,  0.0041,  0.0046,  0.0030],\n",
       "                      [ 0.0038,  0.0002,  0.0076,  ...,  0.0003, -0.0139,  0.0135],\n",
       "                      [ 0.0051,  0.0064, -0.0103,  ...,  0.0013, -0.0138,  0.0001]])),\n",
       "             ('decoder.layers.23.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2495d0>),\n",
       "             ('decoder.layers.23.mlp.linear_fc2.weight',\n",
       "              tensor([[ 0.0006,  0.0049,  0.0018,  ...,  0.0037,  0.0037,  0.0005],\n",
       "                      [-0.0159, -0.0045, -0.0116,  ..., -0.0095,  0.0107,  0.0078],\n",
       "                      [ 0.0067, -0.0001,  0.0053,  ..., -0.0049,  0.0101, -0.0012],\n",
       "                      ...,\n",
       "                      [ 0.0085,  0.0089, -0.0071,  ..., -0.0027, -0.0139,  0.0044],\n",
       "                      [-0.0056, -0.0091, -0.0071,  ..., -0.0231, -0.0201, -0.0055],\n",
       "                      [-0.0277, -0.0186,  0.0121,  ...,  0.0013,  0.0003,  0.0050]])),\n",
       "             ('decoder.layers.23.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249670>),\n",
       "             ('decoder.layers.24.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2496c0>),\n",
       "             ('decoder.layers.24.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0063,  0.0101, -0.0009,  ..., -0.0011,  0.0006,  0.0040],\n",
       "                      [ 0.0031, -0.0064, -0.0256,  ..., -0.0007, -0.0023,  0.0056],\n",
       "                      [-0.0016,  0.0034, -0.0016,  ..., -0.0014, -0.0019, -0.0090],\n",
       "                      ...,\n",
       "                      [-0.0085,  0.0090, -0.0149,  ...,  0.0042,  0.0116,  0.0027],\n",
       "                      [-0.0145,  0.0128, -0.0352,  ..., -0.0164, -0.0058, -0.0007],\n",
       "                      [-0.0003, -0.0029, -0.0042,  ...,  0.0061, -0.0003,  0.0013]])),\n",
       "             ('decoder.layers.24.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249760>),\n",
       "             ('decoder.layers.24.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4883, 0.4922, 0.4922,  ..., 0.5156, 0.4707, 0.4902])),\n",
       "             ('decoder.layers.24.self_attention.linear_qkv.weight',\n",
       "              tensor([[-0.0068,  0.0033,  0.0166,  ...,  0.0069, -0.0109, -0.0256],\n",
       "                      [-0.0106, -0.0044, -0.0138,  ..., -0.0098,  0.0045, -0.0182],\n",
       "                      [-0.0117,  0.0244,  0.0154,  ...,  0.0020,  0.0229,  0.0200],\n",
       "                      ...,\n",
       "                      [-0.0177,  0.0014, -0.0089,  ...,  0.0034, -0.0135, -0.0025],\n",
       "                      [ 0.0077, -0.0072, -0.0078,  ..., -0.0162, -0.0231, -0.0009],\n",
       "                      [-0.0036, -0.0079, -0.0172,  ...,  0.0007,  0.0092, -0.0011]])),\n",
       "             ('decoder.layers.24.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249850>),\n",
       "             ('decoder.layers.24.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.4922, 0.4902, 0.4961,  ..., 0.4824, 0.4980, 0.5000])),\n",
       "             ('decoder.layers.24.mlp.linear_fc1.weight',\n",
       "              tensor([[ 9.6512e-04, -7.2021e-03, -2.7313e-03,  ...,  2.9785e-02,\n",
       "                       -1.4832e-02,  7.6599e-03],\n",
       "                      [ 2.7313e-03, -1.7822e-02, -6.0730e-03,  ...,  1.8433e-02,\n",
       "                        2.2949e-02, -1.4038e-02],\n",
       "                      [-1.3123e-02, -2.5787e-03,  1.4305e-04,  ...,  2.8687e-02,\n",
       "                        1.0376e-02, -9.2773e-03],\n",
       "                      ...,\n",
       "                      [ 7.3853e-03, -8.3618e-03,  9.3842e-04,  ..., -1.1719e-02,\n",
       "                        1.5564e-02, -8.3008e-03],\n",
       "                      [-2.5940e-03,  4.6082e-03,  8.6308e-05,  ...,  2.0142e-02,\n",
       "                        8.3008e-03, -1.1108e-02],\n",
       "                      [-3.5400e-03,  3.4027e-03, -4.7302e-03,  ..., -1.3062e-02,\n",
       "                       -2.2827e-02, -1.5015e-02]])),\n",
       "             ('decoder.layers.24.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249940>),\n",
       "             ('decoder.layers.24.mlp.linear_fc2.weight',\n",
       "              tensor([[-0.0125,  0.0092, -0.0026,  ..., -0.0043, -0.0113, -0.0112],\n",
       "                      [-0.0039, -0.0047, -0.0129,  ..., -0.0205,  0.0013, -0.0060],\n",
       "                      [-0.0031,  0.0136,  0.0007,  ...,  0.0135,  0.0176, -0.0059],\n",
       "                      ...,\n",
       "                      [-0.0198,  0.0081,  0.0032,  ..., -0.0061,  0.0089,  0.0027],\n",
       "                      [ 0.0120,  0.0007, -0.0118,  ..., -0.0027,  0.0093,  0.0038],\n",
       "                      [-0.0198,  0.0159, -0.0067,  ..., -0.0054, -0.0078, -0.0123]])),\n",
       "             ('decoder.layers.24.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f2499e0>),\n",
       "             ('decoder.layers.25.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249a30>),\n",
       "             ('decoder.layers.25.self_attention.linear_proj.weight',\n",
       "              tensor([[-1.1108e-02,  5.0964e-03, -9.5825e-03,  ..., -8.9722e-03,\n",
       "                        1.5625e-02,  8.3160e-04],\n",
       "                      [-2.7954e-02,  5.7068e-03,  7.3242e-03,  ..., -1.1902e-02,\n",
       "                       -1.1230e-02, -1.1475e-02],\n",
       "                      [-1.9287e-02, -2.1729e-02, -1.7090e-02,  ...,  5.3711e-03,\n",
       "                        2.6367e-02, -2.1851e-02],\n",
       "                      ...,\n",
       "                      [ 1.0559e-02,  4.5166e-03, -3.3569e-03,  ..., -1.3062e-02,\n",
       "                        8.1787e-03,  1.2131e-03],\n",
       "                      [ 2.6489e-02, -3.7079e-03, -1.2573e-02,  ...,  8.9264e-04,\n",
       "                        1.0071e-02,  1.4465e-02],\n",
       "                      [-1.6846e-02,  5.6028e-05,  1.9169e-04,  ...,  5.0659e-03,\n",
       "                       -6.1646e-03, -7.7820e-03]])),\n",
       "             ('decoder.layers.25.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249ad0>),\n",
       "             ('decoder.layers.25.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.5000, 0.5039, 0.4922,  ..., 0.5352, 0.4766, 0.5078])),\n",
       "             ('decoder.layers.25.self_attention.linear_qkv.weight',\n",
       "              tensor([[ 0.0117, -0.0027, -0.0017,  ...,  0.0059,  0.0109,  0.0106],\n",
       "                      [ 0.0011,  0.0007, -0.0112,  ..., -0.0137,  0.0049,  0.0104],\n",
       "                      [ 0.0214, -0.0046, -0.0132,  ..., -0.0136, -0.0084, -0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0010,  0.0009,  0.0075,  ...,  0.0208,  0.0084, -0.0229],\n",
       "                      [-0.0070,  0.0074,  0.0020,  ...,  0.0109,  0.0104, -0.0189],\n",
       "                      [ 0.0042,  0.0126,  0.0023,  ..., -0.0079, -0.0132,  0.0050]])),\n",
       "             ('decoder.layers.25.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249bc0>),\n",
       "             ('decoder.layers.25.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.5078, 0.5039, 0.5117,  ..., 0.5000, 0.5117, 0.5078])),\n",
       "             ('decoder.layers.25.mlp.linear_fc1.weight',\n",
       "              tensor([[ 1.0498e-02,  9.7656e-04, -5.6458e-03,  ...,  1.4420e-03,\n",
       "                        1.0803e-02,  2.1210e-03],\n",
       "                      [-3.6926e-03, -9.9487e-03, -2.1973e-02,  ..., -7.7820e-03,\n",
       "                       -1.9897e-02,  8.0566e-03],\n",
       "                      [ 1.1902e-02,  1.8555e-02, -1.0315e-02,  ..., -4.2236e-02,\n",
       "                       -3.2654e-03, -7.5989e-03],\n",
       "                      ...,\n",
       "                      [-2.4170e-02, -3.0212e-03, -3.7842e-03,  ..., -1.4343e-03,\n",
       "                       -4.1199e-03,  1.5137e-02],\n",
       "                      [-1.5747e-02,  1.8082e-03, -2.5177e-03,  ...,  1.8311e-02,\n",
       "                       -2.1484e-02,  6.3171e-03],\n",
       "                      [ 1.4893e-02, -2.9297e-03, -1.5259e-02,  ...,  9.0942e-03,\n",
       "                       -6.5863e-06,  1.5137e-02]])),\n",
       "             ('decoder.layers.25.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249c60>),\n",
       "             ('decoder.layers.25.mlp.linear_fc2.weight',\n",
       "              tensor([[ 0.0035, -0.0155,  0.0079,  ...,  0.0136, -0.0089,  0.0042],\n",
       "                      [-0.0199, -0.0179, -0.0065,  ..., -0.0117, -0.0081, -0.0089],\n",
       "                      [-0.0027, -0.0010, -0.0023,  ...,  0.0059,  0.0018,  0.0060],\n",
       "                      ...,\n",
       "                      [-0.0134,  0.0280, -0.0106,  ...,  0.0097, -0.0198,  0.0003],\n",
       "                      [ 0.0002,  0.0087,  0.0054,  ..., -0.0115, -0.0018,  0.0057],\n",
       "                      [ 0.0201,  0.0189,  0.0118,  ..., -0.0003,  0.0029, -0.0244]])),\n",
       "             ('decoder.layers.25.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249d00>),\n",
       "             ('decoder.layers.26.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249d50>),\n",
       "             ('decoder.layers.26.self_attention.linear_proj.weight',\n",
       "              tensor([[-0.0265, -0.0031, -0.0003,  ...,  0.0015,  0.0029, -0.0090],\n",
       "                      [-0.0085,  0.0134,  0.0045,  ...,  0.0214,  0.0212, -0.0117],\n",
       "                      [-0.0081,  0.0014,  0.0010,  ..., -0.0240,  0.0010, -0.0046],\n",
       "                      ...,\n",
       "                      [ 0.0119, -0.0070, -0.0054,  ...,  0.0201, -0.0117,  0.0069],\n",
       "                      [ 0.0006,  0.0010, -0.0056,  ..., -0.0181,  0.0092, -0.0123],\n",
       "                      [ 0.0002, -0.0087,  0.0044,  ..., -0.0047,  0.0006,  0.0192]])),\n",
       "             ('decoder.layers.26.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249df0>),\n",
       "             ('decoder.layers.26.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4844, 0.4746, 0.4609,  ..., 0.4902, 0.4512, 0.4785])),\n",
       "             ('decoder.layers.26.self_attention.linear_qkv.weight',\n",
       "              tensor([[ 0.0002, -0.0017, -0.0089,  ...,  0.0046, -0.0008,  0.0081],\n",
       "                      [ 0.0047, -0.0005, -0.0204,  ...,  0.0023, -0.0069,  0.0011],\n",
       "                      [ 0.0019, -0.0112, -0.0084,  ..., -0.0079, -0.0182, -0.0001],\n",
       "                      ...,\n",
       "                      [-0.0145, -0.0015,  0.0053,  ..., -0.0057, -0.0186,  0.0039],\n",
       "                      [ 0.0027,  0.0247, -0.0090,  ...,  0.0127,  0.0014, -0.0028],\n",
       "                      [-0.0105,  0.0128, -0.0095,  ...,  0.0149, -0.0184, -0.0039]])),\n",
       "             ('decoder.layers.26.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249ee0>),\n",
       "             ('decoder.layers.26.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.5234, 0.5273, 0.5352,  ..., 0.5234, 0.5391, 0.5273])),\n",
       "             ('decoder.layers.26.mlp.linear_fc1.weight',\n",
       "              tensor([[-0.0197, -0.0171, -0.0095,  ...,  0.0056, -0.0288, -0.0107],\n",
       "                      [-0.0092, -0.0258, -0.0255,  ...,  0.0183, -0.0112,  0.0045],\n",
       "                      [ 0.0194,  0.0058, -0.0143,  ..., -0.0050,  0.0182,  0.0067],\n",
       "                      ...,\n",
       "                      [ 0.0143,  0.0132,  0.0014,  ...,  0.0186, -0.0198,  0.0060],\n",
       "                      [ 0.0156,  0.0056,  0.0036,  ..., -0.0130, -0.0260,  0.0132],\n",
       "                      [ 0.0073,  0.0010,  0.0032,  ..., -0.0092, -0.0140,  0.0020]])),\n",
       "             ('decoder.layers.26.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f249fd0>),\n",
       "             ('decoder.layers.26.mlp.linear_fc2.weight',\n",
       "              tensor([[-0.0026,  0.0035,  0.0061,  ...,  0.0065, -0.0123, -0.0083],\n",
       "                      [ 0.0060, -0.0010,  0.0159,  ..., -0.0034,  0.0214,  0.0212],\n",
       "                      [ 0.0069,  0.0131, -0.0049,  ...,  0.0040,  0.0082,  0.0026],\n",
       "                      ...,\n",
       "                      [ 0.0032,  0.0110, -0.0140,  ...,  0.0048,  0.0051, -0.0090],\n",
       "                      [ 0.0014,  0.0020,  0.0153,  ..., -0.0002,  0.0017, -0.0014],\n",
       "                      [-0.0072, -0.0076,  0.0104,  ...,  0.0171,  0.0037, -0.0004]])),\n",
       "             ('decoder.layers.26.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24a070>),\n",
       "             ('decoder.layers.27.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24a0c0>),\n",
       "             ('decoder.layers.27.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0048,  0.0114, -0.0055,  ...,  0.0033,  0.0006, -0.0099],\n",
       "                      [ 0.0112,  0.0064,  0.0219,  ..., -0.0041,  0.0002,  0.0070],\n",
       "                      [-0.0115,  0.0156, -0.0135,  ...,  0.0106, -0.0017,  0.0137],\n",
       "                      ...,\n",
       "                      [ 0.0015,  0.0100, -0.0035,  ...,  0.0040, -0.0187, -0.0081],\n",
       "                      [ 0.0114, -0.0069,  0.0059,  ..., -0.0089,  0.0102,  0.0022],\n",
       "                      [-0.0193,  0.0098,  0.0183,  ...,  0.0015, -0.0067, -0.0077]])),\n",
       "             ('decoder.layers.27.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24a160>),\n",
       "             ('decoder.layers.27.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.5312, 0.5312, 0.4980,  ..., 0.5352, 0.4844, 0.5117])),\n",
       "             ('decoder.layers.27.self_attention.linear_qkv.weight',\n",
       "              tensor([[-0.0242, -0.0103, -0.0178,  ...,  0.0200,  0.0063, -0.0134],\n",
       "                      [ 0.0144, -0.0036, -0.0197,  ...,  0.0093,  0.0100,  0.0077],\n",
       "                      [ 0.0114,  0.0102,  0.0102,  ..., -0.0067, -0.0214, -0.0020],\n",
       "                      ...,\n",
       "                      [ 0.0038, -0.0094,  0.0027,  ..., -0.0033,  0.0118, -0.0215],\n",
       "                      [ 0.0145, -0.0003,  0.0010,  ..., -0.0133,  0.0140, -0.0058],\n",
       "                      [-0.0101, -0.0062, -0.0258,  ..., -0.0002,  0.0090, -0.0066]])),\n",
       "             ('decoder.layers.27.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24a250>),\n",
       "             ('decoder.layers.27.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.5469, 0.5547, 0.5547,  ..., 0.5469, 0.5586, 0.5508])),\n",
       "             ('decoder.layers.27.mlp.linear_fc1.weight',\n",
       "              tensor([[ 0.0009, -0.0220,  0.0031,  ..., -0.0123, -0.0054, -0.0044],\n",
       "                      [-0.0056, -0.0098, -0.0249,  ..., -0.0051,  0.0038,  0.0134],\n",
       "                      [-0.0123, -0.0175,  0.0008,  ...,  0.0132, -0.0053,  0.0253],\n",
       "                      ...,\n",
       "                      [-0.0234, -0.0007,  0.0099,  ..., -0.0063, -0.0148, -0.0123],\n",
       "                      [-0.0082,  0.0175,  0.0041,  ..., -0.0002, -0.0269,  0.0210],\n",
       "                      [-0.0097, -0.0025,  0.0160,  ..., -0.0103,  0.0011, -0.0094]])),\n",
       "             ('decoder.layers.27.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24a2f0>),\n",
       "             ('decoder.layers.27.mlp.linear_fc2.weight',\n",
       "              tensor([[-1.1902e-02,  1.5259e-02,  1.7471e-03,  ..., -3.9368e-03,\n",
       "                       -4.7607e-03,  9.8419e-04],\n",
       "                      [-2.7313e-03, -1.7929e-03, -5.5542e-03,  ...,  4.5776e-03,\n",
       "                        1.2512e-02,  9.3384e-03],\n",
       "                      [-9.0790e-04,  7.2327e-03,  2.8534e-03,  ..., -6.7749e-03,\n",
       "                       -4.6082e-03,  7.2632e-03],\n",
       "                      ...,\n",
       "                      [-1.7212e-02,  1.5137e-02, -8.6975e-04,  ...,  8.6670e-03,\n",
       "                       -4.8828e-03,  3.3112e-03],\n",
       "                      [ 2.8229e-03, -1.1658e-02,  8.7261e-05,  ...,  1.2024e-02,\n",
       "                       -1.0376e-02, -1.3885e-03],\n",
       "                      [-3.5706e-03, -8.0566e-03,  1.2634e-02,  ...,  8.9111e-03,\n",
       "                        4.6158e-04, -9.8877e-03]])),\n",
       "             ('decoder.layers.27.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24a390>),\n",
       "             ('decoder.layers.28.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24a3e0>),\n",
       "             ('decoder.layers.28.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0005, -0.0168, -0.0371,  ..., -0.0074, -0.0049,  0.0132],\n",
       "                      [ 0.0088, -0.0069, -0.0143,  ...,  0.0135, -0.0031, -0.0104],\n",
       "                      [ 0.0058, -0.0031, -0.0221,  ...,  0.0149, -0.0272,  0.0036],\n",
       "                      ...,\n",
       "                      [-0.0134,  0.0034, -0.0057,  ..., -0.0016,  0.0062, -0.0041],\n",
       "                      [ 0.0217, -0.0232, -0.0002,  ..., -0.0267,  0.0211,  0.0115],\n",
       "                      [ 0.0219,  0.0132,  0.0092,  ...,  0.0069, -0.0044, -0.0325]])),\n",
       "             ('decoder.layers.28.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24a480>),\n",
       "             ('decoder.layers.28.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.5039, 0.5156, 0.4941,  ..., 0.5156, 0.4902, 0.4922])),\n",
       "             ('decoder.layers.28.self_attention.linear_qkv.weight',\n",
       "              tensor([[ 0.0060,  0.0011, -0.0132,  ..., -0.0084,  0.0044,  0.0067],\n",
       "                      [ 0.0197, -0.0201,  0.0016,  ..., -0.0129, -0.0176, -0.0154],\n",
       "                      [ 0.0208, -0.0045,  0.0082,  ..., -0.0077,  0.0102,  0.0094],\n",
       "                      ...,\n",
       "                      [ 0.0053,  0.0061,  0.0138,  ...,  0.0054, -0.0063,  0.0013],\n",
       "                      [ 0.0042,  0.0208, -0.0109,  ...,  0.0021,  0.0112, -0.0051],\n",
       "                      [-0.0139, -0.0220,  0.0043,  ...,  0.0143,  0.0067, -0.0220]])),\n",
       "             ('decoder.layers.28.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24a570>),\n",
       "             ('decoder.layers.28.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.5742, 0.5820, 0.5820,  ..., 0.5742, 0.5820, 0.5781])),\n",
       "             ('decoder.layers.28.mlp.linear_fc1.weight',\n",
       "              tensor([[-2.2705e-02,  8.3008e-03, -1.2817e-02,  ..., -4.3335e-03,\n",
       "                       -1.2451e-02,  1.2817e-02],\n",
       "                      [-7.5684e-03,  7.9956e-03,  9.5215e-03,  ...,  8.1177e-03,\n",
       "                       -1.3184e-02, -9.5215e-03],\n",
       "                      [ 1.1963e-02, -3.3875e-03,  2.0752e-02,  ...,  1.9409e-02,\n",
       "                       -2.9785e-02, -8.2970e-05],\n",
       "                      ...,\n",
       "                      [-1.1902e-02, -1.0193e-02,  9.1934e-04,  ..., -1.3245e-02,\n",
       "                        2.2278e-03, -9.2163e-03],\n",
       "                      [ 1.5354e-04, -1.2695e-02, -7.5378e-03,  ..., -5.7373e-03,\n",
       "                       -5.7678e-03,  7.2937e-03],\n",
       "                      [-5.8289e-03, -8.4229e-03, -4.5471e-03,  ..., -7.4463e-03,\n",
       "                       -4.9591e-04, -6.4697e-03]])),\n",
       "             ('decoder.layers.28.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24a660>),\n",
       "             ('decoder.layers.28.mlp.linear_fc2.weight',\n",
       "              tensor([[-0.0016, -0.0055, -0.0036,  ...,  0.0153,  0.0047,  0.0009],\n",
       "                      [ 0.0011,  0.0033,  0.0102,  ..., -0.0054, -0.0042, -0.0249],\n",
       "                      [-0.0088, -0.0137,  0.0020,  ..., -0.0056, -0.0008,  0.0126],\n",
       "                      ...,\n",
       "                      [-0.0198, -0.0297,  0.0035,  ..., -0.0005,  0.0093, -0.0146],\n",
       "                      [ 0.0095,  0.0020,  0.0098,  ..., -0.0147,  0.0022, -0.0024],\n",
       "                      [ 0.0004, -0.0019,  0.0129,  ..., -0.0126,  0.0005, -0.0041]])),\n",
       "             ('decoder.layers.28.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24a700>),\n",
       "             ('decoder.layers.29.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24a750>),\n",
       "             ('decoder.layers.29.self_attention.linear_proj.weight',\n",
       "              tensor([[-3.0518e-02,  1.5991e-02,  3.2806e-03,  ..., -7.9346e-03,\n",
       "                        5.7373e-03,  1.7456e-02],\n",
       "                      [ 1.3428e-02,  1.1414e-02, -5.1270e-03,  ..., -1.4771e-02,\n",
       "                       -7.0496e-03,  7.4463e-03],\n",
       "                      [-2.7100e-02,  9.0332e-03,  2.2793e-04,  ..., -3.6621e-03,\n",
       "                       -2.8992e-03,  4.6997e-03],\n",
       "                      ...,\n",
       "                      [ 1.0132e-02, -4.1504e-03, -3.3569e-03,  ...,  1.0620e-02,\n",
       "                       -6.2866e-03, -1.0620e-02],\n",
       "                      [-4.1962e-05, -1.3428e-02, -9.2773e-03,  ...,  1.5625e-02,\n",
       "                       -1.1047e-02,  1.2207e-02],\n",
       "                      [ 2.0142e-02,  4.8065e-04,  4.3030e-03,  ..., -2.0599e-03,\n",
       "                        8.8501e-03,  7.2479e-04]])),\n",
       "             ('decoder.layers.29.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24a7f0>),\n",
       "             ('decoder.layers.29.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4590, 0.5547, 0.5391,  ..., 0.5234, 0.4922, 0.5391])),\n",
       "             ('decoder.layers.29.self_attention.linear_qkv.weight',\n",
       "              tensor([[-3.8574e-02, -1.1780e-02,  4.0588e-03,  ..., -2.4780e-02,\n",
       "                       -1.7700e-02, -8.7280e-03],\n",
       "                      [ 1.4160e-02,  1.6846e-02, -1.3916e-02,  ...,  1.8921e-02,\n",
       "                       -4.1199e-04, -1.1215e-03],\n",
       "                      [ 3.9368e-03, -1.4801e-03,  1.1169e-02,  ..., -8.8811e-06,\n",
       "                       -1.4221e-02, -2.5146e-02],\n",
       "                      ...,\n",
       "                      [-2.9419e-02, -2.9053e-02, -4.5776e-03,  ...,  1.6708e-03,\n",
       "                        1.2390e-02, -5.4321e-03],\n",
       "                      [ 1.6479e-02, -2.3499e-03, -9.5215e-03,  ...,  2.6093e-03,\n",
       "                       -5.5542e-03,  9.7656e-03],\n",
       "                      [ 1.4343e-02,  1.2573e-02, -1.4404e-02,  ..., -7.7820e-03,\n",
       "                        1.5381e-02, -1.6113e-02]])),\n",
       "             ('decoder.layers.29.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24a8e0>),\n",
       "             ('decoder.layers.29.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.5859, 0.5859, 0.5938,  ..., 0.5820, 0.5938, 0.5938])),\n",
       "             ('decoder.layers.29.mlp.linear_fc1.weight',\n",
       "              tensor([[ 0.0045, -0.0500,  0.0121,  ..., -0.0118,  0.0116,  0.0214],\n",
       "                      [ 0.0137, -0.0143,  0.0153,  ..., -0.0095,  0.0008,  0.0115],\n",
       "                      [ 0.0026, -0.0131, -0.0190,  ...,  0.0107,  0.0106, -0.0261],\n",
       "                      ...,\n",
       "                      [-0.0003, -0.0139,  0.0195,  ...,  0.0039,  0.0214, -0.0041],\n",
       "                      [ 0.0172,  0.0143, -0.0072,  ...,  0.0088,  0.0029, -0.0062],\n",
       "                      [-0.0198,  0.0262, -0.0005,  ...,  0.0038, -0.0150, -0.0057]])),\n",
       "             ('decoder.layers.29.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24a9d0>),\n",
       "             ('decoder.layers.29.mlp.linear_fc2.weight',\n",
       "              tensor([[-0.0101, -0.0014,  0.0172,  ...,  0.0034, -0.0042,  0.0026],\n",
       "                      [ 0.0184, -0.0133, -0.0010,  ...,  0.0115, -0.0079,  0.0020],\n",
       "                      [-0.0065, -0.0029,  0.0104,  ...,  0.0216,  0.0052,  0.0058],\n",
       "                      ...,\n",
       "                      [-0.0018,  0.0022, -0.0221,  ..., -0.0037,  0.0166,  0.0008],\n",
       "                      [ 0.0039,  0.0076, -0.0014,  ...,  0.0195,  0.0226,  0.0036],\n",
       "                      [-0.0112, -0.0087,  0.0076,  ..., -0.0095,  0.0276, -0.0228]])),\n",
       "             ('decoder.layers.29.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24aa70>),\n",
       "             ('decoder.layers.30.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24aac0>),\n",
       "             ('decoder.layers.30.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0182, -0.0138,  0.0059,  ..., -0.0140,  0.0135, -0.0138],\n",
       "                      [ 0.0096, -0.0120, -0.0167,  ..., -0.0059, -0.0154, -0.0144],\n",
       "                      [ 0.0050,  0.0188, -0.0312,  ...,  0.0145, -0.0201,  0.0299],\n",
       "                      ...,\n",
       "                      [-0.0124, -0.0277,  0.0074,  ..., -0.0139, -0.0011,  0.0015],\n",
       "                      [ 0.0247,  0.0018,  0.0260,  ...,  0.0069,  0.0240, -0.0034],\n",
       "                      [ 0.0134, -0.0101,  0.0103,  ..., -0.0089, -0.0278, -0.0074]])),\n",
       "             ('decoder.layers.30.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24ab60>),\n",
       "             ('decoder.layers.30.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.5000, 0.5000, 0.4766,  ..., 0.5078, 0.4727, 0.4531])),\n",
       "             ('decoder.layers.30.self_attention.linear_qkv.weight',\n",
       "              tensor([[ 0.0195,  0.0017,  0.0204,  ...,  0.0113, -0.0078, -0.0062],\n",
       "                      [ 0.0200, -0.0505,  0.0162,  ...,  0.0033,  0.0289,  0.0164],\n",
       "                      [-0.0211, -0.0210, -0.0182,  ...,  0.0004,  0.0082, -0.0043],\n",
       "                      ...,\n",
       "                      [-0.0177, -0.0062,  0.0349,  ..., -0.0155,  0.0074, -0.0052],\n",
       "                      [-0.0222, -0.0168, -0.0194,  ...,  0.0031,  0.0149, -0.0178],\n",
       "                      [-0.0233, -0.0140, -0.0226,  ...,  0.0298,  0.0171,  0.0133]])),\n",
       "             ('decoder.layers.30.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24ac50>),\n",
       "             ('decoder.layers.30.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.6016, 0.5938, 0.5898,  ..., 0.5898, 0.6094, 0.5938])),\n",
       "             ('decoder.layers.30.mlp.linear_fc1.weight',\n",
       "              tensor([[-0.0029, -0.0140, -0.0122,  ..., -0.0042, -0.0036, -0.0004],\n",
       "                      [ 0.0029,  0.0089, -0.0187,  ...,  0.0084,  0.0089, -0.0066],\n",
       "                      [-0.0041, -0.0040, -0.0035,  ..., -0.0008,  0.0134, -0.0039],\n",
       "                      ...,\n",
       "                      [-0.0019,  0.0125,  0.0139,  ...,  0.0009,  0.0056, -0.0034],\n",
       "                      [ 0.0154,  0.0086, -0.0208,  ...,  0.0020, -0.0009, -0.0030],\n",
       "                      [-0.0021,  0.0022, -0.0046,  ...,  0.0095,  0.0035, -0.0103]])),\n",
       "             ('decoder.layers.30.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24ad40>),\n",
       "             ('decoder.layers.30.mlp.linear_fc2.weight',\n",
       "              tensor([[-0.0170, -0.0019,  0.0036,  ...,  0.0175, -0.0195,  0.0165],\n",
       "                      [-0.0004, -0.0063, -0.0132,  ..., -0.0271, -0.0019, -0.0010],\n",
       "                      [-0.0030,  0.0039, -0.0120,  ..., -0.0139, -0.0109, -0.0082],\n",
       "                      ...,\n",
       "                      [ 0.0028,  0.0077,  0.0060,  ...,  0.0058, -0.0063,  0.0063],\n",
       "                      [ 0.0004, -0.0068, -0.0044,  ...,  0.0193,  0.0233,  0.0200],\n",
       "                      [ 0.0069, -0.0028,  0.0061,  ...,  0.0107, -0.0238,  0.0052]])),\n",
       "             ('decoder.layers.30.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24ade0>),\n",
       "             ('decoder.layers.31.self_attention.core_attention.fused_attention._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24ae30>),\n",
       "             ('decoder.layers.31.self_attention.linear_proj.weight',\n",
       "              tensor([[ 0.0199, -0.0128, -0.0223,  ...,  0.0118, -0.0031,  0.0134],\n",
       "                      [ 0.0003, -0.0035, -0.0044,  ...,  0.0014, -0.0001,  0.0105],\n",
       "                      [-0.0073,  0.0129, -0.0082,  ..., -0.0009,  0.0072, -0.0061],\n",
       "                      ...,\n",
       "                      [ 0.0019,  0.0116, -0.0045,  ..., -0.0007, -0.0011,  0.0127],\n",
       "                      [-0.0167,  0.0031,  0.0167,  ..., -0.0004, -0.0119,  0.0073],\n",
       "                      [ 0.0179, -0.0021,  0.0160,  ..., -0.0198,  0.0091, -0.0002]])),\n",
       "             ('decoder.layers.31.self_attention.linear_proj._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24aed0>),\n",
       "             ('decoder.layers.31.self_attention.linear_qkv.layer_norm_weight',\n",
       "              tensor([0.4648, 0.4004, 0.4453,  ..., 0.4297, 0.3906, 0.3008])),\n",
       "             ('decoder.layers.31.self_attention.linear_qkv.weight',\n",
       "              tensor([[ 8.3618e-03, -3.4790e-03,  7.8735e-03,  ..., -6.8665e-04,\n",
       "                       -1.4114e-03, -1.9684e-03],\n",
       "                      [ 3.1128e-03, -1.5717e-03,  1.3428e-02,  ..., -1.5991e-02,\n",
       "                       -3.8452e-03,  1.1902e-03],\n",
       "                      [ 7.1716e-03,  1.2207e-02, -1.0376e-02,  ...,  2.9504e-06,\n",
       "                       -2.6733e-02,  3.2501e-03],\n",
       "                      ...,\n",
       "                      [ 7.5684e-03,  2.7275e-04, -2.4986e-04,  ...,  9.6436e-03,\n",
       "                       -4.6997e-03, -4.3335e-03],\n",
       "                      [-1.5945e-03, -4.0588e-03, -7.3242e-03,  ..., -1.0742e-02,\n",
       "                       -1.4801e-03, -2.1362e-02],\n",
       "                      [-2.3071e-02,  2.2278e-03,  5.6763e-03,  ...,  6.8970e-03,\n",
       "                       -9.5825e-03, -6.9275e-03]])),\n",
       "             ('decoder.layers.31.self_attention.linear_qkv._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24afc0>),\n",
       "             ('decoder.layers.31.mlp.linear_fc1.layer_norm_weight',\n",
       "              tensor([0.5273, 0.4902, 0.5078,  ..., 0.5273, 0.4883, 0.4258])),\n",
       "             ('decoder.layers.31.mlp.linear_fc1.weight',\n",
       "              tensor([[-1.4160e-02,  1.5503e-02, -2.3071e-02,  ...,  1.3855e-02,\n",
       "                        4.3640e-03,  6.8054e-03],\n",
       "                      [-8.6212e-04,  7.7209e-03,  7.7209e-03,  ...,  1.5381e-02,\n",
       "                        7.3547e-03,  1.2207e-02],\n",
       "                      [-1.1169e-02,  2.0264e-02, -1.4099e-02,  ...,  6.4697e-03,\n",
       "                       -7.1411e-03, -9.1553e-05],\n",
       "                      ...,\n",
       "                      [ 2.6245e-02,  1.7929e-03,  1.8677e-02,  ...,  4.8828e-03,\n",
       "                        1.1108e-02, -3.4571e-05],\n",
       "                      [-4.8218e-03,  1.3367e-02, -8.7280e-03,  ...,  6.3171e-03,\n",
       "                        1.1292e-03,  1.1841e-02],\n",
       "                      [ 9.5825e-03, -1.3855e-02, -2.0294e-03,  ..., -4.5776e-03,\n",
       "                       -3.0670e-03, -9.1553e-03]])),\n",
       "             ('decoder.layers.31.mlp.linear_fc1._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24b0b0>),\n",
       "             ('decoder.layers.31.mlp.linear_fc2.weight',\n",
       "              tensor([[-0.0344,  0.0157,  0.0188,  ...,  0.0018,  0.0121,  0.0007],\n",
       "                      [-0.0042, -0.0064,  0.0035,  ..., -0.0035,  0.0032, -0.0089],\n",
       "                      [-0.0090, -0.0223, -0.0219,  ...,  0.0029, -0.0059, -0.0084],\n",
       "                      ...,\n",
       "                      [ 0.0126,  0.0019, -0.0042,  ..., -0.0164, -0.0074,  0.0112],\n",
       "                      [-0.0132,  0.0069,  0.0142,  ..., -0.0195,  0.0114,  0.0111],\n",
       "                      [-0.0074, -0.0138,  0.0107,  ...,  0.0106,  0.0027,  0.0029]])),\n",
       "             ('decoder.layers.31.mlp.linear_fc2._extra_state',\n",
       "              <_io.BytesIO at 0x151d8f24b150>),\n",
       "             ('decoder.final_layernorm.weight',\n",
       "              tensor([2.6562, 2.5781, 2.6094,  ..., 2.5938, 2.2656, 2.5156])),\n",
       "             ('output_layer.weight',\n",
       "              tensor([[ 0.0099,  0.0173,  0.0034,  ...,  0.0007, -0.0168, -0.0110],\n",
       "                      [-0.0069,  0.0117,  0.0112,  ..., -0.0085,  0.0091, -0.0015],\n",
       "                      [ 0.0143,  0.0096,  0.0089,  ..., -0.0024, -0.0062, -0.0142],\n",
       "                      ...,\n",
       "                      [-0.0034,  0.0020,  0.0058,  ...,  0.0015,  0.0059,  0.0069],\n",
       "                      [-0.0034,  0.0020,  0.0058,  ...,  0.0015,  0.0059,  0.0069],\n",
       "                      [-0.0034,  0.0020,  0.0058,  ...,  0.0015,  0.0059,  0.0069]])),\n",
       "             ('output_layer._extra_state', None)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.word_embeddings.weight\n",
      "Shape:  torch.Size([128256, 4096])\n",
      "\n",
      "decoder.layers.0.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2345e0>\n",
      "\n",
      "decoder.layers.0.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.0.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f234680>\n",
      "\n",
      "decoder.layers.0.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.0.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.0.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f234770>\n",
      "\n",
      "decoder.layers.0.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.0.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.0.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f234860>\n",
      "\n",
      "decoder.layers.0.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.0.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f234900>\n",
      "\n",
      "decoder.layers.1.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f234950>\n",
      "\n",
      "decoder.layers.1.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.1.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2349f0>\n",
      "\n",
      "decoder.layers.1.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.1.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.1.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f234ae0>\n",
      "\n",
      "decoder.layers.1.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.1.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.1.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d93575300>\n",
      "\n",
      "decoder.layers.1.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.1.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f213d80>\n",
      "\n",
      "decoder.layers.2.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f213740>\n",
      "\n",
      "decoder.layers.2.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.2.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f234bd0>\n",
      "\n",
      "decoder.layers.2.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.2.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.2.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f234cc0>\n",
      "\n",
      "decoder.layers.2.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.2.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.2.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f234db0>\n",
      "\n",
      "decoder.layers.2.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.2.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f234e50>\n",
      "\n",
      "decoder.layers.3.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f234ea0>\n",
      "\n",
      "decoder.layers.3.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.3.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f234f40>\n",
      "\n",
      "decoder.layers.3.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.3.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.3.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235030>\n",
      "\n",
      "decoder.layers.3.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.3.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.3.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235120>\n",
      "\n",
      "decoder.layers.3.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.3.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2351c0>\n",
      "\n",
      "decoder.layers.4.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235210>\n",
      "\n",
      "decoder.layers.4.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.4.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2352b0>\n",
      "\n",
      "decoder.layers.4.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.4.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.4.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2353a0>\n",
      "\n",
      "decoder.layers.4.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.4.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.4.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235490>\n",
      "\n",
      "decoder.layers.4.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.4.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235530>\n",
      "\n",
      "decoder.layers.5.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235580>\n",
      "\n",
      "decoder.layers.5.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.5.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235620>\n",
      "\n",
      "decoder.layers.5.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.5.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.5.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235710>\n",
      "\n",
      "decoder.layers.5.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.5.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.5.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235800>\n",
      "\n",
      "decoder.layers.5.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.5.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2358a0>\n",
      "\n",
      "decoder.layers.6.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2358f0>\n",
      "\n",
      "decoder.layers.6.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.6.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235990>\n",
      "\n",
      "decoder.layers.6.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.6.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.6.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235a80>\n",
      "\n",
      "decoder.layers.6.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.6.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.6.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235b70>\n",
      "\n",
      "decoder.layers.6.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.6.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235c10>\n",
      "\n",
      "decoder.layers.7.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235c60>\n",
      "\n",
      "decoder.layers.7.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.7.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235d00>\n",
      "\n",
      "decoder.layers.7.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.7.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.7.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235df0>\n",
      "\n",
      "decoder.layers.7.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.7.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.7.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235ee0>\n",
      "\n",
      "decoder.layers.7.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.7.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235f80>\n",
      "\n",
      "decoder.layers.8.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f235fd0>\n",
      "\n",
      "decoder.layers.8.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.8.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236070>\n",
      "\n",
      "decoder.layers.8.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.8.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.8.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236160>\n",
      "\n",
      "decoder.layers.8.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.8.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.8.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236250>\n",
      "\n",
      "decoder.layers.8.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.8.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2362f0>\n",
      "\n",
      "decoder.layers.9.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236340>\n",
      "\n",
      "decoder.layers.9.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.9.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2363e0>\n",
      "\n",
      "decoder.layers.9.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.9.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.9.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2364d0>\n",
      "\n",
      "decoder.layers.9.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.9.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.9.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2365c0>\n",
      "\n",
      "decoder.layers.9.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.9.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236660>\n",
      "\n",
      "decoder.layers.10.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2366b0>\n",
      "\n",
      "decoder.layers.10.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.10.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236750>\n",
      "\n",
      "decoder.layers.10.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.10.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.10.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236840>\n",
      "\n",
      "decoder.layers.10.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.10.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.10.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236930>\n",
      "\n",
      "decoder.layers.10.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.10.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2369d0>\n",
      "\n",
      "decoder.layers.11.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236a20>\n",
      "\n",
      "decoder.layers.11.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.11.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236ac0>\n",
      "\n",
      "decoder.layers.11.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.11.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.11.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236bb0>\n",
      "\n",
      "decoder.layers.11.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.11.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.11.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236ca0>\n",
      "\n",
      "decoder.layers.11.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.11.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236d40>\n",
      "\n",
      "decoder.layers.12.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236d90>\n",
      "\n",
      "decoder.layers.12.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.12.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236e30>\n",
      "\n",
      "decoder.layers.12.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.12.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.12.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f236f20>\n",
      "\n",
      "decoder.layers.12.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.12.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.12.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237010>\n",
      "\n",
      "decoder.layers.12.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.12.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2370b0>\n",
      "\n",
      "decoder.layers.13.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237100>\n",
      "\n",
      "decoder.layers.13.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.13.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2371a0>\n",
      "\n",
      "decoder.layers.13.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.13.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.13.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237290>\n",
      "\n",
      "decoder.layers.13.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.13.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.13.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237380>\n",
      "\n",
      "decoder.layers.13.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.13.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237420>\n",
      "\n",
      "decoder.layers.14.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237470>\n",
      "\n",
      "decoder.layers.14.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.14.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237510>\n",
      "\n",
      "decoder.layers.14.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.14.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.14.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237600>\n",
      "\n",
      "decoder.layers.14.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.14.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.14.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2376f0>\n",
      "\n",
      "decoder.layers.14.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.14.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237790>\n",
      "\n",
      "decoder.layers.15.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2377e0>\n",
      "\n",
      "decoder.layers.15.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.15.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237880>\n",
      "\n",
      "decoder.layers.15.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.15.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.15.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237970>\n",
      "\n",
      "decoder.layers.15.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.15.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.15.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237a60>\n",
      "\n",
      "decoder.layers.15.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.15.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237ab0>\n",
      "\n",
      "decoder.layers.16.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237b00>\n",
      "\n",
      "decoder.layers.16.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.16.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237ba0>\n",
      "\n",
      "decoder.layers.16.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.16.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.16.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237c90>\n",
      "\n",
      "decoder.layers.16.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.16.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.16.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237d80>\n",
      "\n",
      "decoder.layers.16.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.16.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237e20>\n",
      "\n",
      "decoder.layers.17.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237e70>\n",
      "\n",
      "decoder.layers.17.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.17.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f237f10>\n",
      "\n",
      "decoder.layers.17.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.17.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.17.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248040>\n",
      "\n",
      "decoder.layers.17.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.17.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.17.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248130>\n",
      "\n",
      "decoder.layers.17.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.17.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2481d0>\n",
      "\n",
      "decoder.layers.18.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248220>\n",
      "\n",
      "decoder.layers.18.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.18.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2482c0>\n",
      "\n",
      "decoder.layers.18.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.18.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.18.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2483b0>\n",
      "\n",
      "decoder.layers.18.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.18.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.18.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2484a0>\n",
      "\n",
      "decoder.layers.18.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.18.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248540>\n",
      "\n",
      "decoder.layers.19.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248590>\n",
      "\n",
      "decoder.layers.19.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.19.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248630>\n",
      "\n",
      "decoder.layers.19.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.19.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.19.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248720>\n",
      "\n",
      "decoder.layers.19.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.19.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.19.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248810>\n",
      "\n",
      "decoder.layers.19.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.19.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2488b0>\n",
      "\n",
      "decoder.layers.20.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248900>\n",
      "\n",
      "decoder.layers.20.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.20.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2489a0>\n",
      "\n",
      "decoder.layers.20.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.20.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.20.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248a90>\n",
      "\n",
      "decoder.layers.20.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.20.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.20.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248b80>\n",
      "\n",
      "decoder.layers.20.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.20.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248c20>\n",
      "\n",
      "decoder.layers.21.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248c70>\n",
      "\n",
      "decoder.layers.21.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.21.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248d10>\n",
      "\n",
      "decoder.layers.21.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.21.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.21.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248e00>\n",
      "\n",
      "decoder.layers.21.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.21.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.21.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248ef0>\n",
      "\n",
      "decoder.layers.21.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.21.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248f90>\n",
      "\n",
      "decoder.layers.22.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f248fe0>\n",
      "\n",
      "decoder.layers.22.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.22.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249080>\n",
      "\n",
      "decoder.layers.22.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.22.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.22.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249170>\n",
      "\n",
      "decoder.layers.22.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.22.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.22.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249260>\n",
      "\n",
      "decoder.layers.22.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.22.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249300>\n",
      "\n",
      "decoder.layers.23.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249350>\n",
      "\n",
      "decoder.layers.23.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.23.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2493f0>\n",
      "\n",
      "decoder.layers.23.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.23.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.23.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2494e0>\n",
      "\n",
      "decoder.layers.23.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.23.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.23.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2495d0>\n",
      "\n",
      "decoder.layers.23.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.23.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249670>\n",
      "\n",
      "decoder.layers.24.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2496c0>\n",
      "\n",
      "decoder.layers.24.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.24.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249760>\n",
      "\n",
      "decoder.layers.24.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.24.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.24.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249850>\n",
      "\n",
      "decoder.layers.24.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.24.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.24.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249940>\n",
      "\n",
      "decoder.layers.24.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.24.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f2499e0>\n",
      "\n",
      "decoder.layers.25.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249a30>\n",
      "\n",
      "decoder.layers.25.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.25.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249ad0>\n",
      "\n",
      "decoder.layers.25.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.25.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.25.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249bc0>\n",
      "\n",
      "decoder.layers.25.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.25.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.25.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249c60>\n",
      "\n",
      "decoder.layers.25.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.25.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249d00>\n",
      "\n",
      "decoder.layers.26.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249d50>\n",
      "\n",
      "decoder.layers.26.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.26.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249df0>\n",
      "\n",
      "decoder.layers.26.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.26.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.26.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249ee0>\n",
      "\n",
      "decoder.layers.26.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.26.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.26.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f249fd0>\n",
      "\n",
      "decoder.layers.26.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.26.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24a070>\n",
      "\n",
      "decoder.layers.27.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24a0c0>\n",
      "\n",
      "decoder.layers.27.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.27.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24a160>\n",
      "\n",
      "decoder.layers.27.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.27.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.27.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24a250>\n",
      "\n",
      "decoder.layers.27.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.27.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.27.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24a2f0>\n",
      "\n",
      "decoder.layers.27.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.27.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24a390>\n",
      "\n",
      "decoder.layers.28.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24a3e0>\n",
      "\n",
      "decoder.layers.28.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.28.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24a480>\n",
      "\n",
      "decoder.layers.28.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.28.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.28.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24a570>\n",
      "\n",
      "decoder.layers.28.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.28.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.28.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24a660>\n",
      "\n",
      "decoder.layers.28.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.28.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24a700>\n",
      "\n",
      "decoder.layers.29.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24a750>\n",
      "\n",
      "decoder.layers.29.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.29.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24a7f0>\n",
      "\n",
      "decoder.layers.29.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.29.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.29.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24a8e0>\n",
      "\n",
      "decoder.layers.29.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.29.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.29.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24a9d0>\n",
      "\n",
      "decoder.layers.29.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.29.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24aa70>\n",
      "\n",
      "decoder.layers.30.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24aac0>\n",
      "\n",
      "decoder.layers.30.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.30.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24ab60>\n",
      "\n",
      "decoder.layers.30.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.30.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.30.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24ac50>\n",
      "\n",
      "decoder.layers.30.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.30.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.30.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24ad40>\n",
      "\n",
      "decoder.layers.30.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.30.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24ade0>\n",
      "\n",
      "decoder.layers.31.self_attention.core_attention.fused_attention._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24ae30>\n",
      "\n",
      "decoder.layers.31.self_attention.linear_proj.weight\n",
      "Shape:  torch.Size([4096, 4096])\n",
      "\n",
      "decoder.layers.31.self_attention.linear_proj._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24aed0>\n",
      "\n",
      "decoder.layers.31.self_attention.linear_qkv.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.31.self_attention.linear_qkv.weight\n",
      "Shape:  torch.Size([6144, 4096])\n",
      "\n",
      "decoder.layers.31.self_attention.linear_qkv._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24afc0>\n",
      "\n",
      "decoder.layers.31.mlp.linear_fc1.layer_norm_weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "decoder.layers.31.mlp.linear_fc1.weight\n",
      "Shape:  torch.Size([28672, 4096])\n",
      "\n",
      "decoder.layers.31.mlp.linear_fc1._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24b0b0>\n",
      "\n",
      "decoder.layers.31.mlp.linear_fc2.weight\n",
      "Shape:  torch.Size([4096, 14336])\n",
      "\n",
      "decoder.layers.31.mlp.linear_fc2._extra_state\n",
      "<_io.BytesIO object at 0x151d8f24b150>\n",
      "\n",
      "decoder.final_layernorm.weight\n",
      "Shape:  torch.Size([4096])\n",
      "\n",
      "output_layer.weight\n",
      "Shape:  torch.Size([128256, 4096])\n",
      "\n",
      "output_layer._extra_state\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in loaded[\"model\"].keys():\n",
    "    print(key)\n",
    "    if hasattr(loaded[\"model\"][key],\"shape\"):\n",
    "        print('Shape: ',loaded[\"model\"][key].shape)\n",
    "    else:\n",
    "        print(loaded[\"model\"][key])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 4/4 [01:46<00:00, 26.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/home/tian/mtian8/LLM4Tutorial/model/HF_model/Meta-Llama-3-8B\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 4096])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.4.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.5.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.6.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.7.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.8.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.9.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.10.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.11.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.12.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.13.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.14.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.15.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.16.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.17.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.18.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.19.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.20.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.21.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.22.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.23.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.24.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.25.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.26.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.27.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.28.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.29.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.30.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.31.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.norm.weight torch.Size([4096])\n",
      "lm_head.weight torch.Size([128256, 4096])\n"
     ]
    }
   ],
   "source": [
    "for key, value in model.named_parameters():\n",
    "    print(key, value.shape)\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict([('inv_freq',\n",
       "               tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,\n",
       "                       2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,\n",
       "                       8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,\n",
       "                       2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,\n",
       "                       7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,\n",
       "                       2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,\n",
       "                       6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,\n",
       "                       1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,\n",
       "                       5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,\n",
       "                       1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,\n",
       "                       4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06]))]),\n",
       " '_non_persistent_buffers_set': {'inv_freq'},\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'rope_kwargs': {},\n",
       " 'rope_type': 'default',\n",
       " 'max_seq_len_cached': 8192,\n",
       " 'original_max_seq_len': 8192,\n",
       " 'config': LlamaConfig {\n",
       "   \"_name_or_path\": \"/home/tian/mtian8/LLM4Tutorial/model/HF_model/Meta-Llama-3-8B\",\n",
       "   \"architectures\": [\n",
       "     \"LlamaForCausalLM\"\n",
       "   ],\n",
       "   \"attention_bias\": false,\n",
       "   \"attention_dropout\": 0.0,\n",
       "   \"bos_token_id\": 128000,\n",
       "   \"eos_token_id\": 128001,\n",
       "   \"hidden_act\": \"silu\",\n",
       "   \"hidden_size\": 4096,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": 14336,\n",
       "   \"max_position_embeddings\": 8192,\n",
       "   \"mlp_bias\": false,\n",
       "   \"model_type\": \"llama\",\n",
       "   \"num_attention_heads\": 32,\n",
       "   \"num_hidden_layers\": 32,\n",
       "   \"num_key_value_heads\": 8,\n",
       "   \"pretraining_tp\": 1,\n",
       "   \"rms_norm_eps\": 1e-05,\n",
       "   \"rope_scaling\": null,\n",
       "   \"rope_theta\": 500000.0,\n",
       "   \"tie_word_embeddings\": false,\n",
       "   \"torch_dtype\": \"bfloat16\",\n",
       "   \"transformers_version\": \"4.43.3\",\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 128256\n",
       " },\n",
       " 'rope_init_fn': <function transformers.modeling_rope_utils._compute_default_rope_parameters(config: Optional[transformers.configuration_utils.PretrainedConfig] = None, device: Optional[ForwardRef('torch.device')] = None, seq_len: Optional[int] = None, **rope_kwargs) -> Tuple[ForwardRef('torch.Tensor'), float]>,\n",
       " 'attention_scaling': 1.0,\n",
       " 'original_inv_freq': tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,\n",
       "         2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,\n",
       "         8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,\n",
       "         2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,\n",
       "         7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,\n",
       "         2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,\n",
       "         6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,\n",
       "         1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,\n",
       "         5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,\n",
       "         1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,\n",
       "         4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06]),\n",
       " '_is_hf_initialized': True}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.rotary_emb.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 2/2 [00:58<00:00, 29.15s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model_1 = AutoModelForCausalLM.from_pretrained(\"/home/tian/mtian8/LLM4Tutorial/model/HF_model/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([32000, 4096])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.4.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.5.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.6.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.7.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.8.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.9.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.10.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.11.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.12.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.13.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.14.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.15.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.16.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.17.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.18.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.19.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.20.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.21.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.22.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.23.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.24.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.25.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.26.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.27.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.28.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.29.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.30.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.31.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.norm.weight torch.Size([4096])\n",
      "lm_head.weight torch.Size([32000, 4096])\n"
     ]
    }
   ],
   "source": [
    "for key, value in model_1.named_parameters():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two models\n",
    "import torch\n",
    "def diff(model1, model2):  \n",
    "    for name1, param1 in model1.named_parameters():\n",
    "        param2 = model2.state_dict()[name1]\n",
    "        if not torch.equal(param1, param2):\n",
    "            print(name1, \"different by\", torch.norm(param1 - param2).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mtian8/anaconda3/envs/HF/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|| 9/9 [04:31<00:00, 30.18s/it]\n",
      "Loading checkpoint shards: 100%|| 19/19 [04:36<00:00, 14.56s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model_1 = AutoModelForCausalLM.from_pretrained(\"/home/tian/mtian8/LLM4Tutorial/model/HF_model/Codestral-22B-v0.1\")\n",
    "model_2 = AutoModelForCausalLM.from_pretrained(\"/home/tian/mtian8/LLM4Tutorial/model/HF_converted_model/Codestral-22B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([32768, 6144])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.0.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.1.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.2.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.3.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.4.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.5.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.6.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.7.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.8.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.9.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.10.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.11.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.12.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.13.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.14.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.15.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.16.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.17.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.18.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.19.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.20.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.21.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.22.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.23.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.24.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.25.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.26.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.27.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.28.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.29.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.30.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.31.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.32.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.32.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.32.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.32.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.32.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.32.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.32.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.32.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.32.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.33.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.33.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.33.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.33.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.33.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.33.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.33.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.33.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.33.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.34.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.34.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.34.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.34.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.34.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.34.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.34.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.34.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.34.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.35.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.35.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.35.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.35.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.35.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.35.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.35.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.35.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.35.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.36.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.36.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.36.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.36.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.36.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.36.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.36.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.36.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.36.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.37.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.37.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.37.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.37.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.37.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.37.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.37.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.37.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.37.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.38.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.38.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.38.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.38.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.38.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.38.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.38.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.38.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.38.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.39.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.39.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.39.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.39.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.39.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.39.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.39.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.39.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.39.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.40.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.40.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.40.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.40.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.40.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.40.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.40.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.40.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.40.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.41.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.41.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.41.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.41.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.41.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.41.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.41.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.41.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.41.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.42.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.42.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.42.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.42.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.42.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.42.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.42.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.42.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.42.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.43.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.43.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.43.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.43.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.43.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.43.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.43.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.43.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.43.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.44.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.44.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.44.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.44.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.44.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.44.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.44.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.44.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.44.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.45.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.45.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.45.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.45.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.45.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.45.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.45.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.45.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.45.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.46.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.46.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.46.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.46.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.46.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.46.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.46.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.46.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.46.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.47.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.47.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.47.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.47.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.47.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.47.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.47.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.47.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.47.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.48.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.48.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.48.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.48.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.48.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.48.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.48.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.48.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.48.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.49.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.49.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.49.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.49.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.49.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.49.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.49.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.49.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.49.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.50.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.50.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.50.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.50.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.50.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.50.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.50.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.50.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.50.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.51.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.51.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.51.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.51.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.51.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.51.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.51.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.51.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.51.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.52.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.52.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.52.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.52.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.52.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.52.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.52.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.52.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.52.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.53.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.53.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.53.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.53.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.53.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.53.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.53.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.53.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.53.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.54.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.54.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.54.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.54.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.54.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.54.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.54.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.54.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.54.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.55.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.55.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.55.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.55.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.55.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.55.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.55.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.55.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.55.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.norm.weight torch.Size([6144])\n",
      "lm_head.weight torch.Size([32768, 6144])\n"
     ]
    }
   ],
   "source": [
    "for key, value in model_1.named_parameters():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([32768, 6144])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.0.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.1.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.2.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.3.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.4.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.5.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.6.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.7.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.8.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.9.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.10.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.11.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.12.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.13.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.14.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.15.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.16.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.17.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.18.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.19.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.20.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.21.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.22.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.23.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.24.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.25.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.26.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.27.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.28.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.29.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.30.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.31.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.32.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.32.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.32.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.32.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.32.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.32.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.32.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.32.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.32.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.33.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.33.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.33.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.33.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.33.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.33.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.33.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.33.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.33.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.34.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.34.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.34.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.34.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.34.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.34.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.34.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.34.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.34.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.35.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.35.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.35.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.35.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.35.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.35.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.35.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.35.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.35.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.36.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.36.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.36.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.36.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.36.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.36.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.36.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.36.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.36.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.37.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.37.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.37.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.37.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.37.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.37.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.37.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.37.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.37.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.38.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.38.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.38.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.38.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.38.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.38.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.38.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.38.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.38.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.39.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.39.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.39.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.39.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.39.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.39.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.39.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.39.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.39.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.40.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.40.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.40.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.40.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.40.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.40.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.40.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.40.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.40.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.41.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.41.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.41.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.41.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.41.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.41.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.41.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.41.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.41.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.42.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.42.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.42.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.42.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.42.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.42.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.42.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.42.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.42.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.43.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.43.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.43.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.43.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.43.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.43.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.43.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.43.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.43.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.44.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.44.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.44.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.44.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.44.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.44.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.44.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.44.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.44.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.45.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.45.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.45.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.45.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.45.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.45.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.45.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.45.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.45.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.46.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.46.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.46.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.46.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.46.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.46.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.46.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.46.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.46.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.47.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.47.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.47.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.47.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.47.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.47.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.47.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.47.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.47.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.48.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.48.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.48.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.48.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.48.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.48.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.48.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.48.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.48.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.49.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.49.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.49.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.49.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.49.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.49.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.49.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.49.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.49.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.50.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.50.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.50.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.50.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.50.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.50.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.50.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.50.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.50.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.51.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.51.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.51.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.51.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.51.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.51.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.51.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.51.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.51.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.52.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.52.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.52.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.52.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.52.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.52.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.52.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.52.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.52.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.53.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.53.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.53.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.53.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.53.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.53.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.53.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.53.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.53.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.54.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.54.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.54.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.54.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.54.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.54.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.54.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.54.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.54.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.layers.55.self_attn.q_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.55.self_attn.k_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.55.self_attn.v_proj.weight torch.Size([1024, 6144])\n",
      "model.layers.55.self_attn.o_proj.weight torch.Size([6144, 6144])\n",
      "model.layers.55.mlp.gate_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.55.mlp.up_proj.weight torch.Size([16384, 6144])\n",
      "model.layers.55.mlp.down_proj.weight torch.Size([6144, 16384])\n",
      "model.layers.55.input_layernorm.weight torch.Size([6144])\n",
      "model.layers.55.post_attention_layernorm.weight torch.Size([6144])\n",
      "model.norm.weight torch.Size([6144])\n",
      "lm_head.weight torch.Size([32768, 6144])\n"
     ]
    }
   ],
   "source": [
    "for key, value in model_2.named_parameters():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "train_ds = datasets.load_from_disk('/lcrc/project/TuningLLMs/du/datasets/slimpajama_per_source_downsample/slimpajama_packed_32768_5b_per_source_down_sample_0.1.hf')\n",
    "try:\n",
    "    train_ds = train_ds['train']\n",
    "except Exception:\n",
    "    pass\n",
    "train_ds = train_ds.with_format(\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149856, 32768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a =np.array([  654, 28649,  1413, 16969,   352, 24004,   271,  2751, 28705, 28781,\n",
    "         28723, 28734, 28723, 28782,   325, 19658,  2383, 24142, 28725,  3885,\n",
    "          6136, 28725, 15472, 28745,  3550,  1508,  2849, 28723,  4179,  2383,\n",
    "         24142, 28723,   675, 28731,   354, 19492,   304,  5166,  6085, 16149])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds['input_ids'][0][:40].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(149856):\n",
    "    print(i)\n",
    "    # compare 2 numpy arrays are the same or not\n",
    "    if np.array_equal(train_ds['input_ids'][i,:40], a):\n",
    "        print(\"Found\",i)\n",
    "        print(train_ds['input_ids'][i][:40])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mtian8/anaconda3/envs/mg-lm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/tian/mtian8/LLM4Tutorial/model/HF_model/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Can be done in the comfort of your own home!\\nOn the verge of Diabetes and Heart Disease? Medical bills can be expensive. Act now!\\nFast Training USA is a fitness center and one of the largest providers of privately labeled nutritional products in the United States! We offer our products to private business owners and trainers at an affordable price without any bulk orders.\\nWe take an innovative approach on health & fitness. By combining our specially designed products that make you feel fuller faster while making you eat less, burn fat, speed metabolism, and block fat and carbs combined with personalized meal plans that get your body to strictly burn fat as it\\'s only energy source, will make you lose weight in a safe, fast, healthy way. Additionally, we offer personal training to firm up the body and get you in the best shape of your life. Our in-house health consultants and personal trainers, will create a weight loss program for you that is guaranteed to help you lose the weight you\\'ve always wanted  faster than you ever imagined possible.\\nFAsT Training USA is Miami\\'s one stop Fitness and Weight Loss Center.<s> Born on April 21, 1931, Dene died on January 6, 2012 in Vernon, BC after losing a battle with pulmonary fibrosis. Dene has gone to be with her Lord and Saviour in Heaven and will be dearly missed by her entire family.\\nDene was born on the family farm in Swift Current, Saskatchewan, eventually moving with her family to Qualicum Beach on Vancouver Island where she attended high school. In 1951, she married James Steven and in 2011 they celebrated their 60th wedding anniversary.\\nDene graduated with honors from UBC as a pharmacist in 1953 and practiced pharmacy until 1987, although she continued to be a wise and studious resource on medical issues for her family and friends. As part of Dene\\'s desire to help people find healthy solutions for life, she was very passionate in her Christian ministry and service for over 50 years.\\nDene was very close and dear to her three younger siblings: Dennis Sackett (Anne), Lorraine Jacklin (Buzz), and Suellen Dodd (Alan). Dene was the much loved wife of James Edward Steven, and wonderful mother and friend to her four children, Douglas (Cindy), Debbie, Rik (Ralph) and Janette (Dean). Dene leaves behind her five grandchildren, Shelby (Dan), Courtenay (Morgan), Tyrel, Roxanne, Mathew, and four great-grandchildren, Emily, Avery, Rowan and Wentworth.<s> Zenobia Galar (born Zenobia Terrero Galarza; 3 May 1958) is a Dominican painter.\\n\\nBiography\\nZenobia Terrero Galarza was born in Enriquillo, Barahona on 3 May 1958, the daughter of professor Elena Galarza de Terrero and the musician and soldier Francisco Corpus Terrero.\\n\\nShe is considered an important canvas artist in the Dominican Republic, and her works are part of multiple private collections in her country and abroad. She studied graphic and advertising arts at the School of Arts of the Universidad Autnoma de Santo Domingo (UASD), where she also studied the history of art and history of civilization. She attended the  from 1975 to 1976, and the Crculo de Bellas Artes in Madrid, Spain in 1979.\\n\\nGalar has served as a board member of the Dominican College of Plastic Artists, and was a painting teacher in its beginnings at educational institutions, as well as in its Painting Workshop, where she is still an instructor for both children and adults. She is also a lecturer and instructor in art subjects.\\n\\nShe has participated in various group exhibitions both locally and internationally in Spain and Colombia.\\n\\nStyle\\n\\nGalar prefers acrylic and oil, but also uses mixed techniques, such as collage. Her style is based on figurative neo-expressionism.\\n\\nAccording to the art analyst Marianne De Tolentino,\\n\\nIndividual exhibitions\\n\\nCollective exhibitions\\n\\nReferences\\n\\n1958 births\\n20th-century Dominican Republic painters\\nDominican Republic women painters\\nLiving people\\nNeo-expressionist artists\\nPeople from Barahona Province\\n21st-century Dominican Republic artists\\n21st-century painters\\n20th-century women artists\\n21st-century women artists\\nUniversidad Autnoma de Santo Domingo alumni<s> Temporary prohibition of traffic: Northwich - Chester Way\\nTemporary prohibition of traffic\\nNotice is hereby given pursuant to the provisions of Section 14 of the Road Traffic Regulation Act 1984 that the Cheshire West and Chester Borough Council intend to make/has made an Order the effect of which will be to temporarily prohibit/restrict traffic on the following length of road in Northwich to enable emergency works on Hayhurst Bridge works to be carried out by Canal and River Trust.\\nChester Way Northwich (Between London Road and Navigation Road)\\nNothing in the Order shall prevent driving upon the said lengths of road of any vehicle which is being used for the conveyance of persons goods or merchandise to or from any premises situate on or adjacent to the lengths of road or in connection with agriculture building construction works of repair and the like or use in an emergency of vehicles for fire brigade ambulance or police purposes. A through route will not always be available and any vehicles not requiring access to properties on the length of road should use the alternative routes.\\nThe Order will come into force on 1 October 2018 and will continue in force for a period not exceeding eighteen months or until the works which it is proposed to carry out have been completed whichever is the earlier. It is anticipated that the works will take approximately 41 nights to complete. The works have been extended to 17 November 2018.\\nMonday to Friday 10pm - 7am\\nWeekends 10pm - 12pm\\nGuilden Sutton Lane\\nGuilden Sutton\\nCheshire CH3 7EX\\nEmail: engch@cheshirewestandchester.gov.uk<s> <!DOCTYPE html>\\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\\n<head>\\n    <title></title>\\n    <link href=\"css/main.css\" rel=\"stylesheet\" />\\n    <script src=\"bower_components/jquery/dist/jquery.min.js\"></script>\\n    <script src=\"bower_components/angular/angular.min.js\"></script>\\n    <script src=\"bower_components/angular-ui-router/release/angular-ui-router.min.js\"></script>\\n    <script src=\"js/app.js\"></script>\\n    <script src=\"js/directives.js\"></script>\\n    <script src=\"js/controllers.js\"></script>\\n</head>\\n<body ng-app=\"testApp\">\\n    <h1>Test for responsive button</h1>\\n    <div ui-view=\"\"></div>\\n\\n</body>\\n</html>\\n<s> Find a Holiday\\nDiscover Scotland\\nOur Hotels in Scotland\\nArdgartan Hotel\\nNestled in a dramatic woodland glen and overlooked by the Arrochar Alps, the Ardgartan Hotel offers forest walks or time to simply relax and contemplate the spectacular views.\\nThe Ardgartan Hotel offers comfortable and accessible bedrooms with exceptional views. All bedrooms have an en-suite bathroom with a choice of bath or walk-in shower, as well as flat screen TV\\'s, hairdryer and tea making facilities.\\nThe impressive glass fronted foyer provides a relaxed space with panoramic views of Loch Long and an opportunity to soak up the scenery whilst the function room holds nightly entertainment for those who enjoy a dance.\\nFind a coach holiday Visit hotel site\\nWalks near The Ardgartan Hotel\\nArdgartan lies on the shores of Loch Long and is part of The Argyll Forest Park which stretches from the Holy Loch to the Firth of Clyde, incorporating the Arrochar Alps and the notorious Cobbler.\\nEnjoy the local surroundings and explore the deep woodland forests with a stroll along Croe Water. Alternatively, there are many way-marked trails to guide keen walkers through the hills, or for the adventurous there are plenty of summits to be conquered within the Arrochar Alps. The most iconic mountain in the area is Ben Arthur, commonly known as the Cobbler, an 882 metre ascent (2,900ft), which rewards walkers with magnificent views overlooking the Firth of Clyde to the south and Ben Lomond to the east.\\nCycle across Scotland\\nSo much of Scotland is still inaccessible by road but there are many well marked trails around Ardgartan and neighbouring Loch Lomond. Jump on the local bus to Tarbet where cycle hire is available from the pier. Speak to hotel reception for bus and ferry timetables.\\nArdgartan\\'s grounds are particularly rich in wildlife and the waters outside the hotel are often home to seals, otters and occasionally bottlenose dolphins. Oyster catchers, gannets, cormorants and ospreys are also regularly spotted. Red, roe and fallow deer are popular in the grounds too which makes Ardgartan the perfect location for nature lovers.\\nThe History of The Ardgartan Hotel\\nIn 2004 Lochs and Glens Holidays purchased the Ardgartan Estate on the shores of Loch Long and began the 8 year journey to complete the Hotel. After 6 years of negotiation with the planning authorities construction began in the spring of 2010 and the hotel opened its doors to guests a little over 2 years later.\\nThe Ardgartan Estate lies at the bottom of Glen Croe in the county of Argyll. The Estate had original ties with the Clan Campbell whose lands, from medieval times, extended across Argyll. The medieval building was destroyed at the end of the 18th century and was replaced with a well-appointed three storey home. The Campbells maintained ownership of Ardgartan for over 500 years until it was sold in the 1880s. Thereafter it changed hands several times, finally becoming the property of the Scottish Youth Hostel Association in 1936. In 1968 the old building closed and a new 82-bed Scandinavian-styled hostel was erected. This building was in operation until 2001 but the decline in hostelling, coupled with high maintenance costs were cited as the reasons for its closure. By the time Lochs and Glens Holidays acquired the site in 2004, the building was in a state of disrepair  derelict, damp and vandalised.\\nAfter 2, 300 tons of concrete, 12 miles of wallpaper and 6,000 square metres of carpet the Ardgartan Hotel welcomed its first guests in July 2012.\\nardgartanhotel@lochsandglens.com\\nArdgartan Hotel ARROCHAR Argyll & Bute G83 7AR\\nLochs & Glens Holidays, School Road, Gartocharn, Dunbartonshire G83 8RW\\nt: 01389 713 713 e: enquiries@lochsandglens.comVAT Number GB 415 4314 82\\nM F Wells Hotels Limited t/a Lochs & Glens Holidays is an Appointed Representative of ITC Compliance Limited who are authorised and regulated by the Financial Conduct Authority (their firm reference is 313486) and which is permitted to advise on and arrange general insurance contracts.\\nIs My Money Safe?<s> Fight to Win Pro 44\\nTravis Stevens: From Judo Olympian To Grappling Prize Fighter\\nStevens, a Renzo Gracie black belt, will be going up against Yuri Simoes in the main event of Fight To Win Pro 44 on August 12.\\nAug 10, 2017 by Averi Clements\\nTravis Stevens is kind of a big deal. At the 2016 Summer Olympics, the Massachusetts native made history when he became the third male judoka from the USA to win a silver medal at the Games.\\nAnd on Saturday, August 12, the jiu-jitsu world is finally going to get to see him on the stage he deserves at Fight To Win Pro 44 at the Reggie Lewis Center in Boston.\\nStevens, a Renzo Gracie black belt, is going up against Yuri Simoes in the main event, but if you think this Olympian is in this competition -- or any event -- for the fame and shiny medals, he\\'ll be quick to put you in your place.\\n\"Honestly, I couldn\\'t care less if I\\'m a part of the main card or not,\" Stevens said. \"I\\'m just glad I\\'m getting paid. After my Olympic medal, I told myself I would be a prize fighter. I don\\'t have any ambition to push myself like I used to for free.\"\\nWatch Fight To Win Pro 44 LIVE or On Demand ONLY on FloGrappling!\\nStevens in his iconic battle against Ole Bischof of Germany in the men\\'s 81kg judo semifinals during the London 2012 Olympic Games. Photo: Matt Kryger-USA TODAY Sports\\nIt makes sense, then, that fans of this multi-talented martial artist haven\\'t gotten the chance to really see him in action at tournaments such as Worlds or Pan Ams. In fact, this will be the first time that many people will get to see him compete in jiu-jitsu at all, and he\\'s aiming to put on a show they won\\'t soon forget.\\nI\\'m hoping I\\'m really able to open up and look for submissions and I\\'m not just stuck in a sweep vs. pass game. That would be boring to watch for the viewers. I would like to have an exciting match with a lot of back-and-forth between the both of us.\\nUnlike a lot of competitors who boast of their ability to dominate their opponents, Stevens has both feet firmly planted in reality. He knows that Simoes, a 2015 ADCC champion, isn\\'t going to be easy prey and predicts the match will go the full eight minutes.\\n\"I\\'m expecting Yuri to come out and grip hard [and attempt] to try a few takedowns and shots,\" Stevens said. \"If it doesn\\'t work I would expect him to jump closed guard and work his game from there. If or when he ends up on top, I\\'m going to expect a heavy pressure passing game that is going to make me very tired.\\n\"I would be shocked if either of us finished the match in a submission. I feel both of us are too skilled to allow that to happen. Although it is possible for one of us to be placed in a bad situation early and the other capitalize on it.\"\\nStevens chokes Avtandili Tchrikishvili of Georgia during men\\'s judo 81kg semifinals in the Rio 2016 Summer Olympic Games . Photo: John David Mercer-USA TODAY Sports\\nStevens speaks with the wisdom of someone who\\'s been a competitive athlete for a long, long time. Just as he\\'s not one to soak up the spotlight for the sake of all the glitz and glamour it offers, he doesn\\'t mince words when it comes to his success as a judoka and jiujiteiro -- which, by the way, have nothing to do with each other, according to him. He says that an expert in either sport shouldn\\'t expect to just dive into the other one expecting immediate success.\\nI\\'m good at jiu-jitsu because I\\'m good at jiu-jitsu and I listen to my instructors, and I\\'m good at judo because I listened and well. I might be able to blend the two sports together on occasion, but that\\'s because I have a deep understanding of both sports, not because I\\'m a one-trick pony in either.\\nRegardless of whether or not Stevens turns out to be right about how his match with Simoes will play out, he\\'s not viewing this as his grand entrance into an illustrious jiu-jitsu career or the launching point to add titles to his name.\\nHe said that he\\'s only going to take matches where the amount he\\'s getting paid to show up is specified up front. For him, this is work, and he\\'s showing up to get it done.\\n\"I\\'m a professional that has fought for years,\" Stevens said. \"I will show up prepared, and if I lose quickly, there is no amount of preparation that could have saved me. I\\'m very good at what I do, and I can compete against anyone.\"\\nHow to Watch Fight To Win Pro 44\\nOn TV: Now available on Roku and Apple TV 4 -- download the FloSports app today.\\nSTREAMING: Available only on FloGrappling via monthly or yearly memberships. A yearly FloPRO subscription provides access to ALL FloSports sites. SIGN UP HERE\\nJoin The Conversation On Social\\n Follow us on Twitter @FloGrappling\\n Follow us on Instagram @FloGrappling\\n Follow us on Facebook\\nFloGrappling\\'s hottest content, delivered to your inbox\\nDon\\'t miss breaking news, feature stories, event updates, and more. Sign up for the FloGrappling mailing list today.\\nMatthew Padgett vs Luca Negro Fight To Win 191\\nCaique Uemura vs Jabari Owens Fight To Win 191\\nJada Pua\\'Lei vs Karina Anguiano Fight To Win 191\\nChristian Perez vs Roman Ramirez Fight To Win 191\\nRodrigo Antunes vs Radji Bryson Barrett Fight To Win 191\\nDante Rosenberg vs Ryan Donovan Fight To Win 191\\nSean Joseph vs Robert Amar Fight To Win 191\\nDevan Barofsky vs Daniel Da Silva Fight To Win 191\\nMarcos Maciel vs Jason Oliveira Fight To Win 191\\nMore Fight to Win Pro 44\\nGrappling Bulletin: Jiu-Jitsu Fighters Poised To Take Over MMA in 2022\\nGrappling Bulletin: 2021, A Year of Jiu-Jitsu & Grappling in Review\\nAnibal Retto vs Wanderson Dos Santos Fight To Win 191\\nGet the most important Grappling stories delivered straight to your inbox.\\nBy submitting my information, I agree to the Terms of Service and Privacy Policy and to receive offers and promotions from FloGrappling. Unsubscribe anytime.<s> Feds Delay Enforcement of EPA Lead RRP\\nFeds delay enforcement of lead training\\nSince April 22, contractors who disturb at least six square feet of painted surfaces in homes built before 1978  and in \"child-occupied\" places like day care centers  must be certified in U.S. Environmental Protection Agency-approved training for lead-safe work practices.\\nThe goal is to prevent the spread of harmful lead dust and paint chips.\\nBut in a concession to contractors who are still scrambling to meet the training requirements, the EPA announced this month that it won\\'t pursue enforcement against violating contractors until Oct. 1.\\nSome contractors affected by the rule, including homebuilders and remodelers, have \"been concerned about not having had enough time to get training,\" said Dan Newman, executive director of the Sustainable Resources Center (SRC) in Minneapolis, an approved training provider.\\nAs of late February, only about 1,000 to 1,500 of Minnesota\\'s 15,000 to 19,000 licensed building contractors had met the requirement. Nationally, 300,000 people have been trained so far in more than 15,000 classes, according to the EPA.\\nDemand has tapered off, but SRC is still getting \"several calls a day about people wanting training,\" Newman said.\\n\"We are continuing to offer training.  People need to be in compliance with the law.\"\\nNewman said his organization has trained more than 500 people so far. Other organizations, such as the Builders Association of the Twin Cities, also offer approved classes.\\n\"It appears to me in Minnesota we have ample opportunities for people to be trained,\" he said.\\nFor whatever reason, however, many people still aren\\'t meeting the requirement.\\nThe federal government has separate requirements for firms and individual workers to be certified under the RRP rule.\\nSince April 22, the \"regulated community\" has raised concerns about \"difficulties experienced in obtaining the rule-required firm certification and renovation worker training,\" according to a June 18 memo from Cynthia Giles, assistant administrator for the U.S. EPA\\'s Office of Enforcement and Compliance Assurance.\\nBecause of those concerns, the memo stated, the EPA won\\'t pursue enforcement actions for violations of the \"firm certification\" requirement until Oct. 1.\\nMoreover, the EPA is giving individual workers until Sept. 30 to enroll or apply to enroll in an approved class. They must complete the trailing by Dec. 31.\\nIndustry groups such as the National Association of Home Builders (NAHB) cheered the EPA\\'s announcement. NAHB officials say the rule affects about 79 million people who live in pre-1978 homes.\\n\"The thing to remember about this, the delay in enforcement is really a delay in certification requirements for the rule, and that the work practices are still required to be done,\" said Matt Watkins, environmental policy analyst for the Washington, D.C.-based NAHB.\\nIn short, the idea is to buy more time for contractors to get into courses.\\n\"In some regions of the country there are fewer trainers, so folks that want to get trained can\\'t find a course to get into or the course they want to get into is full,\" Watkins said.\\nMeanwhile, the EPA memo made it clear that the feds are not backing down from the mandates.\\n\"EPA issued the Lead RRP rule because a disturbing number of America\\'s children are still poisoned by lead-based paint in their homes  leading to learning and behavioral disorders,\" the memo noted.\\nThe rule, which covers work done for hire, requires lead-safe work practices such as minimizing dust, using heavy plastic sheeting to cover the floors and closing windows and doors near the work area.\\nViolators face fines of up to $37,500 per day per job site.\\nMinnesota is making an effort to secure contractor compliance without creating extra levels of bureaucracy.\\nA new law that goes into effect Feb. 1 requires builders who work on properties covered by the RRP rule to provide proof of certification as a condition of obtaining a building permit.\\nNewman said the bill had bipartisan support and buy-in from the contractors\\' groups.\\n\"Contractors who abide by the rules will not have to be concerned, or as concerned, about people who are not abiding by the rule engaging in unfair competition,\" Newman said.\\nMeanwhile, the Minnesota Center for Environmental Advocacy is promoting a new law that sets what the MCEA calls \"one of the toughest standards in the nation for protecting Minnesota children from lead.\"\\nThe law, which takes effect Thursday, requires the state\\'s health department to essentially lower the bar for measuring the point at which it\\'s deemed appropriate to take action on the presence of lead in a child\\'s blood.\\nSpecifically, the new law will create lead-blood guidelines \"at half of the federal standard, making Minnesota one of the first in the nation with this tougher standard,\" the MCEA noted.\\nA news conference about the new law is set for 9:30 a.m. Monday, at Veterans Memorial Park in Richfield. Scheduled speakers include a woman whose child was diagnosed with lead in her blood, according to the MCEA.\\nNewman said the Richfield location was chosen in part to drive home the point that lead safety is not just an inner-city concern.\\n\"Any home built before 1978 has lead,\" he noted. \"The 1950s and \\'60s homes in many Minneapolis and St. Paul suburbs have lead issues.\"\\nwww.Microshield-ES.com http://www.CFL-IAQ.com\\nThis entry was posted on Wednesday, July 21st, 2010 at 2:53 pm and is filed under Commercial IAQ, Residential IAQ. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site.<s> St. Frederick\\'s Church in Verndale as it looks today.\\nA picture of the original church when it burned down on June 15, 1918.\\nIn 1995, the church was completely renovated. The pews were replaced, the floors were resurfaced and carpeted, a new piano was purchased, the tabernacle was refurbished and the Sacred Heart Shrine was added.\\nThree houses of faith still standing 100 years laterSt. Frederick\\'s Catholic Church\\nWith help from the Pages of History, newspaper archives and the History of St. Frederick\\'s Catholic Church book printed for their 100th anniversary, the church recorded its history, cataloging everything ever purchased or brought into the church. St. Frederick\\'s Catholic Church building is 100 years old this year.\\nThe building\\'s history started with the purchase of The Seventh Day Adventist church, which was active in the 1890s. They gathered in a log church and were admitted into the conference on June 25, 1884. The church may have stood on the site of Verndale\\'s last creamery.\\nIn December of 1895, the church trustees transferred the lot to E.W. and E.E. Dickinson.\\nBeth Townsend\\'s memories said that most of the church members had moved to Oregon and Washington by 1908. In 1911, the church was reorganized, called the Wing River Seventh Day Adventists. . . .<s> Q: TypeError: Object function Object() { [native code] } has no method \\'method\\' Going through the example code for chapter 5.4 in Javascript: The Good Parts, the following is used to demonstrate the use of the functional pattern to call super methods:\\nObject.method(\\'superior\\', function (name) {\\n    var that = this, method = that[name];\\n    return function () {\\n        return method.apply(that, arguments);\\n    };\\n});\\n\\nThis would be used as follows (where \"cat\" is another constructor function that has a \\'get_name\\' function defined) :\\nvar coolcat = function (spec) {\\n    var that = cat(spec),\\n        super_get_name = that.superior(\\'get_name\\');\\n    that.get_name = function (n) {\\n        return \\'like \\' + super_get_name(  ) + \\' baby\\';\\n    };\\n    return that;\\n};\\n\\nHowever when running the sample code, F12 tools show the following:\\nUncaught TypeError: Object function Object() { [native code] } has no method \\'method\\'.\\nWhat am I missing here?\\n\\nA: Douglas Crockford uses the following (defined on page 4 of the book)\\nFunction.prototype.method = function (name, func) {\\n    this.prototype[name] = func;\\n    return this;\\n};\\n\\n\\nA: That\\'s because the method method is not defined in your code, check the book for a place where the author defined the method method.\\nApparently @Andreas has found the method, and now I remember it.\\nThe method method is used so that when it is called on any object, it defines a method on that object where the name of that method is the name parameter passed to method, and the implementation of that method is the func function parameter.\\nYou need to include this in your console for things to work correctly.\\n<s> I know it is not really a risotto  OK, so it is really a rice pudding but uses many of the same techniques as risotto to get that over the top creamy texture. Rice pudding is a perfect winter treat served either room temp or slightly warm. Be decadent and add a dollop of honey cinnamon whipped cream. This makes a perfect side to almost any savory breakfast with just the right amount of sweet. It doubles as a great dessert.\\nIn a saucepan measure the milk, cream, sugar, lemon and orange zest, cardamom and cinnamon stick. Slit the vanilla bean down the middle, scrape out the seeds and add both the seeds and the bean to the milk. Bring just to a boil, then turn heat down to warm being careful not to boil it over.\\nI was wondering if Camolino (Milchreis) rice could be used for this recipe. It looks absolutely delicious. I\\'ll have to give it a try. Thanks for posting.<s> What could\\'ve been a raucous, wild comedy is instead let down by its timidity, generating only sporadic laughs. What a shame, writes Wenlei Ma.\\nNew Netflix movie is limp and forgettable\\nby Wenlei Ma\\n26th Jun 2020 4:37 PM\\nWill Ferrell makes a certain kind of movie and as goofy and funny as some of those are, there are also many misses. For every Anchorman there\\'s a Get Hard.\\nEurovision Song Contest: The Story of Fire Saga belongs squarely in the miss column.\\nSuffering from a case of timidity, the Ferrell and Rachel McAdams vehicle, dropping today on Netflix, is occasionally worthy of a chuckle but there\\'s not enough wackiness to fill out its overlong 123 minutes of runtime.\\nThis is a movie that drags - never what you want for a comedy.\\nFerrell plays Lars, an Icelandic man with dreams of winning Eurovision ever since he was a kid and saw ABBA lighting up the stage in the 1974 contest.\\nSigrit (McAdams) is Lars\\' best friend and creative collaborator, and probably not his sister. Together they are Fire Saga, a pop duo whose most requested song from their local townspeople is a little ditty called Ya Ya Ding Dong.\\nWhen, through a cassette audition tape and a couple of freak turns, they find themselves in Edinburgh as Iceland\\'s entry into Eurovision, Lars and Sigrit must discover what is really important to them.\\nEurovision Song Contest could\\'ve been much wackier Credit Jonathan Olley/Netflix\\nYou know Eurovision Song Contest gripes when you immediately notice that the time-jump from 1974 to present day doesn\\'t line up, unless we\\'re meant to believe that McAdams can pass for 50 years old (she really, really can\\'t) or that Pierce Brosnan is realistic as Ferrell\\'s father (only 15 years separate them).\\nThey\\'re small details that would\\'ve fallen by the wayside and easily forgiven if Eurovision Song Contest had the charm to bring you along on its ride.\\nInstead, the time discrepancies fester every time the movie lags - which is a lot, by the way, so be prepared to have many exasperated mutterings of \"but he\\'s way older than her, there\\'s no way she\\'s 50, ugh, what is this timeline?!\".\\nThere are jokes about stuffed unitards, \"classic\" camel toes and murderous elves, but the laughs are sporadic and never really build a momentum - unless you count how Brosnan\\'s Icelandic accent increasingly slips into an Italian one.\\nDan Stevens and his gold outfits are the standout Picture: Jonathan Olley/Netflix\\nBinge is Australia\\'s new streaming service offering the best drama, entertainment and movies from the world\\'s best creators. New to Binge? Get your two week free trial, sign up at binge.com.au\\nIt takes about 30 minutes before the first properly funny scenario is presented: Dan Stevens in a gold lame shirt and gold brocade coat gyrating on stage as Russian singer Alexander Lemtov, singing a ridiculous operatic pop ballad called Lion of Love.\\nStevens as Lemtov is the standout in Eurovision, a movie that manages to be both slickly over-produced and scrappily messy at the same time.\\nYou could argue that would be an apt description of Eurovision itself but for all of its mooted wackiness, Eurovision Song Contest doesn\\'t capture how ridiculously broad the real Eurovision is, even if it brings in some alumni for cameos - Conchita Wurst, Netta and Salvador Sobral among them.\\nMaybe it was stifled by the necessity to try and explain the appeal of Eurovision to millions of Americans who will be introduced to the concept for the first time - and you can thank the Yanks and their ignorance of everything beyond their own borders for the clunky, explanatory title.\\nFerrell and McAdams have a strange chemistry\\nFerrell and McAdams have a weird chemistry that doesn\\'t really work as partners/would-be lovers/maybe siblings, and not in that deliberately cringe Amy Poehler/Will Arnett Blades of Glory way.\\nFerrell\\'s Lars is a character whose obstinance makes it hard to root for him.\\nMcAdams is luminous every time she\\'s on screen and even though she\\'s lip syncing her musical performance, she makes it believable. But she may be slightly miscast in this role, or it, like everything else in Eurovision Song Contest, is severely underwritten.\\nEurovision Song Contest could\\'ve been a wild, raucous time, but instead it\\'s a limp movie you can scroll past without giving it a second thought.\\nEurovision Song Contest: The Story of Fire Saga is on Netflix from Friday, June 26 at 5pm AEST\\nShare your movies and TV obsessions | @wenleima\\nOriginally published as New Netflix movie is limp, forgettable\\nPaul Rudd \\'felt like a prop\\' on Friends\\nBig Simpsons change after 30 years\\neurovision song contest movie review netflix<s> With over 40 years of work in mortgage lending, 17 years at Lake Oconee, and her active involvement in our board and MLS systems, Linda has plenty of experience under her belt, and is bringing her seasoned skills to her clients here in the Athens and Oconee areas.\\nWhen she is not working, Linda enjoys doing volunteer work for various local organizations and cheering on the UGA Bulldogs. With Athens native and original Bulldog on Vince Dooley\\'s first UGA team, Bill Harber by her side she is rooted in the community and ready to work and cheer for the home team.<s> SCALED APPROACH TO OPEN SOURCING DEPARTMENT OF THE NAVY PRODUCED SOFTWARE\\n[archiveorg scaledapproachto1094560404 width=560 height=384 frameborder=0 webkitallowfullscreen=true mozallowfullscreen=true]\\nGarcia, Julian L.; Holloway, Donovan Jr.\\nfree and open source, open source code, open source software\\nMonterey, CA; Naval Postgraduate School\\nnavalpostgraduateschoollibrary; fedlink\\nThe Department of Defense (DoD) must continue to develop, sustain, and update its software-based capabilities. For the Department of the Navy (DoN), the life cycle costs of software continue to grow; over time, developing code will not be cost effective. An alternative to developing code is to further integrate open source software (OSS) into DoN programs. OSS is software that grants users the ability to view, use, and change the software source code. The use of OSS has been extensively researched, as addressed in the MITRE Corporation\\'s study on free and open source software (FOSS) in the DoD, completed in 2003. Despite favorable reports and published DoD policy, and the widespread successful use of OSS in current software, program managers are reluctant to fully integrate OSS into the DoN due to concerns with legal requirements, cybersecurity, total expenses, and the ability to implement and control OSS on classified systems while adhering to security regulations. This study utilized a quantitative, scaled approach to determine the risks and benefits to open sourcing for all DoN software. Several OSS case studies were examined. This research concluded that while OSS has been tested and proven cost-effective in certain areas of the DoN, it may not be the most efficient solution for all DoN projects. Therefore, the DoN should consider further implementation of OSS in security, software development, infrastructure support, and for program lifecycle cost reductions.\\nBurke, Karen L.\\nDegree_discipline\\nDegree_grantor\\nDegree_level\\nDegree_name\\nInformation Sciences (IS)\\nDistributionstatement\\nApproved for public release; distribution is unlimited.\\nDspace_note\\nNote, the Item of Record as published can be found at https://hdl.handle.net/10945/60404.\\nurn:handle:10945/60404\\nscaledapproachto1094560404\\nark:/13960/t72w0559z\\nItem_source\\nABBYY FineReader 11.0 (Extended OCR)\\nThis publication is a work of the U.S. Government as defined in Title 17, United States Code, Section 101. Copyright protection is not available for this work in the United States.\\nSecondreader\\nCook, Glenn R.\\nCaptain, United States Marine Corps\\nFEDLINK - United States Federal Collection\\nUploaded by NPS Calhoun Institutional Repository on May 3, 2019<s> Best of 2010 (Albums): Honorable Mention\\nBefore we get too deep in 2011, I thought I\\'d put a bow on 2010 this week. I\\'ll be listing a CD-R\\'s worth of my favorite tracks, my top 5 albums, and maybe a few other things this week. Too start things off, a list of great albums that deserve some sort of honorable mention\\nCerebellum 1989 (via Slamdek.com)\\nWhile these albums didn\\'t quite make my top 5 this year, I don\\'t think there is any doubt that I will be listening to them frequently for years to come.\\nShe & Him  Volume 2: I almost didn\\'t even bother here, but I\\'m really glad I did. Discovering this was like finding out about an old rock masterpiece. There\\'s a lot of warmth in ZD\\'s voice and songwriting, and M. Ward\\'s flourishes round out the best traditional pop record of the year. (listen to \"Lingering Still\") Read my original review.\\nNew Pornographers  Together: I\\'m not really sure how AC Newman keeps getting such inspiration out of this wacky ensemble, but Together, as its name would imply, manages to achieve both a stunning overall unity and the most idiosyncratic performances from each of the key players yet. Neko\\'s at her best on \"The Crash Years\", Dan\\'s a wacky loverboy on \"Silver Jenny Dollar\", Kathryn Calder shines on \"Sweet Talk, Sweet Talk\", and Dr. Newman rounds out just about everything else. Fantastic artsy power pop, fabulous performances, a great experience all around. (listen to \"Sweet Talk, Sweet Talk\") Read my original review.\\nVenice Is Sinking  Sand & Lines: This is what rainy Saturday mornings feel like. The great thing about this record is that you can get up close to it or leave it on in the background, either way you enjoy it. As I\\'ve said before, I wish more bands would just hang a microphone from the ceiling and play a set. Can\\'t wait to see what the future holds for ViS. (\"Bardstown Road\") Read my original review.\\nFrontier(s)  There Will Be No Miracles Here: Chris Higdon returns 7 years after the demise of Elliott with a record heavy on the DC-style hardcore. As packed as this one is with melodic, chunky riffs, comparisons to Jawbox seem obvious but entirely appropriate. Oh yeah, and it grows on you too. (\"Abul Abbas\") Read my original review.\\nBruce Springsteen  The Promise: Recorded in the late 70\\'s, this is a gold mine for just about anyone who cares about rock and roll. Did we really need a reminder of what a monumental and unique talent Bruce Springsteen is? The Boss himself seems to think we did, and for that, I\\'d like to shake his hand. (\"Someday (We\\'ll Be Together)\") Read my original review.\\nStars  The Five Ghosts: Stars fully embraced 80\\'s synth-pop on this record, and what came of it was one of the most listenable and catchy collection of tunes imaginable. With all their earnestness, I get a feeling it\\'s becoming less and less cool to like Stars, but don\\'t let that hold you back. This is some serious ear candy, so just indulge your sonic sweet tooth already. (\"Wasted Daylight\") Read my original review.\\nCerebellum  Cerebellum: How could this not be awesome? Cerebellum came to a pre-mature end in 1989, leaving only 5 studio tracks (collected here) and a handful of other unrecorded songs. They recorded them for posterity this year, and it\\'s remarkable just how much these guys sound like they are picking up right where they left off. The big highlight is the mighty \"Crawl Out of the Water\", which existed as an inferior Crain demo. It\\'s in all its glory here. (\"Crawl Out of the Water\") Read my original review.\\nFiled under Best of 2010, Louisville Tagged with bruce springsteen, Cerebellum, Frontier(s), New Pornographers, She & Him, Stars (band), Venice Is Sinking\\nQuick Review (EP): Glenn by Slint\\nGlenn EP\\nTouch & Go; 1994\\nMy Rating: A+\\nBest Tracks: both tracks\\nSpiderland gets all the recognition, but when it comes down to it, this is the record that truly DOCUMENTS the reality of this band of bands. First of all, the recording is amazing, the sound of Albini completely redeeming himself after sabotaging Tweez. Second, the band never sounded better, more Slint-ish, than this. \"Glenn\" is perhaps the essential Slint track, immediate and mysterious, sprawling and meticulous, an epic crafted to precision that proceeds to blow your mind. \"Rhoda\" refurbishes the poorest track on Tweez, thrusting it forward as perhaps the best. When Walford crashes in near the end of the track (\"One two three four!!!!!\") and the band goes insane, you get a sense of what this band was capable of live. Doubters, beware. This EP just might convert you.\\nAMG review\\nHardcore For Nerds review\\nFiled under Classic Album, Louisville, Quick Review (EP) Tagged with brian mcmahan, britt walford, dave pajo, slint, slint 10\", slint glenn, Spiderland, Steve Albini, tweez\\nBook Review: Spiderland (33 1/3 series) by Scott Tennent\\nSpiderland (33 1/3 series)\\nby Scott Tennent\\nContinuum; 2010\\nMy Rating: A\\nI can remember it like it was yesterday. The year was 1994, and I was a freshman in high school, examining a double-sided, photo-copied Slamdek Records catalog. My eyes fell upon a blurb about a band named Slint, and I fixated on a quote that went like this: \"Even Stone Temple Pilots rip off big ideas from these guys.\" Not that I was an STP fan, but it didn\\'t take me long to realize that these Slint guys were a big deal. A few days later, I boogied on up to Mike Bucayu\\'s Blue Moon Records in Holiday Manor and bought myself a cassette copy of Tweez. So, when I popped that sucker into my bookshelf setup, and the first discordant notes of \"Ron\" came blaring through my speakers, I was a little taken aback. Was this really the pride of Louisville?\\nSuffice to say, eventually I got it, and that\\'s why I\\'m pleased to say that Scott Tennent has finally written THE BOOK on Slint, a band that was heretofore the subject of so much conjecture, hearsay, and legend that it was often hard to distinguish fact from fiction. Starting in 1982 with Brian McMahan\\'s first band, Languid and Flaccid, the book not only serves as the definitive story on Slint, but it also covers just about everything you\\'d want to know about seminal Louisville acts like Squirrel Bait, Maurice, and Solution Unknown. Tangentially, it even goes quite a ways toward revealing some of Will Oldham\\'s artistic roots as well. Through in-depth research and first-hand accounts from Dave Pajo, Todd Brashear, Ethan Buckler, and the imminently quotable Sean \"Rat\" Garrison, Tennant takes the band from cradle to grave, telling the story of the band\\'s origins as a Pajo/Britt Walford side project, Steve Albini\\'s early embrace of the band, the controversial Tweez sessions and departure of Buckler, the second Albini session that produced the Glenn/Rhoda 10, their efforts to establish themselves as a live act in 1989 and 1990, the Spiderland sessions, and the band\\'s subsequent demise in late 1990.\\nAlong the way, Tennent\\'s account is revelatory, capturing the artistic dynamics that went into composing and making Spiderland, and demonstrates that Slint were truly aiming for something new and unique. They were a band driven towards the sort of precision and craftsmanship that is often dismissed by rock musicians, and one gets the sense from reading Spiderland that one of the reasons the record is so special is that those guys cared about the placement and performance of every single note. Tennent\\'s analysis of Spiderland\\'s tracks is quite insightful as well, and even for those, like myself, who have listened to the record dozens of times, it refreshes the record and illuminates just what it is that makes it such an uncanny experience. Let me just put it this way: having just finished Tennent\\'s Spiderland, \"Good Morning, Captain\" sounds even greater.\\nIt\\'s about time someone got around to writing this book. Tennent\\'s Spiderland is HIGHLY recommended for any Slint fan, Slint-curious music fan, Slint-skeptic, or fan of interesting music in general.\\nScott Tennent\\'s blog, Pretty Goes with Pretty\\nWikipedia article on Spiderland\\nFiled under Book Review, Louisville Tagged with 33 1/3, brian mcmahan, britt walford, dave pajo, ethan buckler, languid and flaccid, Louisville, maurice, scott tennent, sean garrison, slamdek records, slint, solution unknown, Spiderland, squirrel bait, Steve Albini, tweez\\nQuick Review (LP): Tweez by Slint\\nJennifer Hartman; 1989\\nMy Rating: A-\\nBest Tracks: \"Charlotte\", \"Ron\", \"Carol\", \"Darlene\"\\nThey probably didn\\'t realize it at the time, but what the guys in Slint did with Tweez was deconstruct hair metal and hardcore. The guitars, drums, and vocals explode convention (at least the conventions punk kids were used to) and bleed into jazzy noodling and (classical?) dissonance. Most of the lyrics are fractured, dangling, and impressionistic loose ends that yield for the record a remarkably wide open sense of narrative. Albini\\'s production is controversial and intentionally ugly, but to be honest, I can\\'t imagine the record any differently. All that random noise and nonsense just sounds right here, making this the Slint record with a sense of humor. Tweez is a strange and fascinating piece of work, a record with an inexhaustible sense of mystery, and one of the most consistently original rock albums you\\'ll ever hear. Seventeen years on, I\\'m still trying to figure out what there is \"past where they paint the houses\" Sure, it pales in comparison to Spiderland, but what doesn\\'t?\\nAMG review (not entirely accurate)\\nDK Presents review\\nPre-Slint article from the guy who wrote the book on Slint\\nFiled under Classic Album, Louisville, Quick Review (LP) Tagged with albini, britt walford, dave pajo, pajo, slint, Spiderland, tweez\\nQuick Review (LP): One Less Heartless to Fear by Shipping News\\nOne Less Heartless to Fear\\nNoise Pollution/Karate Body; 2010\\nBest Tracks: \"Half A House\", \"(Morays or) Demons\"\\nShipping News speeds things up and unleashes a little bit of the ol\\' Albini charm here, but I gotta admit, I\\'ve always been a sucker for Shipping News\\' slower, brooding side. Therefore, I wasn\\'t immediately thrilled by \"The Delicate\" when it was released a few months back. And to be clear, OLHTF is not really a new LP so much as a live document. It contains a few tracks from their last LP proper, Flies the Fields, and features a solid set of otherwise unreleased material. The good news is that the old tracks sound great here, and the new stuff presents a side to the Noble/Mueller partnership that I hadn\\'t thought of since \"Shiner.\" Overall, I\\'m a big fan of the record\\'s concept. More bands (indie ones specifically) should take a step back from the studio and let their fans hear unreleased songs in process. I\\'m not so much talking about \"LIVE\" albums, greatest hits collections performed on stage, I\\'m talking about single-take sets of old, new, and weird. I could get used to Shipping News (and plenty of other bands) releasing something like this every other year or so. Dig the cover art too.\\nBuilt On A Weak Spot review\\nDusted review\\nYoung Scamels review (SGB)\\nJune of 44 career-in-brief (SGB)\\nFiled under Louisville, Quick Review (LP) Tagged with Flies The Fields, Jason Noble, Jeff Mueller, June of 44, One Less Heartless To Fear, Shellac, shipping news, Steve Albini, Young Scamels\\nNews Bits: Possible Rodan Rarities Collection in 2011\\nCheck out this Magnet Magazine interview with indie rock godfather/superhero Jason Noble. Newsworthy bits:\\nRodan Peel session  Official release likely (along with other rarities) in 2011 on Quarterstick\\nRodan reunion  unlikely (but he doesn\\'t completely rule it out)\\nShipping News  Next project will be another RMSN EP with separate studio recordings by each member\\nYoung Scamels  More music probable in the near future, looking to a more contemporary author for influence.\\nThere\\'s much more where that came from, so you should definitely check it out.\\nMy review of Rodan\\'s 1994 Peel Session\\nMy review of Young Scamel\\'s Tempest LP\\nFiled under Louisville, News Bits Tagged with Jason Noble, John Peel, Louisville Hardcore, Rachel\\'s, rodan, shipping news, Young Scamels\\nQuick Review (LP): There Will Be No Miracles Here by Frontier(s)\\nFrontier(s)\\nThere Will Be No Miracles Here\\nNo Sleep Records; 2010\\nBest Tracks: \"Young Lives\", \"Abul Abbas\"\\nIt goes like this: Chris Higdon used to lead the charge for Elliott. Elliott\\'s first (and, ahem, best) album, US Songs, opened with a track called \"Miracle.\" Said track was one of the most upbeat in their whole catalog, not really fitting in with their later stuff. Higdon now fronts Frontier(s), who have just released their debut LP a few months back. Get it? There will be no \"Miracles\" here. So goodbye Elliott. Now, it\\'s not that this record is a complete departure from Elliott\\'s dense and moody hard rock, it\\'s just that it\\'s stripped of the former band\\'s more atmospheric tendancies. This is a straightforward post-punk record, full of Jawbox-style indie rock. And it\\'s quite a good one at that. Yet the message is clear: Higdon\\'s done with the skies, ready to spend some time at ground level. The result is a very strong record, one that grows on me with each listen. Highly recommended, miracles or no.\\nBand Myspace site\\nPunkNews.org Review\\nMusic-Is-Amazing Review\\nFiled under Louisville, Quick Review (LP) Tagged with Chris Higdon, Elliott, Frontier(s), There Will Be No Miracles Here, US Songs\\nQuick Review (LP): Sweetheart Rodeo by Dawn Landes\\nSweetheart Rodeo\\nEssential Music; 2010\\nMy Rating: C+\\nBest Tracks: \"Romeo\", \"Little Miss Holiday\"\\nComing on the heels of the excellent urbanicana folk of 2007\\'s FIREPROOF, my expectations were high for this one. Unfortunately, it was a letdown. That\\'s not to say this is a bad record; it probably says more about the greatness of FIREPROOF. Landes plays very \"low-key\" music, and this time around it doesn\\'t have the staying power of FIREPROOF tracks like \"Bodyguard\" and \"You Alone\", or even the ridiculously catchy stand-alone single \"Straight Lines\". Folk music seems to go best with a lot of heart-on-sleeve, but SWEETHEART finds our leading lady a little too level-headed. I guess I\\'ll hold out for another great record the next time around.\\nPaste review\\nArtist Myspace\\nFiled under Louisville, Quick Review (LP) Tagged with Bodyguard, Dawn Landes, Fireproof, Josh Ritter, Straight Lines, Sweetheart Rodeo, You Alone\\nQuick Review (LP): Mercury by Follow the Train\\nFollow the Train\\nRemovador; 2010\\nBest Tracks: \"Movin\", \"Coffee\", \"Mellwood\", \"Division\", \"Seamless\"\\nOne thing seems certain; the band\\'s songwriter has an outstanding sense of melody and majesty in a late 80\\'s/early 90\\'s alt-rock sense. Witness: \"Movin\". Yessir. Wow. Let\\'s play that again. And again. Opener \"Coffee\" features a great vocal performance as well, and more melancholy bits like \"Mellwood\" (saxy) and \"Nowhere Night\" (urban lonesome) really shine as well. As I\\'ve said before, they remind me heavily of Afghan Whigs, with some of the mighty blues of Zeppelin thrown in for good measure (\"Division\", \"Mercury\") and a big midwestern heart. I don\\'t see any reason they shouldn\\'t be playing big stages at summer festivals, because this is one of the best albums of 2010.\\nBand website & listen to album\\nRemovador page & listen to album\\nSGB Breath of Sigh review\\nFiled under Louisville, Quick Review (LP) Tagged with Afghan Whigs, Dennis Sheridan, Follow the Train, Led Zeppelin, Mercury, Movin, Removador\\nQuick Review (LP): Cerebellum by Cerebellum\\nCerebellum LP\\nNoise Pollution; 2010\\nBest Tracks: \"Fire\", \"Marble\", \"Calm\", \"Crawl Out of the Water\"\\nThe first five songs were originally released in 1989. This band produced future members of Crain, Rodan, Matmos, Parlour, and other post-punk pioneers. What we have here is, on one hand, above average fare for a group of high-schoolers looking to imitate their musical heroes. On the other hand though, there is evidence of real creative vision here, and the melodies are especially strong on \"Fire\", \"Winter\", and \"Calm.\" \"Marble\" is a lovely little Smiths-esque piece; Drew Daniel\\'s adolescent-in-longing vocals evoke an incredible nostalgia for me now. The best thing here though is the brand new recording of \"Crawl Out Of The Water\" (one among five such tracks). It sounds outstanding, and feels more like a long lost Crain track (which it pretty much is) than anything else. Most of the music world probably won\\'t care about this, but they should.\\nLouisville Hardcore History band story\\nSlamdek band story\\nFiled under Louisville, Quick Review (LP) Tagged with Cerebellum, crain, Crawl Out of the Water, Louisville, Matmos, Parlour, rodan, The Smiths<s> Shortcuts\\n=========\\n\\n### Keyboard ###\\n#### Add and Delete ###\\n|                      |                            |\\n| :------------------- | :------------------------- |\\n| Create new Task      | Command + N    | \\n| Create new List      | Command + Shift + N    |\\n| Delete Task          | Delete |\\n\\n#### Navigation ###\\n|                      |                            |\\n| :------------------- | :------------------------- |\\n| Select Next Task      |   Arrow Down   | \\n| Select Previous Task  |   Arrow Up  |\\n| Edit Selected Task    |   Enter       |\\n| Select Next List      |   Tab |\\n| Select Previous List  |   Shift + Tab |\\n\\n\\n\\n### Task Name Shortcuts ###\\nAppending these to the name of Tasks is a quick way to set properties of those tasks.\\n\\n#### Due Date ####\\n**-d**\\n\\n##### Examples #####\\n```\\nThis task will be due Tomorrow -d tomorrow\\n```\\n\\n```\\nThis task will be due next tuesday -d tues\\n```\\n\\n```\\nThis task will be due on the 27th of feb -d 27/02\\n```\\n\\n### Important ###\\n**-i** or **-hp**\\n\\n#### Examples #####\\n```\\nThis task is important -i\\n```\\n\\n```\\nThis task is also important -hp\\n```\\n\\n### Assignment ###\\n**-a**\\n\\n#### Examples ####\\n```\\nThis task will be assigned to Charlie -a charlie\\n```\\n\\n```\\nThis task will also be assigned to Charlie, despite the fuzzy spelling -a caarlie\\n```<s> For years I\\'ve woken up early in the morning.\\nThere\\'s something so raw and untouched about the early morning hours of the day when the world around is so still and silent. Undistracted by most of what goes on in the afternoon hours, I can power through my morning routine and make significant strides towards my goals.\\nBut there was a period in my life when waking up early was the furthest thing from my mind. I didn\\'t always like to get up before the crack of dawn, feeling deprived from a lack of sleep, or enveloped in the fear of missing out on something from the night before.\\nYet, I always knew there was no other way to really achieve my goals. Sometimes, the morning is all that we have when it comes to free time where we can focus our minds. We feel fresh, relaxed (for the most part), and often able to see the forest through the trees.\\nStill, waking up early can be \"complicated.\" If you have kids or work the night shift, it can be even harder. With so many obligations, how are we supposed to find time to wake up early and use those precious moments as a stepping-stone to getting ahead in life?\\nUsually, it\\'s not that we can\\'t wake up early in the morning, it\\'s that we\\'re not willing to make the big sacrifices required of us to achieve great things. There\\'s a huge difference between not being able to do something and not really wanting to do it because our excuses outweigh our reasons.\\nBut the benefits of waking up early in the morning far outweigh the sacrifices. Still, if you\\'ve had trouble waking up early in the past, there are a few methods that you could employ to help \"trick\" your body into waking up early.\\nBefore I trained myself to become an early riser, I knew there had to be ways that I could hack my mind and my body into getting up early. I also knew, at the time, that I wasn\\'t drinking enough water. So, I decided to kill two birds with one stone.\\nWhen you drink lots of water before bed (I\\'m talking 4 glasses or more), your body doesn\\'t slip into a deep sleep that might be difficult to awake from. Why? Because you\\'ll constantly be waking up to pee in the middle of the night.\\nNow, this might not sound like the greatest approach for most folks, but it works if you\\'re really having trouble waking up early and you\\'ve tried other things that just didn\\'t work. Drink a lot of water 30 minutes before bedtime and you\\'ll have a much easier time waking up when that alarm clock goes off.\\nBefore waking up early in the morning becomes habitual, use the 5-minute rule. Set your alarm clock black 5 minutes every single morning until you reach your goal wake-up time. This slowly retrains the mind and builds momentum.\\nYou\\'ll be surprised at just how well this works. For example, if you have the goal of waking up at 4am in the morning and you currently wake up at 6:30, it would take you a month to achieve this. Over the course of those 30 days, the neurotransmitters in your mind actually shift and slowly adapt to the habit.\\nThis mini-change, if you will, is a novel way of adopting virtually any habit. This is far easier than going from 6:30am to 4am in a single day. When you do that, you feel deprived from a lack of sleep. When you slowly change your waking time, it\\'s far easier to adapt.\\nIn the evening, we tend to have a lot of distractions. From the television, to the Internet, to the phone, and friends, there\\'s always something going on in the evening. It\\'s easy to allow the time to get the best of you. When that happens, it\\'s much harder to go to bed at a respectable hour.\\nAll you need to do is power everything off one hour before bedtime. That means the television, the Internet, the smartphone, and any other \"smart\" device that can cause a distraction. Get into bed and read a book. Non-fiction or fiction, it\\'s your choice.\\nWhatever it is, ensure that you aren\\'t reading from a screen. A study conducted by Anne-Marie Chang of Harvard\\'s Medical School Division of Sleep Medicine found that people who read from a tablet such as an iPad have a harder time getting to sleep, sleep less deeply, and have trouble being alert the next day.\\nReading has a way of cleansing the mind, inspiring, and entertaining. But it also has a way of making you feel sleepy as long as it\\'s not done with a screen. Find a good physical book that you can read before bedtime. This also allows you to develop another good habit while working on your wake-up-early habit.\\nHaving an alarm clock is a vital part of waking up early. In fact, most of us can\\'t seem to wake up without an alarm clock, especially those of us who simply aren\\'t morning persons. But, an alarm clock is only so useful when it can be quickly turned off.\\nIt\\'s easy to reach for that snooze button and drift right back off to sleep. But how do we avoid doing that? Use your normal bedside alarm clock in combination with a \"super-annoying\" alarm clock. That super-annoying clock can be an app from your smart phone that proves you\\'re awake.\\nThere are numerous apps that can help you wake up in the morning. Or, you can simply opt for purchasing a very loud and annoying secondary alarm clock and place it a good distance away from the bed. This might annoy your bedtime partner, but until you can make the habit stick, you might have to go to extremes.\\nOften, we can\\'t think too much about developing habits that are hard to create. But, if you use a cue, you could help move things along easier. For example, if you know you have a hard time waking up, why not doing something to help jolt the system?\\nYou could go for a run or a walk around the block, you could drink a strong cup of espresso, or even listen to some really inspiring music. So, place a cue by your bed. For me, it\\'s my running shoes. By placing my running shoes by the bed, I can just get up and go for my run.\\nRunning helped me to develop the habit of waking up early. Similarly, you could use anything else that motivates and inspires you. Placing your headphones by your bedside gives you a cue to listen to inspiring music when you awake. Anything else that gets you going will work.\\nYou could place a picture of something that symbolizes your goals. A house. A car. A travel destination. Something that you can see right when you open your eyes that might help you get over the overwhelming desire to stay in bed and sleep in.\\nStimulants are a part of life for most people. But, when you indulge in them before bed, you disrupt your body\\'s ability to get a good night\\'s sleep. In turn, you\\'re far more restless in the morning and simply lack the desire of waking up early and getting out of bed.\\nI don\\'t drink alcohol or soda, and I don\\'t smoke cigarettes, but I do enjoy coffee. In fact, you could call what I have as a coffee addiction. But, when I drink coffee too close to bedtime, I have difficulty both getting to bed on time and waking up early.\\nIn 2012, a study determined that caffeine consumption, even up to 6 hours before sleep, can be disruptive. Even one cup of coffee can influence the body\\'s circadian rhythms. And alcohol has a somewhat similar effect as it works to dehydrate the system.\\nAlso, in another study conducted, it was determined that cigarettes are also disruptive to sleep. They affect the ability to fall asleep, the deepness of our sleep, and our ability to wake up. In general, stay away from all stimulants for up to 6 hours before bed if you to wake up early in the morning.\\nOne thing that forces us to stay up until the wee hours of the night, not get enough sleep, and not wake up early, is a careless approach to time management. Without carefully managing your time, especially with a full schedule and a lot on your plate, it\\'s next to impossible to find sanity, clarity, and room to breathe.\\nBut, when you plan your days and manage your time effectively, not only are you less stressed, but you have more free time to your self in the evening, and you\\'re able to get to bed at a respectable time, and thus wake up early in the morning.\\nThe best way to achieve this is to engage in some time management that will assist you in achieving your goals by focusing on the right things rather than the time-wasters that tend to send most us off onto irrecoverable tangents.\\nWhen we waste time throughout the day, we have less free time in the evening, are far more stressed out, and when we do get to bed, we toss and turn due to the overwhelming burden that we\\'ve placed on our minds.\\nTo rectify this, you have to effectively manage your time.\\nI spoke about time management in a recent post here, but I\\'ll give you a quick presentation of the broad strokes.\\nAll of our time can be categorized into four quadrants, which are based off two components of urgency and/or importance.\\nThe way this works is to first audit your time throughout the day. How much time are you spending in each of these quadrants? How much less time could you be spending in quadrant 4, namely, which is the time-wasting quadrant?\\nThe goal? Spend as much time in quadrant 2 as you can, which is dedicated to our long-term goals.\\nAll you need to do is audit your day with this framework in mind. Front-load all of the quadrant 2 activities into the early morning and eliminate the quadrant 4 activities. Do this by jotting down everything you do for a week straight and then labeling the quadrant next to it with a circle.\\nThe more you can effectively manage your time, the more efficient you\\'ll become. Then, waking up early in the morning for you won\\'t be such a monumental feat.\\nSetting goals the right way and creating profound reasons for them is a fundamental necessity for waking up early in the morning. It\\'s hard to lack the desire to get up in the early morning hours when we don\\'t have powerful reasons for wanting to achieve something.\\nHowever, when we do have powerful reasons for wanting to achieve something, we\\'ll make the sacrifices that we need to get there. Think about it for a moment. In the past, when you wanted something badly enough, didn\\'t you move heaven and earth to get it?\\nThe trick is to want your goals bad enough. Come up with some deep and profound reasons for achieving the things you want to achieve. You can\\'t just say money or fame. You have to have deep-rooted reasons.\\nBut first, make sure that you\\'ve set your goals the right way. Make sure you\\'ve written them down and created a real plan for their achievement. Then, come up with profound reasons. What are some examples of profound reasons?\\nWell, why do we do the things that we do? We don\\'t want money for the sake of wanting money. We want money because of what it will bring us: security, freedom, free time, so on.\\nSimilarly, we don\\'t want to do anything in life for superficial reasons. We\\'ll never follow through when our reasons are superficial. So, they have to be deep and profound.\\nSet strong enough reasons and you\\'ll have no problem waking up early in the morning any day.<s> There are two major reasons why many people aren\\'t part of a medical scheme. The cost of it is the biggest problem they often face. However, Medsure medical aid offers you a program with great coverage at an affordable price. They also offer several plans so you get to review them. You get to find out what is affordable and pick the level of coverage that you want. This is a winning outcome so that you aren\\'t without the medical coverage you need.\\nAfter you enter your information online including location, you get results about the types of plans that are offered in your area. You can also talk to a representative to explore any questions or concerns that you may have. That way there aren\\'t any problems standing in your way of getting the coverage you need. When it comes to any medical scheme, you don\\'t want to make assumptions about what coverage you have.\\nWith a huge selection of providers, you can also feel great about the Medsure medical aid that you pursue. By doing so, you can make sure your provider is part of that network. If they don\\'t happen to be, then you can call to see if there is a way they can be added. If you don\\'t have a regular doctor due to no coverage, you can go through the list and find one that is perfect for you.\\nDepending on your medical needs, there could be times when you will be referred to a specialist. With this type of coverage, your needs can also be paid for as long as you go through the proper referral process. Checking with a representative for Medsure will help to ensure you don\\'t overlook any of the necessary steps in that process.\\nFinding affordable coverage that doesn\\'t leave you out in the cold is very important. Just one trip to the hospital ER or to be an overnight patient can cost you a fortune out of pocket. It is certainly worth it to make sure you have a medical scheme in place that helps you out like what Medsure offers.\\nThe claims are so simple to be filed too, and most of the time they will be submitted by the provider. There isn\\'t anything you will have to do in order to get results and the payment goes right to the provider. The out of pocket costs depend on the plan you pick, and with so much flexibility there is nothing that you have to worry about.\\nIt is very simple to get signed up for one of the plans that Medsure offers. By doing so, you will be able to get results that work for your budget. Plus, you will be able to get preventative care, medical care when you are sick, and even emergency services. It is never a good idea to be without any coverage. Before you assume that you can\\'t afford it, take the time to see what options you have.<s> Customizer iPad 2017 | DODOcase, Inc.\\nThe images shown for \"Customizer iPad 2017\" do not necessarily represent the actual model of the device that you have selected. Don\\'t worry, you will receive a case that fits the selected model.<s> Join us by signing this letter to Mexican authorities requesting protection for Jorge Tadeo Vargas, an activist who struggles peacefully for environmental and social justice from the territories.\\nIn Brazil, the fight against incineration continues. In So Bernardo do Campo, waste pickers attended public debate about waste to energy between the municipal government and the Anti-Incineration Coalition held at a Methodist church in the city. So Bernardo plans to install an incinerator that will burn waste in surrounding cities for a period of at least 30 years.<s> A glance at your watch or cell phone quickly shows you the day and date. Before these technological advances, the monthly calendar was where you looked. It was a vital part of every home (often in the kitchen) and office, and was prominently displayed.\\nBusinesses used this to their advantage  some still do today but more subtly. The calendar was given (usually in a face-to-face situation) to faithful customers as a year-end thank you gift. The image on the calendar often reflected the business owner\\'s philosophy, and the name of the business was very large; you couldn\\'t ignore it.\\nOnoway Museum\\'s collection of calendars is wide-ranging but the Eddy Dales General Store collection is especially intriguing.\\nThese calendars are all approximately the same size (10\" x 20\") therefore not easily hidden inside a cupboard door. Wire frames at upper and lower edges ensured that the paper didn\\'t curl or bend. The pictures chosen for the calendar have universal appeal and can fit any room dcor: idyllic landscapes, cuddly animals, smiling children. The business name, mailing address and phone number are very visible. On the bottom quarter of the calendar is the actual calendar  stapled on, with the months to be torn off, page by page.\\nLooking more closely at the information on the calendars over the years, you see that, in 1959, Eddy\\'s phone number is 22. In 1966, it is 932-5335. It changes again in 1978 to 967-5335. And there\\'s one more change in after that.\\nOnoway Museum has calendars from numerous businesses that operated in Onoway over the years. Most of them are long gone but each calendar gives another insight into life in our community.\\nExplore the calendars the next time you visit the museum. And if you happen to have one that would enhance the collection, the museum welcomes donations!<s> Q: web audio in firefox i am trying to build a web app that visualises and and controls the source audio, it works brilliant in chrome, but completely breaks in firefox, it won\\'t even play the audio. here is the code:\\nvar audio = new Audio();\\naudio.src=\\'track.mp3\\';\\naudio.controls = true;\\naudio.loop = false;\\naudio.autoplay = false;\\n\\n\\nwindow.addEventListener(\"load\", initPlayer, false);\\n\\nfunction initPlayer(){\\n  $(\"#player\").append(audio);\\n\\n  context = new AudioContext();\\n  analyser = context.createAnalyser();\\n  canvas = document.getElementById(\"vis\");;\\n  ctx = canvas.getContext(\"2d\");\\n\\n  source = context.createMediaElementSource(audio);\\n  source.connect(analyser);\\n  analyser.connect(context.destination);\\n\\n}\\n\\nthe line that breaks everything is:\\n  source = context.createMediaElementSource(audio);\\n\\nafter adding this line the player just hangs at 0:00 in firefox. i have done my research and have come across CORS, but as far as i can understand this should be irrelevant as the file is kept on the same server.\\nPlease help\\n\\nA: You have to serve the audio correctly with a server so that MIME types are set, so run it from localhost rather than file:///..../track.mp3\\n\\nA: We used to have a bug in Firefox where MediaElementSourceNode did not work properly in some case. It\\'s now fixed (I believe the fix is in Aurora and Nightly, at the time of writing).\\nSorry about that.\\n<s> Mantle Network\\nAbout Joshua Giles\\nJoshua T. Giles is a consecrated servant of the Lord Jesus Christ. He has the heart of a worshipper and a never-ending pursuit for God. He is used of God as an anointed apostle and prophet to the nations. He is known as a preacher, teacher, author and psalmist. Joshua is a sought-after conference speaker. He travels extensively preaching the Gospel of the Kingdom to those of all generations, cultures, and backgrounds. He has had the privilege of ministering the Gospel throughout the world and traveling to over 30 nations including Africa, Europe, Israel and the Middle East.\\nFurthermore, Joshua is the Lead Pastor and founder of Kingdom Embassy Worship Center, a thriving ministry based in Minneapolis, Minnesota. He is also the founder of Joshua Giles Ministries and The Mantle Network; organizations that reach abroad through apostolic centers, prophetic schools, initiatives, and training modules. Joshua is a part of the Jabula Network under the leadership of Bishop Tudor Bismark; where he serves faithfully.\\nJoshua has a unique ministry to the church, governments, and the marketplace. The Spirit of the Lord has given Joshua prophetic insight and foresight as he ministers with accuracy and precision. He has been called on by government officials, dignitaries, national leaders, business leaders and the like, seeking counsel and to hear the Word of the Lord.\\nJoshua has a double degree in Business Management and Psychology and has devoted his time to helping Christian Entrepreneurs. He has a great desire to help others succeed in what God has called them to do. More than anything, it is his ultimate desire to do the will of God for his life.\\n2021 Joshua Giles | Copyright reserved<s> Category: Clinical News\\nKetamine model (WikiMedia)\\nOne of my favorite conference speakers has always been Dr. Al Sacchetti from Camden, NJ, USA. He is passionate about Emergency Medicine and understands his patients. So when Dr. Sacchetti goes on a rantI sit up and listen.\\nWho doesn\\'t love ketamine? It\\'s cheap and available worldwide. Use it for pediatric sedation, status epilepticus, and anesthesia. It can be administered intranasally, intramuscularly and intravenously.\\nBut can it safely be used for agitated delirium? Listen to the October 2019 podcast or read the PDF: Rants from the Community: Ketamine by Al Sacchetti MD. It\\'s sure to calm your agitation when faced with a delirious patient.\\nWho Knew? Ketamine is a drug of abuse. When the drug is diverted for recreational use, the original pharmaceutical form is often abandoned. The most popular method is snorting ketamine powder. The powder is prepared by evaporation of the original solution or ketamine solution may be transferred to a vaporizer to be administered intranasally. As with all illegally sold drugs the concentration and presence of adulterants are mostly unknown and therefore represents a public health risk.\\nPediatric Airway: Crash Review\\nOctober 22, 2019 Debra Stoner Leave a comment\\nPhoto from Wikimedia.\\nMost of us are anxious about taking care of infants and children younger than 2 years old who need airway support. It\\'s intimidating and challenging to face a small airway when most of us face this critical situation only a few times a year. It\\'s imperative to stay current and review the procedure and medications regularly. The September EM:RAP C3 podcast on Pediatric Airways hits all the vital landmarks for troubleshooting and management. Expertly presented by Jessica Mason MD, Mel Herbert MD, and Stuart Swadron MD, you will take home points such as; Infants and children have a much smaller pulmonary reserve than adults; thus they desaturate much more quickly after preoxygenation. More empowering take-home points await you so take a listen and share the knowledge.\\nWho Knew: Dr. Crawford Long administered the first documented ether anesthetic to an 8-year-old boy for a toe amputation on July 3, 1842.\\nEvery Village and Every Health Practitioner\\nDr. Christian, Santo Domingo Clinic, Ecuador\\nWe are passionate about the Continuing Medical Education on Stick (CMES) Project which delivers cme to hundreds of medical practitioners globally. We couldn\\'t do this without the generous in-kind donation of the cme content from our sponsor, Emergency Medicine Reviews and Perspectives (EM:RAP). Mel Herbert, EM:RAP CEO, shared his philosophy in this article.\\nThank you, Dr. Mel, for your foresight and wisdom.\\nWho Knew? \"Europe\\'s formal medical education system started in the late Middle Ages, with the rise of the universities in what is now Northern Italy. From approximately ad 1100 until the mid-19th century, two tiers of medical practitioners existed: (1) academic doctors and (2) practically trained surgeons (which consisted of a motley collection of practitioners, including barbersurgeons, traveling practitioners, ship\\'s surgeons, tooth extractors, etc.).\" Read the full article here.\\nAdvanced Trauma Life Support (ATLS) 10th Edition: Stemming the Hemorrhage of Misinformation\\nOctober 7, 2019 Debra Stoner Leave a comment\\nATLS was a mandatory course during my emergency medicine training with recertification every few years. One of the greatest benefits was recognizing the need to asign a leader and develop a systematic approach to the trauma patient. There is always controvrsy surrounding proptocols and recommendations but the 10th edition is based on decades of trauma experience.\\nOne of the new changes in the shock and circulation section is an emphasis on tourniquets, packing and the application of pressure; some very basic methods that can quickly control hemorrhage. Where do you focus your attention first? Airway? Hemorrhage control?\\nWherever you practice and no matter the resources available you will find something in this podcast to strengthen your skills. Take a listen to the September 2019 EM:RAP podcast or read the PDF called: Trauma Surgeons Gone Wild: ATLS 10th edition update by Stuart Swadron MD, Kenji Inaba MD, and Billy Mallon MD.\\nMorell Wellcome tourniquets. (courtesy WikiMedia Commons)\\nWho Knew? The first recorded efforts to prevent arterial bleeding has been ascribed to Sushruta, the father of surgical art and science, in 600 B.C At that time, he pressed the arteries with pieces of leather that he made himself and it is said that he had used a device in which we now call the tourniquet. (NCBI)\\nSeptember 21, 2019 Debra Stoner Leave a comment\\nOn September 2, 2019, Dr. Manoj Thomas, President of TWB, and Dr. Vera Sistenich, an Emergency Medicine physician with HandUp Congo, spoke to the Sydney Development Circle about \"Knowledge Translation\" (KT). The World Health Organization defines KT as: \"the synthesis, exchange, and application of knowledge by relevant stakeholders to accelerate the benefits of global and local innovation in strengthening health systems and improving people\\'s health\".\\nTWB works in the nonprofit sector under the scope of KT via our Continuing Medical Education on Stick (CMES) Project. It provides health practitioners in remote regions access to up-to-date CME using novel delivery methods, which do not depend on fragile infrastructure. This is assumed to translate into improved clinical practices, self-esteem, and patient outcomes.\\nHowever what is the price paid for any intervention when for every action there is a reaction. Dr. Manoj explains, \"Given that we have technologies to assist with learning, the real question is about Knowledge Translation and ethical dilemmas around it. However, in reality, there are three barriers: political, cultural/social, and financial constraints.\"\\nIn the case of the CMES Project, introducing a product which doesn\\'t depend on local infrastructure points to the governments deficiencies in providing basic services such as electricity and Internet; a cultural consideration in the DRC is that junior doctors taught a specific medical or surgical technique by the senior doctors are unlikely to contradict their superiors and therefore the introduction of up-to-date CME which challenges long-held beliefs can cause staff internal conflict; and a health practitioner may want to use a product but doesn\\'t have finances for a smartphone or access to a computer.\\nWe strive to recognize the pros and cons of each CMES Project we launch by working with; local practitioners to identify needs and challenges; local partners engaged in similar work; and local Ministries of Health.\\nWhat disruptive consequences have you experienced through your knowledge sharing? What was the relevant ethical issue? Share your story in the comments and help us all understand and work better.\\nCMES New Sites\\nMARCH Home Knowledge on Blast Injuries\\nAugust 26, 2019 Debra Stoner Leave a comment\\nAmbroise Par, on the battlefield using a ligature for the artery of an amputated leg of a soldier.(Photo Wikipedia)\\nUnder the best of situations major trauma centers can be overwhelmed with dozens of seriously injured patients but for many CMES participants limited resources are an everyday reality. The August EM:RAP podcast titled, \"Blast Injuries\" by Anand Swaminathan MD and Josh Bucher MD will help you piece together triage and treatments for a mass casualty.\\nTactical Combat Casualty Care (TCCC) uses the mnemonic MARCH for military battlefield medicine.\\nMassive hemorrhage is managed through the use of tourniquets, hemostatic dressings, junctional devices, and pressure dressings.\\nAirway is managed by rapid and aggressive opening of the airway to include cricothyroidotomy for difficult airways.\\nRespirations and breathing is managed by the assessment for tension pneumothorax and aggressive use of needle decompression devices to relieve tension and improve breathing.\\nCirculation impairment is assessed and managed through the initiation of intravenous access followed up by administration of tranexamic acid (TXA) if indicated, and a fluid resuscitation challenge using the principles of hypotensive resuscitation. TCCC promotes the early and far forward use of blood and blood products if available over the use colloids and discourages the administration of crystalloids such as normal saline (sodium chloride).\\nHypothermia prevention is an early and critical intervention to keep a traumatized casualty warm regardless of the operational environment.\\nA wounded knight is carried on a medieval stretcher. (Photo Wikipedia)\\nWho Knew? An early stretcher, possibly made of wicker over a frame, appears in a manuscript from c.1380.\\nAmbroise Pare (c. 1510-1590) is considered one of the fathers of surgery and modern forensic pathology and a pioneer in surgical techniques and battlefield medicine.\\nA Honey of an Idea\\nJuly 8, 2019 Debra Stoner Leave a comment\\nMeet Dr. Vera Sistenich, an Emergency Medicine physician from Sydney, Australia. Dr. Vera is the Project Leader for HandsUp Congo, an Australian nonprofit, \"Building a Healthy Congo\" Project. In collaboration with local partners and the Democratic Republic of Congo\\'s (DRC) government they are committed to bring Emergency Medicine training and integration to the DRC healthcare system. This is her story on one way she supports their goals.\\n\"I started in 2015 when I lived in a seaside suburb here in Sydney called Coogee. As a child, I grew up in Hong Kong (my Mum is Chinese) but our family spent our summer holidays in Germany (my Dad is from Munich). We had a very rural home in a Bavarian suburb next to a forest. Our neighbour, an old man, used to keep his hives in the forest which I used to pass walking our little sausage dog daily. I was always fascinated, and we could see the old man at night through the window processing wax and honey. I thought to myself as a girl I\\'d love to keep bees one day. When I moved to Sydney and bought my own home for the first time, I came across The Urban Beehive, a business and movement promoting responsible beekeeping in the urban environment. The owners Doug Purdie and Vicky Brown are Australian beekeeping royalty now! I did a course with them and then started my own hive in the outdoor area of my ground floor unit in Coogee.\\nThe weather here is so good that my one hive was producing around 100kg of honey a year. There are only so many birthday and Christmas presents you can make with all this honey! This volume would give around 300 jars a year, so I tried my hand at a little social enterprise, creating a label called \"Coogee Bees for Congo\" and selling each jar for AUD $15 and putting all the profit towards our Congo EM Project. There is a famous building in Coogee right by the beach called The Coogee Pavilion. It has a blue and white dome, which is what inspired the blue and white bee of my label, set within the contour of the landmass of the DRCongo. I changed the sting of the bee into a little heart, a reminder to myself of our duty to translate compassion into practice towards those in need everywhere.\\nI now have 2 hives, producing about 200kg per year. I have raised over AUD $ 10,000 since the start of the project with the honey.\\nBeekeeping is very successful in the city. The Sydney Bee Club, of which I\\'m a committee member, has partnered with several universities here for research, providing dead bees and honey samples from our members from numerous suburbs. It turns out that the honey produced in cities is less contaminated with chemicals and pesticides than a lot of rural honeys and the flavours more complex due to the diversity plants and lack of monocultures in the urban setting. Heavy metals from the city environment are stored within the bodies of the bees themselves and secreted somewhat into the wax, but not into the honey. This came as a big and welcomed surprise to us all. Challenges, though, included minimising swarming in the urban environment so our hives don\\'t become a public nuisance, and adhering to rules and regulations regarding safety towards our neighbours. The practice is popular here and encouraged by our local counsellors.\\nI don\\'t do any formal marketing as such. I work at two hospitals here in Sydney and just by word of mouth, colleagues, family and friends buy out the honey every time. I post on Facebook when I have a new batch and also on the HandUp Congo Facebook page. I also make candles from the wax as gifts.\\nIn addition to raising funds for the EM project, one year, we chanced upon the only beekeeper training collective in the whole of the Congo whilst traveling to one of our teaching sites by road. From that, a completely separate Be A Honey Project was born  we have raised funds to bring these experts to the remote village of Lotumbe, where Lucy of HandUp Congo grew up, to train them in sustainable beekeeping, in particular to empower the Pygmy population there.\"\\nWhat\\'s the Buzz About Honey?\\nManuka honey (Wikipedia)\\nThe May edition of Emergency Medical Reviews and Perspectives (EM:RAP), your CME sponsor for the Continuing Medical Education on Stick (CMES) Project, has an article on the use of honey in the emergency department or outpatient clinic. The commonly known medical uses for honey include cough suppression and skin wound antibacterial agent. Other uses that can be life saving are cited in the article titled, Honey for Everything by Ilene Claudius MD and Sol Behar MD. Buzz on over to your thumb drive or CMES-Pi and take a listen or read. It\\'ll sweeten your day.\\nManuka bloom (Wikipedia)\\nWho Knew? The antibacterial effects of honey vary widely depending on the type and production location as cited by Willix et al. of the University of Waikato in New Zealand. Manuka honey found in New Zealand is reported to have high antibacterial activity.\\nCMES-Pi Participant Highlight: Mount St. John\\'s Medical Center, Antigua\\nJune 10, 2019 Debra Stoner Leave a comment\\nMeet Dr. Vonetta George who works at Mount St. John\\'s Medical Center (MSJMC) in Antigua. Dr. Vonetta works in all critical care areas of the hospital including supervising the 15 doctors and 2 dozen+ nurses in the Emergency Department.\\nAntigua is located in the West Indies, a Leeward Island in the Caribbean. Mount St. John\\'s serves the population of Antigua and also Barbuda. Working on an isolated island directly affects the doctors and nurses ability to access current continuing medical education in a cost effective manner. Dr. Vonetta was the gail force hurricane behind getting the CMES-Pi Project installed in her hospital. MSJMC installed a CMES-Pi in June last year. Using our smart phone apps the staff can look up CME current practice topics at bedside. The CME is provided by our partner Emergency Medicine Reviews and Perspectives. The PDF files provide helpful bullet points and take seconds to read. The MP3 files are providing topics for weekly group CME conferences and discussions. The CMES-Pi Project directly impacts access to CME for 101 doctors and 179 nurses at the hospital. Thank you Dr. Vonetta!\\nWho Knew? The first inhabitants were the Siboney, who can be dated back to 2400 BCE. Arawaks settled subsequently, around the 1st century CE. The Caribs arrived later, but abandoned Antigua around the 16th century, due to the shortage of fresh water. Christopher Columbus sighted the larger island in 1493, and named it after a church in Seville, Santa Maria de la Antigua. (Commonwealth)\\nCongestive Heart Failure: Bring on the Leeches?\\nMay 21, 2019 Debra Stoner Leave a comment\\nWikimedia photo.\\nThe April edition of Right on Prime covers everything you need to know about congestive heart failure from the definition to palliative care, including advice on therapeutic phlebotomy. No matter where you practice you will find breath-taking take home points. Take a listen or read: The Generalist: Acute and End Stage CHF in the ED by Vanessa Cardy MD, Mel Herbert MD, and Heidi James MD in the April edition of Right on Prime available to all CMES participants using either the CMES thumb drive or Pi.\\nLeech application tubes and blood letting tool, probably from 1800s. Photo from Wikimedia.\\nWho Knew? Bloodletting (or blood-letting) is the withdrawal of blood from a patient to prevent or cure illness and disease. Bloodletting, now called therapeutic phlebotomy, whether by a physician or by leeches, was based on an ancient system of medicine in which blood and other bodily fluids were regarded as \"humours\" that had to remain in proper balance to maintain health. It is claimed to have been the most common medical practice performed by surgeons from antiquity until the late 19th century, a span of almost 2,000 years. (article content from Wikipedia)<s> Datasets can be stored in either ASCII or binary files. Multiple datasets can be stored in a single file and both scalar and vector datasets can be saved to the same file. For scalar dataset files, one value is listed per vertex, cell, node, or scatter point. For vector dataset files, one set of XY vector components is listed per vertex, cell, node, or scatter point. If necessary, a set of status flags can be included in the file. If the status flag is false (0), the corresponding item (node, cell, etc.) is inactive. If status flags are not included in the file, it is assumed that all items are active. Dataset files are opened through File | Open and are saved when other files are saved such as 2D scatter point files or through the Export Dataset dialog.\\nNote: This sample file is using an activity array, so there are 16 values per TS. The first 8 values are activity flags for each of the 8 nodes. Values 9-16 are the scalar values for the 8 nodes.\\nDescription File type identifier. Must be on first line of file. No fields.\\nDescription Identifies the type of objects that the datasets in the file are associated with.\\nRequired YES. If card does not exist, the file can only be read through the data browser. The datasets would then be assigned to the objects corresponding to the active module.\\nDescription Identifies the object that the datasets in the file are associated with.\\nRequired NO. Card is only used if the OBJTYPE is scat2d.\\n1 id Integer > 0 The ID of the object.\\nDescription The reference time as a Julian number.\\n1 reference time 8 byte float +/- Continuous count of days and fractions of days since noon Universal Time on January 1, 4713 BCE (on the Julian calendar).\\nDescription Scalar dataset file identifier. Marks beginning of scalar dataset. No fields.\\nDescription Vector dataset file identifier. Marks beginning of vector dataset. No fields.\\nDescription Identifies the type of vector data that will be read and where to apply it.\\nThe vectors are applied to the nodes/grid nodes.\\nThe vectors are applied to the elements/cells.\\nDescription The number of data values that will be listed per time step. This number should correspond to the total number of vertices, nodes, cells centers (cell-centered grid), cell corners (mesh-centered grid), maximum node id (meshes) or scatter points.\\n1 numdata + The number of items. At each time step, numdata values are printed.\\nDescription This number should correspond to the maximum element id (meshes), the number of cells (grids), or the number of vertices (scatter sets).\\nDescription The name of the dataset.\\n1 \"name\" str The name of the dataset in double quotes.\\nDescription Marks the beginning of a new time step, indicates if stat flags are given, and defines the time step value, status flags, and scalar data values for each item.\\nActivity or status values specified - A value of 0 (inactive) or 1 (active) must be specified for each node following the TS card, before the scalar or vector values are specified.\\nThis page was last edited on 19 January 2018, at 20:53.<s> HomeHollywoodShakira\\'s Ex Gerard Piqu Retires From Soccer Nearly 5 Months After Their...\\nShakira\\'s Ex Gerard Piqu Retires From Soccer Nearly 5 Months After Their Split: Watch\\nImage Credit: Manu Fernandez/AP/Shutterstock\\nShakira\\'s ex, Gerard Piqu, 35, shocked fans on Nov. 3 when he took to Twitter and Instagram to share a video (WATCH HERE) where he announced his retirement from soccer after playing the sport for nearly 20 years. \"Culers, us he de dir una cosa,\" he captioned his posts. Gerard continued to narrate the video where he revealed that the match between FC Barcelona and UD Almera on Nov. 5 will be his final one. \"The last few weeks, months, lots of people have been talking about me. Until now, I haven\\'t said anything. But now, I want to be the one talking about me,\" he said. \"Now is the time to bring this journey to its end. I\\'ve always said that there would not be any other team after Barca. And that\\'s how it will be.\"\\nMore AboutShakira\\nShakira\\'s ex, Gerard Piqu, announced his retirement from soccer on Nov. 3. (Manu Fernandez/AP/Shutterstock)\\nGerard, who began playing soccer at the age of 10, also made sure to clarify that he is excited to be a \"real fan\" and enjoy the sport from the stands. Throughout the video, the 35-year-old gave his dedicated fan base an inside look at his personal life, especially his life as a child. Gerard noted that he was \"born into a football-loving family of Barca fans\" and from \"a very young age\" he knew it wasn\\'t merely soccer he wanted to play, but rather, he wanted to join FC Barcelona.\\nDuring the emotional clip, which spans a total of just over two-minutes in length, a present-day Gerard is seen watching home videos of himself as a child to seeing himself achieve his life\\'s dream. \"I\\'ve been thinking a lot about that kid lately,\" he said. \"About what little Gerard would have thought if he had been told that all of his dreams would come true. That he would make it to Barca first team. That he would win every trophy possible. That he would become European Champion and World Champion. That he would play alongside the best players in history. That he would become one of the team\\'s captains. That he would make friends for life.\"\\nThe star athlete began playing for FC Barcelona in 2008. (David Aliaga/NurPhoto/Shutterstock)\\nThe dad-of-two joined U12B, the club\\'s youth team, in 1997, and the defender later joined Manchester United in England at the age of 17 in 2004. By 2008, Gerard headed back to his home country to officially join FC Barcelona, where he played for over 14 years. Throughout his wildly successful career, Gerard has won over 35 trophies and has become a household name for soccer fans across the globe.\\nHis retirement video comes just five months after he and the \"Hips Don\\'t Lie\" singer called it quits. \"We regret to confirm that we are separating,\" the couple said in a statement released by Shakira\\'s public relations firm. \"For the well-being of our children, who are our highest priority, we ask that you respect our privacy. Thank you for your understanding.\" Shakira and the beloved soccer player share two kids: Sasha, 7, and Milan, 9.\\nGerard Piqu: 5 Things To Know About Shakira\\'s Former BF & Father Of Her 2 Children\\nKim Kardashian \\'Getting Heat\\' From Parents At Kids\\' Sporting Events After Kanye\\'s Argument (Exclusive)\\nLittle Richard: I Am Everything Review: A Look at an Underrated Rock Icon\\nBritney Spears Returns To IG After Concerned Fans Call Police For Welfare Check\\nAshton Kutcher Reflects On \\'Painful\\' Loss When Ex-Wife Demi Moore Suffered Late-Term Miscarriage\\nJawan director Atlee and wife Priya blessed with a baby boy\\nEileen Review: Anne Hathaway & Thomasin McKenzie Film Defies Expectations<s> Embarrassed? Blame Your Brain\\nBy Jennifer Welsh 15 April 2011\\nA thumb-sized tissue in the brain may explain embarrassment.\\n(Image:   Rebecca Abell | Dreamstime.com)\\nFlushed, red-hot cheeks. Sweating palms. Hearing your rendition of \"My Girl\"  but you aren\\'t at karaoke. You are in the lab of Virginia Sturm at the University of California, San Francisco, and she\\'s making you watch your own off-key rendition of The Temptations\\' 1964 hit.\\nSturm\\'s team is working to isolate the part of the brain in control of embarrassment. They\\'ve found that the feeling of embarrassment that comes with experiences such as hearing your own singing is isolated to a thumb-sized bit of tissue deep within your brain.\\nIn people who show low levels of embarrassment  including those with dementia  this brain region is smaller than normal. \"This region is actually essential for this reaction. When you lose this region, you lose this embarrassment response,\" Sturm told LiveScience. (Most of Sturm\\'s study participants are actually patients with dementia, including disorders such as Alzheimer\\'s disease.)\\nPersonality centers\\nThe embarrassment center is focused in an area called the pregenual anterior cingulate cortex; this tissue resides deep inside your brain, to the front and the right. This region is integral in regulating many automatic bodily functions, such as sweating, heartbeat and breathing, but also participates in many thinking-related functions, including emotions, reward-searching behaviors (like those implicated in addiction) and decision-making.\\n\"It has projections to higher centers and also has projections down to lower centers,\" Sturm said. \"It has a dual role in both visceral and also motor reactions.\"\\nSize and shape of brain regions near this one have been associated with differences in personality. Scientists believe that the bigger a particular brain region, the more powerful the functions associated with it would be. For instance, extroverts have larger reward-processing centers, while anxious and self-conscious people have larger error-detection centers. Very giving people have larger areas associated with understanding other\\'s beliefs, studies have shown.\\nDegeneration of embarrassment\\nThose with dementia tend to have lowered levels of embarrassment, even when watching themselves sing along to cheesy Motown hits.\\nMany things that those with dementia do, such as giving strangers massages or eating off of others\\' plates, don\\'t seem to embarrass them. When Sturm scanned their brains, she noticed that the less self-conscious and embarrassed the participants were, the smaller this embarrassment region in their cingulate cortex was.\\nScanning this region of the brain could help diagnose these conditions earlier, since behavioral and social changes tend to happen before other symptoms that manifest themselves more obviously. \"A better understanding of the emotional changes that occurring in these diseases could be helpful early in the course of disease when the diagnosis might not be so obvious,\" Sturm said. \"There could be a host of emotional or social changes that go along with the diseases.\"\\nThe work was presented in a talk by Sturm Thursday (April 14) at the 64th annual American Academy of Neurology meeting in Hawaii.\\nYou can follow LiveScience staff writer Jennifer Welsh on Twitter @microbelover.<s> \\u200bIf you are interested in Hunter, you may go to the contact us page of www.savethatdog.com and find the application links. This is the start of our adoption process.\\nHunter has been neutered, had a well check, had a dental, heart worm tested negative, has a current rabies shot, had a distemper/parvo booster, Bordatella Vaccination and has been de-wormed. He is microchipped and a nail trim.\\nHunter, is a lover! He is very smart and trainable. He is willing to learn. He wants to please his person. He is very playful with the other dogs. He runs and bounces around the yard in play.<s> Nomi Sasaki is a visual artist devoted to Chinese black ink tradition and animation. She pursued studies at the Purple Cloud Calligraphy Association  in Tokyo and holds a bachelor\\'s degree in communication sciences from Lima University. Her work has been featured in international festivals and venues from Art Basel Miami Week to National Sawdust in Brooklyn. Her videos are designed for performative environments, seeking the resignification of spaces through projected images. Each piece is designed considering spaces\\' real dimensions, which involves the construction of small-scale models. Inspired by puppetry, her process includes the creation of miniature stages in its interaction with lights, objects, and projection. Once the video is produced and projected in the performance space, the resulting image is a multilayered visual composition that challenges the audience\\'s final perception of both spaces\\' and performers\\' dimensions. She also blends footage of live paintings in Chinese black ink, integrating this visual tradition with multimedia formats. Although her videos are processed and manipulated by software, her aesthetic intends to preserve image\\'s and movement\\'s original handcraft. Since 2012, she has been a collaborator in composer Pauchi Sasaki\\'s GAMA project. She currently works as producer and cultural manager at the Peruvian British Cultural Association in Lima.<s> Picasa\\'s bugs cause my images to be deleted.\\nI use Picasa client and Picasa Web Album for a time and it works enough for me. In addition to managing my photos in Picasa client, I also use Picasa Web Album as images hosting for my thai blog. It\\'s really easy and safe in an album.\\nYesterday, an unlucky day, I wanted to sync the blog album to my Picasa client and tried setting them to keep syncing in the future therefore I can upload an image via either the client or the web album. It would be great, isn\\'t it?\\nI clicked the \"download album to picasa\" button in the web client. The album was downloaded to the client beautifully and it was shown as a downloaded web album. Then I clicked the \"sync to web\" button in the client. It asked something and uploaded all images in the album. Why did they need to be uploaded again?\\nThe problem is here. In the web client, there were 2 album now in the same name, the original and the recently uploaded. At first I thought it was not a bug, you might need to have a fresh uploaded album at the first time to keep syncing in the future and these twin albums was different album and unique.\\nand this idea leads to the mistaken operation.\\nI right-clicked at the downloaded web album in the client and chose to delete the album with \"delete the online copies\" checked. Then both original and recently uploaded album were gradually deleted! and they were deleted successfully.\\nSo I don\\'t have any copies of this album left. It\\'s a bug of Picasa and I reported to its forum yesterday.<s> Results from real-world data and post-hoc analysis of Novartis Beovu pivotal trials presented at AAO 2020\\nBack to News Archive\\nInitial findings on patient characteristics and event likelihood provide insights related to Beovu use in wet AMD1,2\\nFollows establishment of multi-disciplinary expert coalition and Novartis commitment to sharing data and findings with ophthalmology community\\nDespite existing therapies, significant unmet need still exists for wet AMD patients; data shows around half of patients have unresolved fluid, with a third requiring monthly injections3,4\\nBasel, November 13, 2020  Novartis today reported initial findings from a coalition convened to answer key questions related to treatment with Beovu (brolucizumab) for adults with wet age-related macular degeneration (AMD). Analyses of US real-world and Phase III data presented at the American Academy of Ophthalmology (AAO) 2020 Annual Meeting identified baseline patient characteristics potentially associated with the incidence of inflammation-related adverse events that may occur following treatment with Beovu1,2. Novartis has a comprehensive program of work underway examining the root cause and potential risk factors for these events, as well as identifying mitigation strategies and treatment protocols.\\nIn the analysis of data from the IRIS Registry, including 12,000 patients treated with Beovu, the highest observed risk for experiencing retinal vasculitis (RV) and/or retinal vascular occlusion (RO) in the six months after first treatment with Beovu was prior intraocular inflammation (IOI) and/or prior RO in the 12 months before first Beovu injection1. Against an observed overall RV/RO risk rate of 0.46% for all Beovu-treated patients in the registry, this increased to 3.97% in individuals with prior IOI and/or RO1.\\n\"We are pleased to share these findings that underscore the importance of carefully examining a patient for active ocular inflammation before injecting Beovu and throughout the course of treatment,\" said Marcia Kayath, Global Head of Medical Affairs and Chief Medical Officer, Novartis Pharmaceuticals. \"Even with the great advancements made in treating wet AMD, data shows 50% of patients have unresolved fluid and a third require monthly injections, highlighting the persistent unmet need that Beovu may help address3,4.\"\\nIn a post-hoc unmasked assessment of the Phase III HAWK and HARRIER data, there was an observed trend toward increased incidence of RV/RO in patients with treatment emergent (boosted/induced) anti-drug antibodies (ADAs)2. Further analyses of the data presented and additional data collection are ongoing.\\nNovartis has five presentations at the congress including results from a post-hoc HAWK and HARRIER analysis showing Beovu is associated with greater and sustained reduction in Pigment Epithelial Detachments and Subretinal Hyper-reflective Material compared with aflibercept5. Novartis also sponsored a symposium including description of US real-world wet AMD patient case studies with Beovu.\\nBeovu is now approved in more than 50 countries, including in the US, EU, UK, Japan, Canada and Australia, based on the results of the HAWK and HARRIER clinical trials6-10.\\nNovartis is confident that Beovu continues to represent an important treatment option for patients with wet AMD, with an overall favorable benefit/risk profile.\\nAbout Beovu (brolucizumab)\\nBeovu (brolucizumab, also known as RTH258) is the first advanced humanized single-chain antibody fragment (scFv) approved for clinical use11,12,13. Single-chain antibody fragments are highly sought after in drug development due to their small size, enhanced tissue penetration, rapid clearance from systemic circulation and drug delivery characteristics13-15.\\nThe proprietary innovative structure results in a small molecule (26 kDa) with potent inhibition of, and high affinity to, all VEGF-A isoforms14. Beovu is engineered to deliver a high concentration of drug, thus providing more active binding agents11,12,13. In preclinical studies, Beovu inhibited activation of VEGF receptors through prevention of the ligand-receptor interaction14-16. Increased signaling through the VEGF pathway is associated with pathologic ocular angiogenesis and retinal edema17. Inhibition of the VEGF pathway has been shown to inhibit the growth of neovascular lesions and suppress endothelial cell proliferation and vascular permeability17.\\nAbout the HAWK and HARRIER studies\\nWith more than 1,800 patients across nearly 400 centers worldwide, HAWK (NCT02307682) and HARRIER (NCT02434328) are the first global head-to-head trials in patients with wet AMD that prospectively demonstrated efficacy of Beovu at week 48 using an innovative q12w/q8w regimen, with a majority of patients on q12w immediately following the loading phase11,12. Both studies are 96-week prospective, randomized, double-masked multi-center studies and part of the Phase III clinical development of Beovu11,12. The studies were designed to compare the efficacy and safety of intravitreal injections of brolucizumab 6 mg (HAWK and HARRIER) and 3 mg (HAWK only) versus aflibercept 2 mg in patients with wet AMD11,12. The most common adverse events (5% of patients) with Beovu were vision blurred, cataract, conjunctival hemorrhage, vitreous floaters and eye pain11,12.\\nIn early 2020, following post-marketing reports of vasculitis, Novartis initiated a review of post-marketing safety case reports and together with an external review committee confirmed a safety signal of uncommon adverse events termed as \"retinal vasculitis\" and/or \"retinal vascular occlusion\" that may result in severe vision loss. As a result, Novartis initiated worldwide label updates to reflect this adverse event information.\\nNovartis is dedicated to examining the root causes and potential risk factors associated with these adverse events and has convened a fully dedicated team of Novartis research, drug development and medical specialists, who are working with an external team of top global experts to thoroughly investigate risk factors and identify mitigation strategies and treatment protocols.\\nAbout wet age-related macular degeneration\\nWet AMD is the leading cause of severe vision loss and legal blindness in people over the age of 65 in North America, Europe, Australia and Asia, impacting an estimated 20 million people worldwide18-20. Wet AMD occurs when abnormal blood vessels form underneath the macula, the area of the retina responsible for sharp, central vision21,22. These blood vessels are fragile and leak fluid, disrupting the normal retinal architecture and ultimately causing damage to the macula21,22.\\nEarly symptoms of wet AMD include distorted vision (or metamorphopsia) and difficulties seeing objects clearly23. Prompt diagnosis and intervention are essential24. As the disease progresses, cell damage increases, further reducing vision quality24. This progression can lead to a complete loss of central vision, leaving the patient unable to read, drive or recognize familiar faces and potentially depriving them of their independence24,25. Without treatment, vision can rapidly deteriorate26.\\nAbout Novartis in ophthalmology\\nAt Novartis, our mission is to discover new ways to improve and extend people\\'s lives. In ophthalmology, we develop and deliver life-changing medicines and therapies for diseases and conditions from front to back of the eye, enabled by data and transformative technologies. Our ophthalmic solutions reach more than 150M people per year, from premature infants to the elderly.\\nThis media update contains forward-looking statements within the meaning of the United States Private Securities Litigation Reform Act of 1995. Forward-looking statements can generally be identified by words such as \"potential,\" \"can,\" \"will,\" \"plan,\" \"may,\" \"could,\" \"would,\" \"expect,\" \"anticipate,\" \"look forward,\" \"believe,\" \"committed,\" \"investigational,\" \"pipeline,\" \"launch,\" or similar terms, or by express or implied discussions regarding potential marketing approvals, new indications or labeling for the investigational or approved products described in this media update, or regarding potential future revenues from such products. You should not place undue reliance on these statements. Such forward-looking statements are based on our current beliefs and expectations regarding future events, and are subject to significant known and unknown risks and uncertainties. Should one or more of these risks or uncertainties materialize, or should underlying assumptions prove incorrect, actual results may vary materially from those set forth in the forward-looking statements. There can be no guarantee that the investigational or approved products described in this media update will be submitted or approved for sale or for any additional indications or labeling in any market, or at any particular time. Nor can there be any guarantee that such products will be commercially successful in the future. In particular, our expectations regarding such products could be affected by, among other things, the uncertainties inherent in research and development, including clinical trial results and additional analysis of existing clinical data; regulatory actions or delays or government regulation generally; global trends toward health care cost containment, including government, payor and general public pricing and reimbursement pressures and requirements for increased pricing transparency; our ability to obtain or maintain proprietary intellectual property protection; the particular prescribing preferences of physicians and patients; general political, economic and business conditions, including the effects of and efforts to mitigate pandemic diseases such as COVID-19; safety, quality, data integrity or manufacturing issues; potential or actual data security and data privacy breaches, or disruptions of our information technology systems, and other risks and factors referred to in Novartis AG\\'s current Form 20-F on file with the US Securities and Exchange Commission. Novartis is providing the information in this media update as of this date and does not undertake any obligation to update any forward-looking statements contained in this media update as a result of new information, future events or otherwise.\\nNovartis is reimagining medicine to improve and extend people\\'s lives. As a leading global medicines company, we use innovative science and digital technologies to create transformative treatments in areas of great medical need. In our quest to find new medicines, we consistently rank among the world\\'s top companies investing in research and development. Novartis products reach nearly 800 million people globally and we are finding innovative ways to expand access to our latest treatments. About 110,000 people of more than 140 nationalities work at Novartis around the world. Find out more at https://www.novartis.com.\\nNovartis is on Twitter. Sign up to follow @Novartis at https://twitter.com/novartisnews\\nFor Novartis multimedia content, please visit https://www.novartis.com/news/media-library\\nFor questions about the site or required registration, please contact [email protected]\\nIp M, et al. The Brolucizumab Experience Thus Far: A Health Economics and Outcomes Research Analysis. Presented at: American Academy of Ophthalmology 2020 Virtual Congress. November 2020.\\nHeier JS, et al. Assessing characteristics of patients with or without intraocular inflammation (IOI) in the brolucizumab treatment arms from the HAWK and HARRIER, Phase 3 studies. Presented at: American Academy of Ophthalmology 2020 Virtual Congress. November 2020.\\nSinger M. Two-Year Real-World Treat and Extend Patterns and Fluid Outcomes Among Neovascular Age-Related Macular Degeneration Patients Treated With Anti-VEGFs. Presented at the American Society of Retina Specialists (ASRS) Annual Meeting (Virtual). July 2426 2020.\\nKhanani AM, et al. SIERRA-AMD: A Retrospective, Real-World Evidence Study of Patients With Neovascular Age-Related Macular Degeneration in the United States. Ophthalmol Retina. 2020;4:122133.\\nSadda S, et al. Pigment Epithelial Detachments and Subretinal Hyper-reflective Material: A HAWK and HARRIER Analysis. Presented at: American Academy of Ophthalmology 2020 Virtual Congress. November 2020.\\nBeovu [US prescribing information] East Hanover, NJ. Novartis: 2019.\\nBeovu [summary of product characteristics] Basel, Switzerland. Novartis: 2020.\\nPharma Japan. National Health Insurance Pricing. Available at: https://pj.jiho.jp/sites/default/files/pj/document/2020/05/New%20Drugs%20to%20Be%20Added%20to%20NHI%20Price%20List%20on%20May%2020_1.pdf. Accessed September 2020.\\nCanadian Agency for Drugs and Technologies in Health. CADTH Canadian Drug Expert Committee Recommendation. Available at: https://cadth.ca/sites/default/files/cdr/complete/SR0632%20Beovu%20-%20CDEC%20Final%20Recommendation%20%E2%80%93%20May%2025%2C%202020_for%20posting.pdf. Accessed September 2020.\\nBeovu [prescription medicine decision summary] Australia. Novartis: 2020.\\nDugel P, Koh A, Ogura Y, et al; HAWK and HARRIER Study Investigators. HAWK and HARRIER: Phase 3, multicenter, randomized, double-masked trials of brolucizumab for neovascular age-related macular degeneration. J Ophthalmol. 2020;127(1):72-84\\nDugel PU, Singh RP, Koh A, et al. HAWK and HARRIER: 96-week outcomes from the phase 3 trials of brolucizumab for neovascular age-related macular degeneration [published online ahead of print]. J Ophthalmol. 2020. https://doi.org/10.1016/j.ophtha.2020.06.028.\\nNimz EL, et al. Intraocular and systemic pharmacokinetics of brolucizumab (RTH258) in nonhuman primates. Presented at: Association for Research in Vision and Ophthalmology (ARVO) annual meeting. 2016. Abstract 4996\\nEscher D, et al. Single-chain antibody fragments in ophthalmology. Presented at: EURETINA congress. 2015. Abstract.\\nGaudreault J, et al. Preclinical pharmacology and safety of ESBA1008, a single-chain antibody fragment, investigated as potential treatment for age related macular degeneration. Invest Ophthalmol Vis Sci 2012;53:3025.\\nTietz J, et al. Affinity and Potency of RTH258 (ESBA1008), a Novel Inhibitor of Vascular Endothelial Growth Factor A for the Treatment of Retinal Disorders. IOVS. 2015; 56(7):1501.\\nKim R. Introduction, mechanism of action and rationale for anti-vascular endothelial growth factor drugs in age-related macular degeneration. Indian J Ophthalmol. 2007;55(6):413-415.\\nSchmidt-Erfurth U, et al. Guidelines for the management of neovascular age-related macular degeneration by the European Society of Retina Specialists (EURETINA). Br J Ophthalmol. 2014;98:1144-1167.\\nWong WL, Su X, Li X, et al. Global prevalence of age-related macular degeneration and disease burden projection for 2020 and 2040: a systematic review and met analysis. Lancet Glob Health. 2014;2:106-16.\\nSinger M. Advances in the management of macular degeneration. F1000Prime Rep. 2014;6:29.\\nMedlinePlus. Age-related macular degeneration. Available at: https://medlineplus.gov/genetics/condition/age-related-macular-degeneration/. Accessed November 2020.\\nWorld Health Organization. Priority eye diseases: Age-related macular degeneration. Available at http://www.who.int/blindness/causes/priority/en/index7.html. Accessed November 2020.\\nHealthline. What is metamorphopsia? Available at https://www.healthline.com/health/metamorphopsia. Accessed November 2020.\\nNHS Choices. Macular degeneration - Symptoms. Available at http://www.nhs.uk/Conditions/Macular-degeneration/Pages/Symptoms.aspx. Accessed November 2020\\nMitchell J, Bradley C. Quality of life in age-related macular degeneration: a review of the literature. Health Qual Life Outcomes. 2006;4:97.\\nvan Lookeren Campagne M, et al. Mechanisms of age-related macular degeneration and therapeutic opportunities. J Pathol. 2014; 232(2):151-64.\\nNovartis Media Relations\\nPeter Zuest\\nNovartis External Communications\\n+ 41 79 899 9812 (mobile)\\nEric Althoff\\nNovartis US External Communications\\n[email protected] Amy Wolf\\nNovartis Division Communications\\n+ 41 79 576 07 23 (mobile)\\nNovartis Investor Relations\\nCentral investor relations line: +41 61 324 7944\\nCentral North America\\nSamir Shah +41 61 324 7944 Sloan Simpson +1 862 778 5052\\nThomas Hungerbuehler +41 61 324 8425\\nIsabella Zinck +41 61 324 7188<s> Newly elected MP says voters think it is \\'Labour\\'s turn\\'\\nDecember 2, 2022, 3:16 am Updated: December 2, 2022, 5:39 am\\nThe new MP for Chester has said voters sent a strong message that they think \\'it\\'s Labour\\'s turn\\' (Danny Lawson/PA)\\nThe new MP for Chester has said voters sent a strong message that they think \"it\\'s Labour\\'s turn\".\\nSamantha Dixon, the former leader of Cheshire West and Chester Council, won the seat in Thursday\\'s by-election, increasing the Labour majority in what was the first electoral test for Prime Minister Rishi Sunak.\\nSpeaking to media after the result, she said: \"I think I have been in receipt of a very strong message from the voters of Chester that they want the Conservative cost-of-living crisis tackled immediately.\\n\"I don\\'t think they believe that the Conservatives have the answers, I think they think it\\'s Labour\\'s turn now.\"\\nShe described the City of Chester as a \"bellwether constituency\" and said the result was \"really, really encouraging\".\\nIn a speech following the result, Ms Dixon described meeting a pensioner who answered her door \"wrapped in blankets\" and was worried about her heating bills.\\nShe said: \"People in Chester and across our country are really worried.\\nMs Dixon re-won the seat for Labour with a 10,974 vote majority, after the contest was triggered by the resignation of Labour MP Christian Matheson (Danny Lawson/PA)\\n\"Worried about losing their homes because they can\\'t afford the mortgage repayments or the rent, worried about whether they can put the heating on, worried about whether they can put food on the table for their families.\\n\"This is the cost of 12 years of Conservative Government.\\n\"The Government which has wreaked havoc with our economy, destroyed our public services and betrayed the people who put their trust in them at the last general election.\"\\nShe said voters had sent a \"clear message\" to the Government.\\nMs Dixon said the Government \\'is on borrowed time and people want to change\\' (Danny Lawson/PA)\\nShe added: \"They have said unreservedly that Rishi Sunak\\'s Conservatives no longer have a mandate to govern.\\n\"His Government has broken the promise that each generation should do better than the last.\\n\"His Government has no ideas, no plan to address the big issues facing our country.\\n\"His Government is on borrowed time and people want to change.\\n\"It\\'s time for a general election and it\\'s time for a Labour Government.\"\\nStarmer warns against \\'complacency\\' but says Labour has \\'changed\\'<s> Sybille is a slow vinyasa flow yoga teacher, and vegetarian cookery teacher in London.\\nShe loves to share her enthusiasm for a vibrant lifestyle with as many people as possible.\\nShe teaches a strong, slow, creative flow yoga with a focus on alignment, breathing and relaxation allowing the body to open, strengthen, stretch, soften, rebalancing the whole being. At the end of a class you may feel energised, relaxed and perhaps stronger and limber.\\nThrough her cookery demonstrations and writing, she is hoping to give people an opportunity to feel inspired and excited about beautiful healthy food.\\nShe is always looking for inspiration and new ideas to share with the world and is forever a student herself!\\nWether you are looking for a Yoga class, a vegetarian cooking workshop, private classes in your home or group classes, take a look through the website and feel free to contact her if you have any questions.<s> SH blasts neutered Ontario regs as \\'bad for fans\\'\\nStubHub\\'s general manager for concerts and theatre in North America, Jeff Poirier, has penned an \"open letter to fans\" criticising Ontario\\'s abandonment of the planned ticket transparency provisions in its new Ticket Sales Act, which passed into law yesterday.\\nIn its current form, the Ticket Sales Act caps the price of resold tickets at 150% of face value, bans ticket bots and requires business selling or reselling tickets to disclose certain information, including the capacity of the venue, the number of tickets on general on-sale and the original face-value ticket price.\\nIt also originally required ticket sellers to disclose how many tickets are available to the public for a given event seven days before they go on sale  a provision abandoned last month following reported opposition from the concert industry. Among those believed to have pushed back against the transparency clause were Live Nation/Ticketmaster Canada and industry association Music Canada Live; according to local media, Ticketmaster\\'s Canadian COO, Patti-Anne Tarlton, told Ontarian parliamentarians that revealing total ticket numbers \"could enable [touts] to better use bots to buy bulk tickets where they\\'re known to be scarce\".\\nPoirier disagrees, and in the open letter, published yesterday, says the stripped-back legislation will be remembered for its \"unintended consequences\" on ordinary ticket buyers  and push the secondary market underground.\\n\"Today, the Ontario Liberals passed their Ticket Sales Act,\" he writes. \"Consultations were initially approached with the best of intentions: increase transparency on availability of tickets on the market and level the playing field so you have better access and more insight into the ticket buying process. In the end, this legislation will be known more for its unintended consequences than its protection of fans like you.\\n\"The government has maintained proposals that set fans back and stripped important transparency requirements that could have truly benefited you\"\\n\"In its original form, the Ticket Sales Act banned the use of bots to procure tickets, required ticket businesses to disclose more information to consumers and capped the resale price of tickets. Yet the government has maintained proposals that set fans back and stripped important transparency requirements that could have truly benefited you.\"\\nWhile he reiterates StubHub\\'s previously expressed support for banning ticket bots, Poirier cites the January 2016 study by New York attorney-general Eric Schneiderman  which found that up to 75% of tickets are being held back from the general public  as evidence that \"the issues impacting ticket access are broader than just bots\", which many consider to be only a small part of wider structural issues affecting the ticketing sector. This shortage of publicly available tickets, he continues, \"is one of the reasons why you see popular shows \\'sell out\\' so quickly\".\\n\"The original legislation required ticket sellers to disclose how many tickets were actually being made available for sale  a simple concept that would provide you better insight into the actual availability of tickets,\" writes Poirier. \"This is the very issue the proposed legislation was trying to solve. Yet, the government chose to remove this critical provision from the legislation, citing pressure from the live entertainment industry as a prevailing reason over establishing transparency for Ontario fans like you.\\n\"At StubHub, we understand transparency is important across the entire ticket industry, not just in the resale market. You should be able to know how many tickets are available for an event, what your seats will look like and how much you\\'re going to pay for them. Only in that circumstance can you make a purchase that you truly feel good about.\"\\n\"You should be able to know how many tickets are available for an event, what your seats will look like and how much you\\'re going to pay for them\"\\n\"When it comes to price caps,\" he continues, \"StubHub joins the industry in opposing this measure. This proposal stands to negatively impact Ontario fans like you and Ontario-based businesses like StubHub as ticket resales are driven off platforms that have robust consumer protections. Ticket resale prices will continue to be driven by supply and demand, not by arbitrarily set price caps. The fact is, if a venue holds 20,000 fans, but 100,000 fans want to attend the performance, ticket prices will reflect that demand. If the established market rate exceeds the 50% cap established by government, those sales won\\'t stop or adapt to reflect the price caps  they\\'ll just occur at their true value through channels the government cannot regulate. It will happen on street corners where the risk of counterfeit and fraud is significant, and no guarantees are in place; or it will happen on ticket resale websites located outside of jurisdiction of the Ontario government. Either way, you and businesses that have invested in the province will be hurt.\\n\"Consumers benefit from a competitive ticket market where transactions occur through secure channels that prioritise fans. At the same time, it is important to incentivise and encourage this ecommerce to remain right here, in Ontario.\\n\"We have said from the onset that we believe there is a better way for the industry and for you. It\\'s our mission at StubHub to connect you to incredible live event experiences, and to do so safely and securely by including money back guarantees and fraud prevention measures. This legislation is a disappointment for the ticketing industry, and a disappointment for fans like you.\"<s> This potential is the reason why there needs to be a commitment to innovation within the mobile space in Africa. A focus on developing solutions that allow for organisation, government and individual to embrace digital transformation by expanding network capabilities. Head of Sub-region, Southern Africa and Vodafone Market Unit, Deon Geyser from Nokia, it is critical that network and mobile innovation recognise the trends that are impacting on IP infrastructure and develop solutions that are capable of delivering what industry and business need.\\nAs the continent moves towards the imminent arrival of 5G the demand for bandwidth, reduced latency and jitter, IoT capability and high availability will rise exponentially. The path to 5G expects IP transport to accommodate this traffic growth and to have 10 times existing interface speeds with lower latency and packetized fronthaul. 5G introduces more complex traffic with service characteristics that range from extreme bandwidth for consumer broadband to ultra-reliable, low-latency communications for mission-critical applications, to the myriad sensors that comprise IoT. All this requires better coverage, greater capacity and enhanced quality of service.\\nSome service providers are actively engaging with the potential of 5G and the infrastructure required to harness it. Operators need to look to the future technology evolution in this arena so as to prepare the path for 5G. It is critical to pay attention as this could affect Africa\\'s move onto the global 4IR stage. Connectivity is critical to digital transformation on the continent and 5G must become the core of investment and innovation across both infrastructure and technology. This is the path to unlocking new opportunities and leading the race into the future.<s> Now, here\\'s a tournament with a difference. The first ever HeadHunterPoker Championship hits Asp'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_ds['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'were analyzed using Mutation Surveyor version 4.0.5 (Softgenetics, State College, Pennsylvania; http://www.softgenetics.com) for alignment and multiple comparisons'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tokenizer.decode(np.array([  654, 28649,  1413, 16969,   352, 24004,   271,  2751, 28705, 28781,\n",
    "         28723, 28734, 28723, 28782,   325, 19658,  2383, 24142, 28725,  3885,\n",
    "          6136, 28725, 15472, 28745,  3550,  1508,  2849, 28723,  4179,  2383,\n",
    "         24142, 28723,   675, 28731,   354, 19492,   304,  5166,  6085, 16149]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thus permanently deny the setting of cookies. Such an adjustment to the Internet browser used would also prevent Automattic/Quantcast from setting a cookie on the information technology system of the data subject.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(np.array([ 5884, 26815, 19685,   272,  5587,   302, 15648, 28723, 10373,   396,\n",
    "         7392,   466,   298,   272,  8712, 11806,  1307,   682,   835,  5297,\n",
    "        15939,  1061,   294, 28748, 19701,  2867,   477,  5587,   264, 17661,\n",
    "          356,   272,  1871,  5514,  1587,   302,   272,  1178,  3817, 28723]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mg-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
