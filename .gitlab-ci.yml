workflow:
  rules:
    - if: ($CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /Run tests and nightly/) || ($CI_PIPELINE_SOURCE == "schedule")
      variables:
        JET_CUSTOM_FILTER: "type == 'build' or 'mr' in spec.scope or 'nightly' in spec.scope"
    - if: $CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /Run tests/
      variables:
        JET_CUSTOM_FILTER: "type == 'build' or 'mr' in spec.scope"
    # always run MR pipelines
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    # always run web pipelines
    - if: $CI_PIPELINE_SOURCE == "web"
    # do not run branch pipelines if open MR exists
    - if: $CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS
      when: never
    # run branch pipeline if no open MR and on main
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    

stages:
  - build
  - unit_tests
  - functional_tests

variables:
  JET_CUSTOM_FILTER:
    description: |
      Selects what functional tests to run. For mr tests: "type == 'build' or 'mr' in spec.scope". For nightly tests: "type == 'build' or 'nightly' in spec.scope"
    value: ""
  TIME_LIMIT: "10:00" # Default time limit for all jobs
  SLURM_CLUSTER:
    value: "dgxa100_dracooci"
    options:
      - "dgxa100_dracooci"
      - "dgxh100_eos"
    description: '"dgxa100_dracooci" for OCI-IAD, "dgxh100_eos" for EOS'
  CI_MCORE_IMAGE: gitlab-master.nvidia.com:5005/adlr/megatron-lm/mcore_ci
  CI_NEMO_IMAGE: gitlab-master.nvidia.com:5005/adlr/megatron-lm/nemo_ci
  LINTING_IMAGE: gitlab-master.nvidia.com:5005/adlr/megatron-lm/mcore_linting

metadata:
  image: python:3.10
  stage: .pre
  tags: 
    - os/linux
  script:
    - env
    - |
      if [[ $SLURM_CLUSTER == dgxh100_eos ]]; then
        JET_CI_BRANCH=mcore/eos;
      elif [[ $SLURM_CLUSTER == dgxa100_dracooci ]]; then
        JET_CI_BRANCH=mcore/draco-oci;
      else
        echo "Unsupported value of SLURM_CLUSTER=$SLURM_CLUSTER";
        exit 1;
      fi
    - echo "JET_CI_BRANCH=$JET_CI_BRANCH" | tee -a build.env
  artifacts:
    reports:
      dotenv: build.env
  interruptible: true

ppp_capacity_statistics:
  tags: [mcore-ssh-agent]
  stage: .pre
  script:
    - |
      set -x

      ALL_USER=$(sshare -aP | grep coreai_dlalgo_mcore | tail -n +2 | awk -F '|' '{print $2}' | tr '\n' ',')

      # Get the current year, month, and day
      YEAR=$(date +%Y)
      MONTH=$(date +%m)
      DAY=$([[ $(date +%-d) -le 15 ]] && echo "01" || echo "15")
      TIMESTAMP="${YEAR}-${MONTH}-${DAY}T00:00:01"

      CLUSTER_ID=$(curl "${RESOURCE_ENDPOINT}/api/v1/clusters" \
        -H "accept: application/json, text/plain, */*" \
        -H "accept-language: en-US,en;q=0.9" \
        -H "authorization: Bearer $CSRG_API_KEY" | jq '.[] | select(.name == "draco-oci-iad") | .id' | tr -d '"')

      INITIATIVE_ITEM_ID=$(curl "${RESOURCE_ENDPOINT}/api/v1/initiative-items" \
        -H "accept: application/json, text/plain, */*" \
        -H "accept-language: en-US,en;q=0.9" \
        -H "authorization: Bearer $CSRG_API_KEY" | jq '.[] | select(.name == "coreai_dlalgo_mcore") | .id' | tr -d '"')

      QUOTA=$(curl "${RESOURCE_ENDPOINT}/api/v1/capacity-requests" \
        -H "accept: application/json, text/plain, */*" \
        -H "accept-language: en-US,en;q=0.9" \
        -H "authorization: Bearer $CSRG_API_KEY" | jq --arg CLUSTER_ID $CLUSTER_ID --arg INITIATIVE_ITEM_ID $INITIATIVE_ITEM_ID '[.[] | select(.clusterId == $CLUSTER_ID and .initiativeItemId == $INITIATIVE_ITEM_ID)] | to_entries | [last] | .[0].value.quantity')

      USED_CAPA=$(sacct \
        -u ${ALL_USER} \
        --partition batch_block1,batch_block3,batch_block4 \
        --truncate \
        -A coreai_dlalgo_mcore \
        -S ${TIMESTAMP} \
        -X \
        --format JobID,JobName%20,Partition,AllocNodes,ElapsedRaw \
        -p \
        -n \
      | awk -F "|" '{{sum+=$4*$5}} END {{print sum*8/3600}}')
      TOTAL_CAPA=$(( $QUOTA*24*30 ))

      USAGE=$(echo "$USED_CAPA $TOTAL_CAPA" | awk '{print (1 - $1/$2)*100}')%

      echo "Usage left: $USAGE"
      echo "Disclaimer: Please be careful with this number. Usage does not imply
        what we are guaranteed to get a slot, SLURM scheduling is more complicated
        than that. The number is rather a proxy to the FairShare that determines
        our job-scheduling-priority.

        Most important take-away of this number is to get a sense how much much
        we are eating up our budget such that we can discuss this with capacity planning.
        "

build_image:
  tags:
    - mcore-docker-node
  image: docker:26.1.4-dind
  needs: []  # May start ASAP
  stage: build
  timeout: 30m
  parallel:
    matrix:
      - IMAGE: CI_MCORE_IMAGE
        FILE: Dockerfile.ci
        BASE_IMAGE: nvcr.io/nvidia/pytorch:24.01-py3
      - IMAGE: CI_NEMO_IMAGE
        FILE: Dockerfile.ci
        BASE_IMAGE: nvcr.io/nvidian/nemo:nightly
      - IMAGE: LINTING_IMAGE
        FILE: Dockerfile.linting
        BASE_IMAGE: python:3.10
  before_script:
    - echo "$NGC_API_KEY" | docker login nvcr.io -u '$oauthtoken' --password-stdin
    - echo "$CI_REGISTRY_PASSWORD" | docker login $CI_REGISTRY -u $CI_REGISTRY_USER --password-stdin
  script:
    - |
      set -x
      eval "IMAGE=\$$IMAGE"

      OLD_IMAGES=$(docker image ls --format "{{.ID}} {{.Repository}}:{{.Tag}}" \
                    | grep -v 'nvcr.io/nvidia/pytorch:24.01-py3' \
                    | grep -v 'gitlab-master.nvidia.com:5005/adlr/megatron-lm/mcore_ci:buildcache' \
                    | grep -v 'gitlab-master.nvidia.com:5005/adlr/megatron-lm/mcore_nemo:buildcache' \
                    | grep -v 'gitlab-master.nvidia.com:5005/adlr/megatron-lm/mcore_linting:buildcache' \
                    | grep -v 'nvcr.io/nvidian/nemo:nightly' \
                    | grep -v 'python:3.10' | awk '{ print $1 }'
                 )
      docker rmi $OLD_IMAGES || true

      if [[ "$CI_COMMIT_BRANCH" == "$CI_DEFAULT_BRANCH" ]]; then
        ADDITIONAL_PARAMS="--pull"
      fi

      docker build \
        -f $FILE \
        -t ${IMAGE}:${CI_PIPELINE_ID} \
        --cache-to type=inline \
        --cache-from type=registry,ref=${IMAGE}:buildcache \
        --build-arg FROM_IMAGE_NAME=$BASE_IMAGE \
        ${ADDITIONAL_PARAMS} .

      docker push ${IMAGE}:${CI_PIPELINE_ID}  

      if [[ "$CI_COMMIT_BRANCH" == "$CI_DEFAULT_BRANCH" ]]; then
        docker tag ${IMAGE}:${CI_PIPELINE_ID} ${IMAGE}:buildcache
        docker push ${IMAGE}:buildcache
      fi
  interruptible: true

.unit_test_common:
  image: ${CI_MCORE_IMAGE}:${CI_PIPELINE_ID} 
  stage: unit_tests
  needs: [build_image]
  tags:
    - 8xL40S
  variables:
    MOE_GROUPED_GEMM: 0 # Set to 1 to enable grouped gemm for MoE
  interruptible: true
  retry:
    max: 2
    when: job_execution_timeout

unit_tests:
  extends: [.unit_test_common]
  script:
    - torchrun --nproc_per_node=8 -m pytest -x -v -s --cov-report=term --cov-report=html --cov=megatron/core --no-cov-on-fail tests/unit_tests
  coverage: '/(?i)total.*? (100(?:\.0+)?\%|[1-9]?\d(?:\.\d+)?\%)$/'
  artifacts:
    paths:
      - coverage
    expire_in: 30 days
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /Run tests/'
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH

unit_tests-data:
  extends: [.unit_test_common]
  script:
    - torchrun --nproc_per_node=8 -m pytest -x -v -s tests/unit_tests/data
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /Run tests/'
      when: never
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: never
    - when: always

unit_tests-dist-checkpointing:
  extends: [.unit_test_common]
  script:
    - torchrun --nproc_per_node=8 -m pytest -x -v -s tests/unit_tests/dist_checkpointing
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /Run tests/'
      when: never
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: never
    - when: always

unit_tests-fusions:
  extends: [.unit_test_common]
  script:
    - torchrun --nproc_per_node=8 -m pytest -x -v -s tests/unit_tests/fusions
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /Run tests/'
      when: never
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: never
    - when: always
    
unit_tests-inference:
  extends: [.unit_test_common]
  script:
    - torchrun --nproc_per_node=8 -m pytest -x -v -s tests/unit_tests/inference
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /Run tests/'
      when: never
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: never
    - when: always

unit_tests-models:
  extends: [.unit_test_common]
  script:
    - torchrun --nproc_per_node=8 -m pytest -x -v -s tests/unit_tests/models
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /Run tests/'
      when: never
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: never
    - when: always

unit_tests-pipeline-parallel:
  extends: [.unit_test_common]
  script:
    - torchrun --nproc_per_node=8 -m pytest -x -v -s tests/unit_tests/pipeline_parallel
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /Run tests/'
      when: never
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: never
    - when: always

unit_tests-tensor-parallel:
  extends: [.unit_test_common]
  script:
    - torchrun --nproc_per_node=8 -m pytest -x -v -s tests/unit_tests/tensor_parallel
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /Run tests/'
      when: never
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: never
    - when: always

unit_tests-transformer:
  extends: [.unit_test_common]
  script:
    - torchrun --nproc_per_node=8 -m pytest -x -v -s tests/unit_tests/transformer
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /Run tests/'
      when: never
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: never
    - when: always

unit_tests-top-py:
  extends: [.unit_test_common]
  script:
    - torchrun --nproc_per_node=8 -m pytest -x -v -s tests/unit_tests/*.py
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_LABELS =~ /Run tests/'
      when: never
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: never
    - when: always

docs_build_test:
  image: gitlab-master.nvidia.com:5005/adlr/megatron-lm/python-format:0.0.1
  stage: unit_tests
  tags:
    - os/linux
  script:
    - cd ..
    - rm -rf documentation && git clone https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab-master.nvidia.com/nemo-megatron-core-tme/documentation.git
    - mv megatron-lm/ documentation/
    - cd documentation/
    - ./repo docs
  allow_failure: true
  except:
    - main
  interruptible: true

formatting:
  image: ${LINTING_IMAGE}:${CI_PIPELINE_ID} 
  tags:
    - os/linux
  stage: unit_tests
  before_script:
    - git fetch origin main
  script:
    - CHECK_ONLY=true bash tools/autoformat.sh

  rules:
    - when: always
  interruptible: true

include:
  - jet-tests.yml
