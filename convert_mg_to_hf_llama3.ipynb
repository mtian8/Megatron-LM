{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching all parameters from the checkpoint at /home/tian/mtian8/LLM4Tutorial/model/MG_model/llama3_8B_tp_1_pp_4.\n",
      "Fetching iteration iter_0000001\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Unshard your model with checkpoint_util.py first!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 231\u001B[0m\n\u001B[1;32m    222\u001B[0m         write_llama_model(\n\u001B[1;32m    223\u001B[0m             model_path\u001B[38;5;241m=\u001B[39moutput_dir,\n\u001B[1;32m    224\u001B[0m             input_base_path\u001B[38;5;241m=\u001B[39minput_dir,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    227\u001B[0m             rope_theta\u001B[38;5;241m=\u001B[39mrope_theta,\n\u001B[1;32m    228\u001B[0m         )\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 231\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 222\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    220\u001B[0m rotary_base \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1e4\u001B[39m\n\u001B[1;32m    221\u001B[0m rope_theta \u001B[38;5;241m=\u001B[39m rotary_base\n\u001B[0;32m--> 222\u001B[0m \u001B[43mwrite_llama_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_base_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_output_shards\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_output_shards\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnorm_eps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    227\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrope_theta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrope_theta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    228\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 99\u001B[0m, in \u001B[0;36mwrite_llama_model\u001B[0;34m(model_path, input_base_path, num_output_shards, norm_eps, rope_theta)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;66;03m# Load weights\u001B[39;00m\n\u001B[1;32m     98\u001B[0m base_path \u001B[38;5;241m=\u001B[39m Path(input_base_path)\u001B[38;5;241m/\u001B[39miteration\n\u001B[0;32m---> 99\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mlist\u001B[39m(base_path\u001B[38;5;241m.\u001B[39mglob(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmp_rank_*\u001B[39m\u001B[38;5;124m\"\u001B[39m))) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnshard your model with checkpoint_util.py first!\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    100\u001B[0m loaded \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(base_path\u001B[38;5;241m/\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmp_rank_00\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_optim_rng.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m, map_location\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    101\u001B[0m args \u001B[38;5;241m=\u001B[39m loaded[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124margs\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[0;31mAssertionError\u001B[0m: Unshard your model with checkpoint_util.py first!"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import trange\n",
    "from transformers import LlamaConfig, LlamaForCausalLM, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "from megatron.training.tokenizer import build_tokenizer\n",
    "\n",
    "def permute_qkv(qkv_w: torch.Tensor, dim: int, n_heads: int,\n",
    "                n_heads_kv: int, revert: bool = False) -> torch.Tensor:\n",
    "\n",
    "    def permute(x):\n",
    "        if revert:\n",
    "            return x.view(head_dim//2, 2, dim).transpose(0, 1).reshape(head_dim, dim)\n",
    "        return x.view(2, head_dim//2, dim).transpose(0, 1).reshape(head_dim, dim)\n",
    "\n",
    "    head_dim = dim//n_heads\n",
    "    n_qs_per_kv = n_heads//n_heads_kv\n",
    "    n_groups = qkv_w.size(0)//head_dim//(n_qs_per_kv + 2)\n",
    "    groups = torch.chunk(qkv_w, n_groups, dim=0)\n",
    "    new = []\n",
    "    for group in groups:\n",
    "        *qs, k, v = torch.split(group, head_dim, dim=0)\n",
    "        assert len(qs) == n_qs_per_kv, f\"{len(qs)}, {n_qs_per_kv}\"\n",
    "        new += list(map(permute, qs)) + [permute(k), v]\n",
    "    return torch.cat(new, dim=0)\n",
    "\n",
    "def write_json(text, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(text, f)\n",
    "\n",
    "\n",
    "def convert_wqkv(llama_mega, layer_idx=0, n_heads=32, n_heads_kv=8):\n",
    "    qkv_w = llama_mega[f'decoder.layers.{layer_idx}.self_attention.linear_qkv.weight']\n",
    "    n_hidden = qkv_w.size(1)\n",
    "    hidden_dim = n_hidden//n_heads\n",
    "    print(n_hidden, hidden_dim, n_heads, n_heads_kv)\n",
    "    qkv_w = permute_qkv(qkv_w, n_hidden, n_heads, n_heads_kv, revert=True)\n",
    "\n",
    "    n_qs_per_kv = n_heads//n_heads_kv\n",
    "    n_groups = qkv_w.size(0)//hidden_dim//(n_qs_per_kv + 2)\n",
    "    qkv_w = list(torch.split(qkv_w, hidden_dim, dim=0))\n",
    "\n",
    "    wq, wk, wv = [], [], []\n",
    "    for group in range(n_groups):\n",
    "        for qs in range(n_qs_per_kv):\n",
    "            wq.append(qkv_w[0])\n",
    "            del qkv_w[0]\n",
    "        wk.append(qkv_w[0])\n",
    "        del qkv_w[0]\n",
    "        wv.append(qkv_w[0])\n",
    "        del qkv_w[0]\n",
    "    assert len(qkv_w) == 0\n",
    "\n",
    "    wq = torch.concat(wq, dim=0)\n",
    "    wk = torch.concat(wk, dim=0)\n",
    "    wv = torch.concat(wv, dim=0)\n",
    "    return wq, wk, wv\n",
    "\n",
    "\n",
    "def convert_ffn(llama_mega, layer_idx=0, n_dense=11008):\n",
    "    mega_ffn = llama_mega[f\"decoder.layers.{layer_idx}.mlp.linear_fc1.weight\"]\n",
    "    ffn_w3, ffn_w1 = mega_ffn.split(n_dense, dim=0)\n",
    "    return ffn_w1, ffn_w3\n",
    "\n",
    "# copy the functions from the original MergeLM code\n",
    "def copy_params_to_model(params, model):  # copying code from model_merging_methods.merging_methods\n",
    "    for param_name, param_value in model.named_parameters():\n",
    "        if param_name in params:\n",
    "            param_value.data.copy_(params[param_name])\n",
    "        else:\n",
    "            print(f\"param_name {param_name} not in params\")\n",
    "\n",
    "def write_llama_model(model_path,\n",
    "                input_base_path,\n",
    "                num_output_shards: int=2,\n",
    "                norm_eps: float=1e-05,\n",
    "                rope_theta: float=1e4):\n",
    "\n",
    "    # Preliminaries\n",
    "    print(f\"Fetching all parameters from the checkpoint at {input_base_path}.\")\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    with open(os.path.join(input_base_path, 'latest_checkpointed_iteration.txt')) as f:\n",
    "        iteration = f.read()\n",
    "    if iteration != \"release\":\n",
    "        iteration = f\"iter_{int(iteration):07d}\"\n",
    "    print(f\"Fetching iteration {iteration}\")\n",
    "\n",
    "    # Load weights\n",
    "    base_path = Path(input_base_path)/iteration\n",
    "    assert len(list(base_path.glob(\"mp_rank_*\"))) == 1, \"Unshard your model with checkpoint_util.py first!\"\n",
    "    loaded = torch.load(base_path/\"mp_rank_00\"/\"model_optim_rng.pt\", map_location=\"cpu\")\n",
    "    args = loaded['args']\n",
    "\n",
    "    loaded = loaded['model']\n",
    "    if False:  # 'transformer' not in loaded:  # normalize key names\n",
    "        loaded[\"transformer\"] = loaded.pop(\"encoder\")\n",
    "        for key in list(loaded[\"transformer\"].keys()):\n",
    "            loaded[\"transformer\"][key.replace(\"self_attention\", \"attention\")] = loaded[\"transformer\"].pop(key)\n",
    "        loaded[\"embedding\"][\"word_embeddings.weight\"] = loaded[\"embedding\"].pop(\"word_embeddings\")[\"weight\"]\n",
    "        args.num_layers = args.encoder_num_layers\n",
    "\n",
    "    # Load arguments\n",
    "    n_layers = args.num_layers\n",
    "    n_heads = args.num_attention_heads\n",
    "    n_heads_kv = getattr(args, \"num_query_groups\", n_heads)\n",
    "    n_dense = args.ffn_hidden_size\n",
    "    n_hidden = args.hidden_size\n",
    "    hidden_per_head = n_hidden // n_heads\n",
    "    dim = n_hidden // n_heads\n",
    "    q_size = n_heads//n_heads_kv\n",
    "    intermediate_size = args.ffn_hidden_size\n",
    "    inv_freq = 1.0 / (rope_theta ** (torch.arange(0, hidden_per_head, 2).float() / hidden_per_head))\n",
    "\n",
    "    print('Llama-Megatron Loaded!')\n",
    "    param_count = 0\n",
    "    index_dict = {\"weight_map\": {}}\n",
    "    \n",
    "    weight_parameters = {}\n",
    "    \n",
    "    # Start conversion\n",
    "    # with TemporaryDirectory(prefix=model_path) as tmp_model_path:\n",
    "    print(f'Weighted Converting for {n_layers} layers...')\n",
    "    for layer_i in range(n_layers):\n",
    "        # filename = f\"pytorch_model-{layer_i + 1}-of-{n_layers + 1}.bin\"\n",
    "        megatron_qkv_weight = loaded[f'decoder.layers.{layer_i}.self_attention.linear_qkv.weight'].view(n_heads_kv, (q_size+2)*dim, -1)\n",
    "        q_proj_w_megatron = megatron_qkv_weight[:, :q_size*dim, :]\n",
    "        k_proj_w_megatron = megatron_qkv_weight[:, q_size*dim:(q_size+1)*dim, :]\n",
    "        v_proj_w_megatron = megatron_qkv_weight[:, (q_size+1)*dim:, :]\n",
    "        wq_proj = q_proj_w_megatron.reshape(-1, n_hidden)\n",
    "        wk_proj = k_proj_w_megatron.reshape(-1, n_hidden)\n",
    "        wv_proj = v_proj_w_megatron.reshape(-1, n_hidden)\n",
    "        print(wq_proj.shape, wk_proj.shape, wv_proj.shape)\n",
    "\n",
    "        ffn_w1, ffn_w3 = convert_ffn(llama_mega=loaded, \n",
    "                                    layer_idx=layer_i, \n",
    "                                    n_dense=n_dense)\n",
    "        state_dict = {\n",
    "            f\"model.layers.{layer_i}.self_attn.q_proj.weight\": wq_proj,\n",
    "            f\"model.layers.{layer_i}.self_attn.k_proj.weight\": wk_proj,\n",
    "            f\"model.layers.{layer_i}.self_attn.v_proj.weight\": wv_proj,\n",
    "            f\"model.layers.{layer_i}.self_attn.o_proj.weight\": loaded[f\"decoder.layers.{layer_i}.self_attention.linear_proj.weight\"],\n",
    "            f\"model.layers.{layer_i}.mlp.gate_proj.weight\": ffn_w3,\n",
    "            f\"model.layers.{layer_i}.mlp.down_proj.weight\": loaded[f\"decoder.layers.{layer_i}.mlp.linear_fc2.weight\"],\n",
    "            f\"model.layers.{layer_i}.mlp.up_proj.weight\": ffn_w1,\n",
    "            f\"model.layers.{layer_i}.input_layernorm.weight\": loaded[f\"decoder.layers.{layer_i}.self_attention.linear_qkv.layer_norm_weight\"],\n",
    "            f\"model.layers.{layer_i}.post_attention_layernorm.weight\": loaded[f\"decoder.layers.{layer_i}.mlp.linear_fc1.layer_norm_weight\"],\n",
    "            f\"model.layers.{layer_i}.self_attn.rotary_emb.inv_freq\": inv_freq\n",
    "        }\n",
    "        for k, v in state_dict.items():\n",
    "            print(f'key: {k}, shape: {v.shape}')\n",
    "        \n",
    "        weight_parameters.update(state_dict)\n",
    "\n",
    "        # for k, v in state_dict.items():\n",
    "        #     index_dict[\"weight_map\"][k] = filename\n",
    "        #     param_count += v.numel()\n",
    "        # # print(\"start writing...\", flush=True)\n",
    "        # # print(state_dict)\n",
    "        # # print(\"saving to\", os.path.join(tmp_model_path, filename))\n",
    "        # torch.save(state_dict, os.path.join(tmp_model_path, filename))\n",
    "        print(f'Sharded file saved')\n",
    "\n",
    "    # filename = f\"pytorch_model-{n_layers + 1}-of-{n_layers + 1}.bin\"\n",
    "    state_dict = {\n",
    "        \"model.norm.weight\": loaded['decoder.final_layernorm.weight'],\n",
    "        \"lm_head.weight\": loaded['output_layer.weight'],\n",
    "        \"model.embed_tokens.weight\": loaded['embedding.word_embeddings.weight']\n",
    "    }\n",
    "    \n",
    "    weight_parameters.update(state_dict)\n",
    "\n",
    "    for k, v in weight_parameters.items():\n",
    "        param_count += v.numel()\n",
    "    torch_dtype = state_dict[\"lm_head.weight\"].dtype\n",
    "    # print(\"start writing...\", flush=True)\n",
    "    # torch.save(state_dict, os.path.join(tmp_model_path, filename))\n",
    "    # print(f'Sharded file saved to {filename}')\n",
    "\n",
    "    # # Write configs and save\n",
    "    # index_dict[\"metadata\"] = {\"total_size\": param_count * 2}\n",
    "    # write_json(index_dict, os.path.join(tmp_model_path, \"pytorch_model.bin.index.json\"))\n",
    "    # config = LlamaConfig.from_json_file('/home/tian/mtian8/LLM4Tutorial/model/HF_model/Meta-Llama-3-8B/config.json')\n",
    "    # config.rope_theta = rope_theta\n",
    "    # config.save_pretrained(tmp_model_path)\n",
    "\n",
    "    # Make space so we can load the model properly now.\n",
    "    del state_dict\n",
    "    del loaded\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Loading the checkpoint in a Llama model...\")\n",
    "    model = LlamaForCausalLM.from_pretrained(\"/home/tian/mtian8/LLM4Tutorial/model/HF_model/Meta-Llama-3-8B/\", torch_dtype=torch_dtype)\n",
    "    copy_params_to_model(weight_parameters, model)\n",
    "    \n",
    "    # Avoid saving this as part of the config.\n",
    "    del model.config._name_or_path\n",
    "\n",
    "    print(\"Saving in the Transformers format.\")\n",
    "    max_num_params_per_shard = param_count*2 // max(1,(num_output_shards-1))\n",
    "    print(f\"Saving with a maximum of {max_num_params_per_shard} parameters per shard.\")\n",
    "    print(f\"There are {param_count*2} parameters in total. num_output_shards={num_output_shards}\")\n",
    "    model.save_pretrained(model_path, max_shard_size=max_num_params_per_shard)\n",
    "\n",
    "def main():\n",
    "    # make sure megatron is importable\n",
    "    output_dir = \"/home/tian/mtian8/LLM4Tutorial/model/HF_converted_model\"\n",
    "    input_dir = \"/home/tian/mtian8/LLM4Tutorial/model/MG_model/llama3_8B_tp_1_pp_4\"\n",
    "    num_output_shards = 2\n",
    "    if True:\n",
    "        eps = 1e-5\n",
    "        rotary_base = 1e4\n",
    "        rope_theta = rotary_base\n",
    "        write_llama_model(\n",
    "            model_path=output_dir,\n",
    "            input_base_path=input_dir,\n",
    "            num_output_shards=num_output_shards,\n",
    "            norm_eps=eps,\n",
    "            rope_theta=rope_theta,\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mtian8/anaconda3/envs/mg-lm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching all parameters from the checkpoint at /home/tian/mtian8/LLM4Tutorial/model/MG_model/llama3_8B_tp_1.\n",
      "Fetching iteration iter_0000001\n",
      "Llama-Megatron Loaded!\n",
      "Weighted Converting for 32 layers...\n",
      "Sharded file saved to pytorch_model-1-of-33.bin\n",
      "Sharded file saved to pytorch_model-2-of-33.bin\n",
      "Sharded file saved to pytorch_model-3-of-33.bin\n",
      "Sharded file saved to pytorch_model-4-of-33.bin\n",
      "Sharded file saved to pytorch_model-5-of-33.bin\n",
      "Sharded file saved to pytorch_model-6-of-33.bin\n",
      "Sharded file saved to pytorch_model-7-of-33.bin\n",
      "Sharded file saved to pytorch_model-8-of-33.bin\n",
      "Sharded file saved to pytorch_model-9-of-33.bin\n",
      "Sharded file saved to pytorch_model-10-of-33.bin\n",
      "Sharded file saved to pytorch_model-11-of-33.bin\n",
      "Sharded file saved to pytorch_model-12-of-33.bin\n",
      "Sharded file saved to pytorch_model-13-of-33.bin\n",
      "Sharded file saved to pytorch_model-14-of-33.bin\n",
      "Sharded file saved to pytorch_model-15-of-33.bin\n",
      "Sharded file saved to pytorch_model-16-of-33.bin\n",
      "Sharded file saved to pytorch_model-17-of-33.bin\n",
      "Sharded file saved to pytorch_model-18-of-33.bin\n",
      "Sharded file saved to pytorch_model-19-of-33.bin\n",
      "Sharded file saved to pytorch_model-20-of-33.bin\n",
      "Sharded file saved to pytorch_model-21-of-33.bin\n",
      "Sharded file saved to pytorch_model-22-of-33.bin\n",
      "Sharded file saved to pytorch_model-23-of-33.bin\n",
      "Sharded file saved to pytorch_model-24-of-33.bin\n",
      "Sharded file saved to pytorch_model-25-of-33.bin\n",
      "Sharded file saved to pytorch_model-26-of-33.bin\n",
      "Sharded file saved to pytorch_model-27-of-33.bin\n",
      "Sharded file saved to pytorch_model-28-of-33.bin\n",
      "Sharded file saved to pytorch_model-29-of-33.bin\n",
      "Sharded file saved to pytorch_model-30-of-33.bin\n",
      "Sharded file saved to pytorch_model-31-of-33.bin\n",
      "Sharded file saved to pytorch_model-32-of-33.bin\n",
      "model.norm.weight: tensor([2.6562, 2.5781, 2.6094,  ..., 2.5938, 2.2656, 2.5156])\n",
      "lm_head.weight: tensor([[ 0.0099,  0.0173,  0.0034,  ...,  0.0007, -0.0168, -0.0110],\n",
      "        [-0.0069,  0.0117,  0.0112,  ..., -0.0085,  0.0091, -0.0015],\n",
      "        [ 0.0143,  0.0096,  0.0089,  ..., -0.0024, -0.0062, -0.0142],\n",
      "        ...,\n",
      "        [-0.0034,  0.0020,  0.0058,  ...,  0.0015,  0.0059,  0.0069],\n",
      "        [-0.0034,  0.0020,  0.0058,  ...,  0.0015,  0.0059,  0.0069],\n",
      "        [-0.0034,  0.0020,  0.0058,  ...,  0.0015,  0.0059,  0.0069]])\n",
      "model.embed_tokens.weight: tensor([[ 1.3733e-03,  5.0964e-03, -3.0365e-03,  ...,  2.2888e-03,\n",
      "         -1.9531e-03, -1.7166e-05],\n",
      "        [-2.7313e-03,  1.9379e-03, -1.3733e-03,  ..., -5.1498e-05,\n",
      "         -1.3962e-03, -1.9836e-03],\n",
      "        [ 9.5367e-04, -1.3367e-02,  4.1771e-04,  ...,  2.5940e-03,\n",
      "          7.0496e-03,  4.1809e-03],\n",
      "        ...,\n",
      "        [ 1.8715e-23,  3.2699e-24,  1.8198e-23,  ...,  5.3767e-23,\n",
      "         -2.2360e-24, -1.9852e-23],\n",
      "        [ 1.9335e-23, -1.8612e-24, -1.8818e-23,  ...,  2.3368e-23,\n",
      "          7.3412e-24, -3.1226e-23],\n",
      "        [-7.4860e-23, -6.3693e-23,  5.5059e-24,  ...,  4.9631e-24,\n",
      "         -5.4594e-23, -2.2877e-24]])\n",
      "Sharded file saved to pytorch_model-33-of-33.bin\n",
      "Loading the checkpoint in a Llama model...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /home/tian/mtian8/LLM4Tutorial/model/llama3_convert/xf3l504j.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 174\u001B[0m\n\u001B[1;32m    165\u001B[0m         write_llama_model(\n\u001B[1;32m    166\u001B[0m             model_path\u001B[38;5;241m=\u001B[39moutput_dir,\n\u001B[1;32m    167\u001B[0m             input_base_path\u001B[38;5;241m=\u001B[39minput_dir,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    170\u001B[0m             rope_theta\u001B[38;5;241m=\u001B[39mrope_theta,\n\u001B[1;32m    171\u001B[0m         )\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 174\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[1], line 165\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    163\u001B[0m rotary_base \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1e4\u001B[39m\n\u001B[1;32m    164\u001B[0m rope_theta \u001B[38;5;241m=\u001B[39m rotary_base\n\u001B[0;32m--> 165\u001B[0m \u001B[43mwrite_llama_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_base_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_output_shards\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_output_shards\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnorm_eps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrope_theta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrope_theta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[1], line 146\u001B[0m, in \u001B[0;36mwrite_llama_model\u001B[0;34m(model_path, input_base_path, num_output_shards, norm_eps, rope_theta)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading the checkpoint in a Llama model...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    143\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;66;03m# Change for model\u001B[39;00m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m--> 146\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mLlamaForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtmp_model_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;66;03m# Avoid saving this as part of the config.\u001B[39;00m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_name_or_path\n",
      "File \u001B[0;32m~/mtian8/anaconda3/envs/mg-lm/lib/python3.11/site-packages/transformers/modeling_utils.py:3460\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3455\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m   3456\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError no file named \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001B[38;5;250m \u001B[39mvariant)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m found in directory\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3457\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3458\u001B[0m         )\n\u001B[1;32m   3459\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3460\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m   3461\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError no file named \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_add_variant(WEIGHTS_NAME,\u001B[38;5;250m \u001B[39mvariant)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001B[38;5;250m \u001B[39mvariant)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3462\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mTF2_WEIGHTS_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mTF_WEIGHTS_NAME\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.index\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m or \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mFLAX_WEIGHTS_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m found in directory\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3463\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3464\u001B[0m         )\n\u001B[1;32m   3465\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(subfolder, pretrained_model_name_or_path)):\n\u001B[1;32m   3466\u001B[0m     archive_file \u001B[38;5;241m=\u001B[39m pretrained_model_name_or_path\n",
      "\u001B[0;31mOSError\u001B[0m: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /home/tian/mtian8/LLM4Tutorial/model/llama3_convert/xf3l504j."
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import trange\n",
    "from transformers import LlamaConfig, LlamaForCausalLM, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "from megatron.training.tokenizer import build_tokenizer\n",
    "\n",
    "def write_json(text, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(text, f)\n",
    "\n",
    "\n",
    "def convert_ffn(llama_mega, layer_idx=0, n_dense=11008):\n",
    "    mega_ffn = llama_mega[f\"decoder.layers.{layer_idx}.mlp.linear_fc1.weight\"]\n",
    "    ffn_w3, ffn_w1 = mega_ffn.split(n_dense, dim=0)\n",
    "    return ffn_w1, ffn_w3\n",
    "\n",
    "\n",
    "def write_llama_model(model_path,\n",
    "                input_base_path,\n",
    "                num_output_shards: int=2,\n",
    "                norm_eps: float=1e-05,\n",
    "                rope_theta: float=1e4):\n",
    "\n",
    "    # Preliminaries\n",
    "    print(f\"Fetching all parameters from the checkpoint at {input_base_path}.\")\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    with open(os.path.join(input_base_path, 'latest_checkpointed_iteration.txt')) as f:\n",
    "        iteration = f.read()\n",
    "    if iteration != \"release\":\n",
    "        iteration = f\"iter_{int(iteration):07d}\"\n",
    "    print(f\"Fetching iteration {iteration}\")\n",
    "\n",
    "    # Load weights\n",
    "    base_path = Path(input_base_path)/iteration\n",
    "    assert len(list(base_path.glob(\"mp_rank_*\"))) == 1, \"Unshard your model with checkpoint_util.py first!\"\n",
    "    loaded = torch.load(base_path/\"mp_rank_00\"/\"model_optim_rng.pt\", map_location=\"cpu\")\n",
    "    args = loaded['args']\n",
    "\n",
    "    loaded = loaded['model']\n",
    "    if False:  # 'transformer' not in loaded:  # normalize key names\n",
    "        loaded[\"transformer\"] = loaded.pop(\"encoder\")\n",
    "        for key in list(loaded[\"transformer\"].keys()):\n",
    "            loaded[\"transformer\"][key.replace(\"self_attention\", \"attention\")] = loaded[\"transformer\"].pop(key)\n",
    "        loaded[\"embedding\"][\"word_embeddings.weight\"] = loaded[\"embedding\"].pop(\"word_embeddings\")[\"weight\"]\n",
    "        args.num_layers = args.encoder_num_layers\n",
    "\n",
    "    # Load arguments\n",
    "    n_layers = args.num_layers\n",
    "    n_heads = args.num_attention_heads\n",
    "    n_heads_kv = getattr(args, \"num_query_groups\", n_heads)\n",
    "    n_dense = args.ffn_hidden_size\n",
    "    n_hidden = args.hidden_size\n",
    "    hidden_per_head = n_hidden // n_heads\n",
    "    dim = n_hidden // n_heads\n",
    "    q_size = n_heads//n_heads_kv\n",
    "    intermediate_size = args.ffn_hidden_size\n",
    "    inv_freq = 1.0 / (rope_theta ** (torch.arange(0, hidden_per_head, 2).float() / hidden_per_head))\n",
    "\n",
    "    print('Llama-Megatron Loaded!')\n",
    "    param_count = 0\n",
    "    index_dict = {\"weight_map\": {}}\n",
    "    \n",
    "    weight_parameters = {}\n",
    "    \n",
    "    # Start conversion\n",
    "    with TemporaryDirectory(prefix=model_path) as tmp_model_path:\n",
    "        print(f'Weighted Converting for {n_layers} layers...')\n",
    "        for layer_i in range(n_layers):\n",
    "            filename = f\"pytorch_model-{layer_i + 1}-of-{n_layers + 1}.bin\"\n",
    "            megatron_qkv_weight = loaded[f'decoder.layers.{layer_i}.self_attention.linear_qkv.weight'].view(n_heads_kv, (q_size+2)*dim, -1)\n",
    "            q_proj_w_megatron = megatron_qkv_weight[:, :q_size*dim, :]\n",
    "            k_proj_w_megatron = megatron_qkv_weight[:, q_size*dim:(q_size+1)*dim, :]\n",
    "            v_proj_w_megatron = megatron_qkv_weight[:, (q_size+1)*dim:, :]\n",
    "            wq_proj = q_proj_w_megatron.reshape(-1, n_hidden)\n",
    "            wk_proj = k_proj_w_megatron.reshape(-1, n_hidden)\n",
    "            wv_proj = v_proj_w_megatron.reshape(-1, n_hidden)\n",
    "            \n",
    "            ffn_w1, ffn_w3 = convert_ffn(llama_mega=loaded, \n",
    "                                        layer_idx=layer_i, \n",
    "                                        n_dense=n_dense)\n",
    "            state_dict = {\n",
    "                f\"model.layers.{layer_i}.self_attn.q_proj.weight\": wq_proj,\n",
    "                f\"model.layers.{layer_i}.self_attn.k_proj.weight\": wk_proj,\n",
    "                f\"model.layers.{layer_i}.self_attn.v_proj.weight\": wv_proj,\n",
    "                f\"model.layers.{layer_i}.self_attn.o_proj.weight\": loaded[f\"decoder.layers.{layer_i}.self_attention.linear_proj.weight\"],\n",
    "                f\"model.layers.{layer_i}.mlp.gate_proj.weight\": ffn_w3,\n",
    "                f\"model.layers.{layer_i}.mlp.down_proj.weight\": loaded[f\"decoder.layers.{layer_i}.mlp.linear_fc2.weight\"],\n",
    "                f\"model.layers.{layer_i}.mlp.up_proj.weight\": ffn_w1,\n",
    "                f\"model.layers.{layer_i}.input_layernorm.weight\": loaded[f\"decoder.layers.{layer_i}.self_attention.linear_qkv.layer_norm_weight\"],\n",
    "                f\"model.layers.{layer_i}.post_attention_layernorm.weight\": loaded[f\"decoder.layers.{layer_i}.mlp.linear_fc1.layer_norm_weight\"],\n",
    "                f\"model.layers.{layer_i}.self_attn.rotary_emb.inv_freq\": inv_freq\n",
    "            }\n",
    "\n",
    "            for k, v in state_dict.items():\n",
    "                index_dict[\"weight_map\"][k] = filename\n",
    "                param_count += v.numel()\n",
    "            torch.save(state_dict, os.path.join(tmp_model_path, filename))\n",
    "            print(f'Sharded file saved to {filename}')\n",
    "\n",
    "        filename = f\"pytorch_model-{n_layers + 1}-of-{n_layers + 1}.bin\"\n",
    "        state_dict = {\n",
    "            \"model.norm.weight\": loaded['decoder.final_layernorm.weight'],\n",
    "            \"lm_head.weight\": loaded['output_layer.weight'],\n",
    "            \"model.embed_tokens.weight\": loaded['embedding.word_embeddings.weight']\n",
    "        }\n",
    "\n",
    "        for k, v in weight_parameters.items():\n",
    "            index_dict[\"weight_map\"][k] = filename\n",
    "            param_count += v.numel()\n",
    "        torch_dtype = state_dict[\"lm_head.weight\"].dtype\n",
    "        print('model.norm.weight:',state_dict[\"model.norm.weight\"])\n",
    "        print(\"lm_head.weight:\", state_dict[\"lm_head.weight\"])\n",
    "        print(\"model.embed_tokens.weight:\",state_dict[\"model.embed_tokens.weight\"])\n",
    "        torch.save(state_dict, os.path.join(tmp_model_path, filename))\n",
    "        print(f'Sharded file saved to {filename}')\n",
    "\n",
    "        # Write configs and save\n",
    "        # index_dict[\"metadata\"] = {\"total_size\": param_count * 2}\n",
    "        # write_json(index_dict, os.path.join(tmp_model_path, \"pytorch_model.bin.index.json\"))\n",
    "        #\n",
    "        # Change for Config File \n",
    "        #\n",
    "        config = LlamaConfig.from_json_file('/home/tian/mtian8/LLM4Tutorial/model/HF_model/Meta-Llama-3-8B/config.json')\n",
    "        config.rope_theta = rope_theta\n",
    "        config.save_pretrained(tmp_model_path)\n",
    "\n",
    "        # Make space so we can load the model properly now.\n",
    "        del state_dict\n",
    "        del loaded\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Loading the checkpoint in a Llama model...\")\n",
    "        #\n",
    "        # Change for model\n",
    "        #\n",
    "        model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch_dtype)\n",
    "        # Avoid saving this as part of the config.\n",
    "        del model.config._name_or_path\n",
    "\n",
    "    print(\"Saving in the Transformers format.\")\n",
    "    max_num_params_per_shard = param_count*2 // max(1,(num_output_shards-1))\n",
    "    print(f\"Saving with a maximum of {max_num_params_per_shard} parameters per shard.\")\n",
    "    print(f\"There are {param_count*2} parameters in total. num_output_shards={num_output_shards}\")\n",
    "    model.save_pretrained(model_path, max_shard_size=max_num_params_per_shard)\n",
    "\n",
    "def main():\n",
    "    # make sure megatron is importable\n",
    "    output_dir = \"/home/tian/mtian8/LLM4Tutorial/model/llama3_convert/\"\n",
    "    input_dir = \"/home/tian/mtian8/LLM4Tutorial/model/MG_model/llama3_8B_tp_1\"\n",
    "    num_output_shards = 2\n",
    "    if True:\n",
    "        eps = 1e-5\n",
    "        rotary_base = 1e4\n",
    "        rope_theta = rotary_base\n",
    "        write_llama_model(\n",
    "            model_path=output_dir,\n",
    "            input_base_path=input_dir,\n",
    "            num_output_shards=num_output_shards,\n",
    "            norm_eps=eps,\n",
    "            rope_theta=rope_theta,\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:50<00:00, 27.74s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.95it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:05<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM,LlamaForCausalLM\n",
    "import torch\n",
    "def diff(model1, model2):  # compare two models\n",
    "    for name1, param1 in model1.named_parameters():\n",
    "        param2 = model2.state_dict()[name1]\n",
    "        if not torch.equal(param1, param2):\n",
    "            print(name1, \"different by\", torch.norm(param1 - param2).item())\n",
    "model_1 = AutoModelForCausalLM.from_pretrained(\"/home/tian/mtian8/LLM4Tutorial/model/HF_model/Meta-Llama-3-8B\")\n",
    "model_2 = AutoModelForCausalLM.from_pretrained(\"/home/tian/mtian8/LLM4Tutorial/model/HF_converted_model/\")\n",
    "model_3 = AutoModelForCausalLM.from_pretrained(\"/home/tian/mtian8/LLM4Tutorial/model/HF_converted_from_mcore\")\n",
    "diff(model_1, model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1040, -0.1543,  0.0737,  ...,  0.0312, -0.0231,  0.0442],\n",
       "        [-0.0564, -0.0869,  0.0188,  ...,  0.0193, -0.0073,  0.0293],\n",
       "        [-0.0200, -0.0564,  0.0417,  ...,  0.0056, -0.0159,  0.0449],\n",
       "        ...,\n",
       "        [ 0.0085,  0.0250, -0.0197,  ..., -0.0177, -0.0069,  0.0030],\n",
       "        [ 0.0136,  0.0356, -0.0162,  ..., -0.0177,  0.0018,  0.0102],\n",
       "        [ 0.0039, -0.0100,  0.0118,  ..., -0.0153,  0.0016, -0.0206]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_1.state_dict()[\"model.layers.0.self_attn.k_proj.weight\"].shape)\n",
    "model_1.state_dict()[\"model.layers.0.self_attn.k_proj.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1040, -0.1543,  0.0737,  ...,  0.0312, -0.0231,  0.0442],\n",
       "        [-0.0564, -0.0869,  0.0188,  ...,  0.0193, -0.0073,  0.0293],\n",
       "        [-0.0200, -0.0564,  0.0417,  ...,  0.0056, -0.0159,  0.0449],\n",
       "        ...,\n",
       "        [ 0.0085,  0.0250, -0.0197,  ..., -0.0177, -0.0069,  0.0030],\n",
       "        [ 0.0136,  0.0356, -0.0162,  ..., -0.0177,  0.0018,  0.0102],\n",
       "        [ 0.0039, -0.0100,  0.0118,  ..., -0.0153,  0.0016, -0.0206]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_2.state_dict()[\"model.layers.0.self_attn.k_proj.weight\"].shape)\n",
    "model_2.state_dict()[\"model.layers.0.self_attn.k_proj.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7618e-03, -2.9053e-02, -3.1586e-03,  ...,  7.3547e-03,\n",
       "         -4.6875e-02, -2.1606e-02],\n",
       "        [-1.2512e-02, -6.9824e-02, -3.8605e-03,  ..., -1.2573e-02,\n",
       "         -4.9805e-02,  2.0508e-02],\n",
       "        [-1.8799e-02, -4.6631e-02, -4.7607e-03,  ...,  1.1475e-02,\n",
       "         -1.3245e-02,  1.1536e-02],\n",
       "        ...,\n",
       "        [-4.5776e-03, -4.0283e-02,  7.1777e-02,  ...,  5.0659e-03,\n",
       "         -2.3956e-03,  2.5024e-03],\n",
       "        [-5.2795e-03, -1.4709e-02,  4.1504e-02,  ...,  5.4321e-03,\n",
       "         -3.2349e-03,  4.4346e-05],\n",
       "        [-4.1504e-03, -1.6724e-02,  3.0396e-02,  ...,  8.6060e-03,\n",
       "          8.0872e-04,  3.1433e-03]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.state_dict()[\"model.layers.0.self_attn.q_proj.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7618e-03, -2.9053e-02, -3.1586e-03,  ...,  7.3547e-03,\n",
       "         -4.6875e-02, -2.1606e-02],\n",
       "        [-1.8799e-02, -4.6631e-02, -4.7607e-03,  ...,  1.1475e-02,\n",
       "         -1.3245e-02,  1.1536e-02],\n",
       "        [ 4.7302e-03, -4.7607e-02,  3.9816e-05,  ...,  1.7700e-02,\n",
       "         -1.6479e-02, -1.8066e-02],\n",
       "        ...,\n",
       "        [ 2.1484e-02,  5.3955e-02, -2.6245e-02,  ..., -7.1106e-03,\n",
       "          8.4229e-03, -2.2736e-03],\n",
       "        [-4.5776e-03, -4.0283e-02,  7.1777e-02,  ...,  5.0659e-03,\n",
       "         -2.3956e-03,  2.5024e-03],\n",
       "        [-4.1504e-03, -1.6724e-02,  3.0396e-02,  ...,  8.6060e-03,\n",
       "          8.0872e-04,  3.1433e-03]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.state_dict()[\"model.layers.0.self_attn.q_proj.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Path to the model checkpoint\n",
    "checkpoint_path = '/home/tian/mtian8/LLM4Tutorial/model/MG_model/llama3_8B_tp_1/iter_0000001/mp_rank_00/model_optim_rng.pt'\n",
    "\n",
    "# Load the model checkpoint\n",
    "loaded = torch.load(checkpoint_path, map_location='cpu')\n",
    "args = loaded['args']\n",
    "n_layers = args.num_layers\n",
    "n_heads = args.num_attention_heads\n",
    "n_heads_kv = getattr(args, \"num_query_groups\", n_heads)\n",
    "n_dense = args.ffn_hidden_size\n",
    "n_hidden = args.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_per_head = n_hidden // n_heads\n",
    "dim = n_hidden // n_heads\n",
    "q_size = n_heads//n_heads_kv\n",
    "megatron_qkv_weight = loaded[\"model\"]['decoder.layers.0.self_attention.linear_qkv.weight'].view(n_heads_kv, (q_size+2)*dim, -1)\n",
    "q_proj_w_megatron = megatron_qkv_weight[:, :q_size*dim, :]\n",
    "k_proj_w_megatron = megatron_qkv_weight[:, q_size*dim:(q_size+1)*dim, :]\n",
    "v_proj_w_megatron = megatron_qkv_weight[:, (q_size+1)*dim:, :]\n",
    "wq_proj = q_proj_w_megatron.reshape(-1, n_hidden)\n",
    "wk_proj = k_proj_w_megatron.reshape(-1, n_hidden)\n",
    "wv_proj = v_proj_w_megatron.reshape(-1, n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 4096])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wq_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mg-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
